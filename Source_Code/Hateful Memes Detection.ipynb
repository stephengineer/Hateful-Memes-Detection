{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Copy of Hateful Memes Detection.ipynb","provenance":[{"file_id":"1IrbgnrySPvftd5W3PXRbvQlBVycJF2q0","timestamp":1618594240445},{"file_id":"https://github.com/facebookresearch/mmf/blob/notebooks/notebooks/mmf_hm_example.ipynb","timestamp":1618476523864}],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"}},"cells":[{"cell_type":"markdown","metadata":{"id":"lwCc6zN1BkIE"},"source":["# MMF Colab Demo\n","\n","This notebook provides step-by-step instructions on how to use MMF to build new models and uses the Hateful Memes (HM) dataset for this specific tutorial.\n","\n","Follow these links to learn more about MMF:\n","- [MMF Blog Post]()\n","- [GitHub repo](https://github.com/facebookresearch/mmf)\n","- [Website](https://mmf.sh) and [Documentation](https://mmf.rtfd.io)\n","\n","In general, the notebook demonstrates how to:\n","\n","1. [Download MMF](#scrollTo=l7Eo9ZqTDW3I)\n","2. [Download the HM dataset](#scrollTo=nYyXt9dzEBEU&line=12&uniqifier=1)\n","3. [Test pretrained models on HM](#scrollTo=nYyXt9dzEBEU&line=12&uniqifier=1)\n","4. [Submit a prediction](#scrollTo=uhKvYHtWHlyr&line=3&uniqifier=1)\n","5. [Train existing model on HM](#scrollTo=) \n","6. [Build your model](#scrollTo=)\n","7. [Train your model on HM](#scrollTo=) "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WAfaIFBI4uTY","executionInfo":{"status":"ok","timestamp":1619710968506,"user_tz":420,"elapsed":1080,"user":{"displayName":"Stephen Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQmD7NZJbux8rfGFCm-sLlkAnKJAAZzShruYlawQ=s64","userId":"13207354784520637281"}},"outputId":"9945d908-7a16-48e8-940f-9f95405fe94f"},"source":["# Show GPU Info\n","!nvidia-smi"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Thu Apr 29 15:42:49 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   46C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jA6QD1p5dtRk","executionInfo":{"status":"ok","timestamp":1619710993273,"user_tz":420,"elapsed":20965,"user":{"displayName":"Stephen Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQmD7NZJbux8rfGFCm-sLlkAnKJAAZzShruYlawQ=s64","userId":"13207354784520637281"}},"outputId":"0bea90e7-c9c6-43b2-c4da-dc3d6a43b298"},"source":["# Setup Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UqQQQRy8d-uI","executionInfo":{"status":"ok","timestamp":1619711018059,"user_tz":420,"elapsed":1314,"user":{"displayName":"Stephen Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQmD7NZJbux8rfGFCm-sLlkAnKJAAZzShruYlawQ=s64","userId":"13207354784520637281"}}},"source":["# Setup Saving Path\n","path = '/content/drive/MyDrive/CS7643/'"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l7Eo9ZqTDW3I"},"source":["## Download MMF\n","\n","In this section, we will download the MMF package and required dependencies."]},{"cell_type":"markdown","metadata":{"id":"eTvGiwu5aE91"},"source":["### Prerequisites \n","Please enable GPU in this notebook: Runtime > Change runtime type > Hardware Accelerator > Set to GPU"]},{"cell_type":"markdown","metadata":{"id":"Uxh_vli1Drky"},"source":["First we will install the MMF package and required dependencies"]},{"cell_type":"markdown","metadata":{"id":"gI3IntF-XCYQ"},"source":["Install from source [Recommended]\n","\n","https://mmf.sh/docs/challenges/hateful_memes_challenge/#predicting-for-phase-1"]},{"cell_type":"code","metadata":{"id":"PBbnmvjredOg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619711039711,"user_tz":420,"elapsed":3507,"user":{"displayName":"Stephen Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQmD7NZJbux8rfGFCm-sLlkAnKJAAZzShruYlawQ=s64","userId":"13207354784520637281"}},"outputId":"ac64f3d6-8271-4fa2-dcbc-ed62b9c761de"},"source":["!git clone https://github.com/facebookresearch/mmf.git"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Cloning into 'mmf'...\n","remote: Enumerating objects: 18963, done.\u001b[K\n","remote: Counting objects: 100% (851/851), done.\u001b[K\n","remote: Compressing objects: 100% (462/462), done.\u001b[K\n","remote: Total 18963 (delta 439), reused 699 (delta 347), pack-reused 18112\u001b[K\n","Receiving objects: 100% (18963/18963), 14.57 MiB | 22.78 MiB/s, done.\n","Resolving deltas: 100% (11954/11954), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"peGegtBw5GNA"},"source":["%pwd"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q6USNo8IfkIS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619712403540,"user_tz":420,"elapsed":10979,"user":{"displayName":"Stephen Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQmD7NZJbux8rfGFCm-sLlkAnKJAAZzShruYlawQ=s64","userId":"13207354784520637281"}},"outputId":"286c69a9-6244-4975-89f4-8642c078caea"},"source":["# %cd /home/jupyter/mmf/\n","%cd /content/mmf/\n","!pip install --editable ."],"execution_count":6,"outputs":[{"output_type":"stream","text":["/content/mmf\n","Obtaining file:///content/mmf\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: datasets==1.2.1 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (1.2.1)\n","Requirement already satisfied: demjson==2.2.4 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (2.2.4)\n","Requirement already satisfied: editdistance==0.5.3 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (0.5.3)\n","Requirement already satisfied: torch<=1.8.1,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (1.8.1)\n","Requirement already satisfied: nltk==3.4.5 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (3.4.5)\n","Requirement already satisfied: pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@7a48db5 from git+https://github.com/PyTorchLightning/pytorch-lightning@7a48db5 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (1.3.0rc2)\n","Requirement already satisfied: requests==2.23.0 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (2.23.0)\n","Requirement already satisfied: termcolor==1.1.0 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (1.1.0)\n","Requirement already satisfied: fasttext==0.9.1 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (0.9.1)\n","Requirement already satisfied: matplotlib==3.3.4 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (3.3.4)\n","Requirement already satisfied: lmdb==0.98 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (0.98)\n","Requirement already satisfied: tqdm<4.50.0,>=4.43.0 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (4.49.0)\n","Requirement already satisfied: torchvision<=0.9.1,>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (0.9.1)\n","Requirement already satisfied: transformers==3.4.0 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (3.4.0)\n","Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (1.19.5)\n","Requirement already satisfied: pycocotools==2.0.2 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (2.0.2)\n","Requirement already satisfied: ftfy==5.8 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (5.8)\n","Requirement already satisfied: torchtext==0.5.0 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (0.5.0)\n","Requirement already satisfied: GitPython==3.1.0 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (3.1.0)\n","Requirement already satisfied: sklearn==0.0 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (0.0)\n","Requirement already satisfied: omegaconf==2.0.6 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (2.0.6)\n","Requirement already satisfied: iopath==0.1.7 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (0.1.7)\n","Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1->mmf==1.0.0rc12) (0.3.3)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1->mmf==1.0.0rc12) (0.70.11.1)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1->mmf==1.0.0rc12) (3.10.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1->mmf==1.0.0rc12) (2.0.2)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1->mmf==1.0.0rc12) (1.1.5)\n","Requirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1->mmf==1.0.0rc12) (3.0.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<=1.8.1,>=1.6.0->mmf==1.0.0rc12) (3.7.4.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.4.5->mmf==1.0.0rc12) (1.15.0)\n","Requirement already satisfied: torchmetrics>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@7a48db5->mmf==1.0.0rc12) (0.3.1)\n","Requirement already satisfied: fsspec[http]>=2021.4.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@7a48db5->mmf==1.0.0rc12) (2021.4.0)\n","Requirement already satisfied: pyDeprecate==0.3.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@7a48db5->mmf==1.0.0rc12) (0.3.0)\n","Requirement already satisfied: tensorboard!=2.5.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@7a48db5->mmf==1.0.0rc12) (2.4.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@7a48db5->mmf==1.0.0rc12) (20.9)\n","Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@7a48db5->mmf==1.0.0rc12) (0.18.2)\n","Requirement already satisfied: PyYAML!=5.4.*,>=5.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@7a48db5->mmf==1.0.0rc12) (5.3.1)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests==2.23.0->mmf==1.0.0rc12) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests==2.23.0->mmf==1.0.0rc12) (2020.12.5)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests==2.23.0->mmf==1.0.0rc12) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests==2.23.0->mmf==1.0.0rc12) (1.24.3)\n","Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.7/dist-packages (from fasttext==0.9.1->mmf==1.0.0rc12) (2.6.2)\n","Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext==0.9.1->mmf==1.0.0rc12) (56.0.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.4->mmf==1.0.0rc12) (2.4.7)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.4->mmf==1.0.0rc12) (0.10.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.4->mmf==1.0.0rc12) (1.3.1)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.4->mmf==1.0.0rc12) (7.1.2)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.4->mmf==1.0.0rc12) (2.8.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0->mmf==1.0.0rc12) (3.0.12)\n","Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0->mmf==1.0.0rc12) (0.1.95)\n","Requirement already satisfied: tokenizers==0.9.2 in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0->mmf==1.0.0rc12) (0.9.2)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0->mmf==1.0.0rc12) (0.0.45)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0->mmf==1.0.0rc12) (3.12.4)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0->mmf==1.0.0rc12) (2019.12.20)\n","Requirement already satisfied: cython>=0.27.3 in /usr/local/lib/python3.7/dist-packages (from pycocotools==2.0.2->mmf==1.0.0rc12) (0.29.22)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy==5.8->mmf==1.0.0rc12) (0.2.5)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython==3.1.0->mmf==1.0.0rc12) (4.0.7)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn==0.0->mmf==1.0.0rc12) (0.22.2.post1)\n","Requirement already satisfied: portalocker in /usr/local/lib/python3.7/dist-packages (from iopath==0.1.7->mmf==1.0.0rc12) (2.3.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets==1.2.1->mmf==1.0.0rc12) (3.4.1)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets==1.2.1->mmf==1.0.0rc12) (2018.9)\n","Requirement already satisfied: aiohttp; extra == \"http\" in /usr/local/lib/python3.7/dist-packages (from fsspec[http]>=2021.4.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@7a48db5->mmf==1.0.0rc12) (3.7.4.post0)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@7a48db5->mmf==1.0.0rc12) (0.12.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@7a48db5->mmf==1.0.0rc12) (1.8.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@7a48db5->mmf==1.0.0rc12) (1.0.1)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@7a48db5->mmf==1.0.0rc12) (1.32.0)\n","Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@7a48db5->mmf==1.0.0rc12) (0.36.2)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@7a48db5->mmf==1.0.0rc12) (0.4.4)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@7a48db5->mmf==1.0.0rc12) (1.28.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@7a48db5->mmf==1.0.0rc12) (3.3.4)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.4.0->mmf==1.0.0rc12) (1.0.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.4.0->mmf==1.0.0rc12) (7.1.2)\n","Requirement already satisfied: smmap<5,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython==3.1.0->mmf==1.0.0rc12) (4.0.0)\n","Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn==0.0->mmf==1.0.0rc12) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=2021.4.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@7a48db5->mmf==1.0.0rc12) (5.1.0)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=2021.4.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@7a48db5->mmf==1.0.0rc12) (1.6.3)\n","Requirement already satisfied: async-timeout<4.0,>=3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=2021.4.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@7a48db5->mmf==1.0.0rc12) (3.0.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=2021.4.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@7a48db5->mmf==1.0.0rc12) (20.3.0)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@7a48db5->mmf==1.0.0rc12) (1.3.0)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@7a48db5->mmf==1.0.0rc12) (4.2.1)\n","Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@7a48db5->mmf==1.0.0rc12) (4.7.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@7a48db5->mmf==1.0.0rc12) (0.2.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@7a48db5->mmf==1.0.0rc12) (3.1.0)\n","Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@7a48db5->mmf==1.0.0rc12) (0.4.8)\n","Installing collected packages: mmf\n","  Found existing installation: mmf 1.0.0rc12\n","    Can't uninstall 'mmf'. No files were found to uninstall.\n","  Running setup.py develop for mmf\n","Successfully installed mmf\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nYyXt9dzEBEU"},"source":["## Download dataset\n","\n","We will now download the Hateful Memes dataset. You will require two things to download the datasets: (i) URL (ii) Password to the zip file. To get both of these follow these steps:\n","\n","1. Go to [DrivenData challenge page](https://www.drivendata.org/competitions/64/hateful-memes/)\n","2. Register, read and acknowledge the agreements for data access.\n","3. Go to the [data page](https://www.drivendata.org/competitions/64/hateful-memes/data), right click on the \"Hateful Memes challenge dataset\" link and \"Copy Link Address\" as shown in the image. This will copy the URL for the zip file to your clipboard which you will use in the next step.\n","![data](https://i.imgur.com/JQx2hPm.png)\n","4. Also, note the password provided in the description.\n","5. Run the next code block, fill in the URL and the zipfile's password when prompted.\n","\n","The code blocks after that will download, convert and visualize the dataset."]},{"cell_type":"markdown","metadata":{"id":"i2Q16v8TXRQ5"},"source":["Hateful Memes: Phase 1\n","\n","https://www.drivendata.org/competitions/64/hateful-memes/data/\n","\n","url: https://drivendata-competition-fb-hateful-memes-data.s3.amazonaws.com/XjiOc5ycDBRRNwbhRlgH.zip?AWSAccessKeyId=AKIARVBOBDCY4MWEDJKS&Signature=FpmkioFlEFPvW%2FMtmwfZIgJ%2BGCE%3D&Expires=1618941090\n","\n","password: EWryfbZyNviilcDF"]},{"cell_type":"code","metadata":{"id":"ulosPHAE-eto","executionInfo":{"status":"ok","timestamp":1619712409025,"user_tz":420,"elapsed":416,"user":{"displayName":"Stephen Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQmD7NZJbux8rfGFCm-sLlkAnKJAAZzShruYlawQ=s64","userId":"13207354784520637281"}}},"source":["# from getpass import getpass, getuser\n","# url = getpass(\"Enter the Hateful Memes data URL:\")\n","# password = getpass(\"Enter ZIP file's Password:\")\n","\n","url='https://drivendata-competition-fb-hateful-memes-data.s3.amazonaws.com/XjiOc5ycDBRRNwbhRlgH.zip?AWSAccessKeyId=AKIARVBOBDCY4MWEDJKS&Signature=ey9vLRX9%2FMRFZRKyFOIlJiJtjmo%3D&Expires=1620143289'\n","password='EWryfbZyNviilcDF'"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oiux2MWzFRPz"},"source":["This will actually download the data."]},{"cell_type":"code","metadata":{"id":"i5Y8wI6BoNN6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619712493549,"user_tz":420,"elapsed":81756,"user":{"displayName":"Stephen Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQmD7NZJbux8rfGFCm-sLlkAnKJAAZzShruYlawQ=s64","userId":"13207354784520637281"}},"outputId":"855b9667-e644-4dcd-f745-3f336c611c91"},"source":["# !curl -o /home/jupyter/hm.zip \"$url\" -H 'Referer: https://www.drivendata.org/competitions/64/hateful-memes/data/' --compressed\n","!curl -o /content/hm.zip \"$url\" -H 'Referer: https://www.drivendata.org/competitions/64/hateful-memes/data/' --compressed"],"execution_count":8,"outputs":[{"output_type":"stream","text":["  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100 4029M  100 4029M    0     0  49.7M      0  0:01:20  0:01:20 --:--:-- 50.2M\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6aiiXO7ZLaXN","executionInfo":{"status":"ok","timestamp":1619712512739,"user_tz":420,"elapsed":5301,"user":{"displayName":"Stephen Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQmD7NZJbux8rfGFCm-sLlkAnKJAAZzShruYlawQ=s64","userId":"13207354784520637281"}},"outputId":"a0b82f35-c0f1-40da-9d92-cc8916524237"},"source":["# run if run into PyYAML version error   \n","!pip install PyYAML==5.1"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Collecting PyYAML==5.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9f/2c/9417b5c774792634834e730932745bc09a7d36754ca00acf1ccd1ac2594d/PyYAML-5.1.tar.gz (274kB)\n","\r\u001b[K     |█▏                              | 10kB 13.1MB/s eta 0:00:01\r\u001b[K     |██▍                             | 20kB 11.7MB/s eta 0:00:01\r\u001b[K     |███▋                            | 30kB 8.8MB/s eta 0:00:01\r\u001b[K     |████▉                           | 40kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████                          | 51kB 5.1MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 61kB 5.4MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 71kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 81kB 6.5MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 92kB 6.2MB/s eta 0:00:01\r\u001b[K     |████████████                    | 102kB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 112kB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 122kB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 133kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 143kB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 153kB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 163kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 174kB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 184kB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 194kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 204kB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 215kB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 225kB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 235kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 245kB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 256kB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 266kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 276kB 6.4MB/s \n","\u001b[?25hBuilding wheels for collected packages: PyYAML\n","  Building wheel for PyYAML (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for PyYAML: filename=PyYAML-5.1-cp37-cp37m-linux_x86_64.whl size=44074 sha256=a57c3def833ad30844719ec681791961b500fdd9f89f8e7549877f6e049f732c\n","  Stored in directory: /root/.cache/pip/wheels/ad/56/bc/1522f864feb2a358ea6f1a92b4798d69ac783a28e80567a18b\n","Successfully built PyYAML\n","Installing collected packages: PyYAML\n","  Found existing installation: PyYAML 5.3.1\n","    Uninstalling PyYAML-5.3.1:\n","      Successfully uninstalled PyYAML-5.3.1\n","Successfully installed PyYAML-5.1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xPYBxsyRFUUb"},"source":["The next command will convert the zip file into required MMF format."]},{"cell_type":"code","metadata":{"id":"NsMmmOB3_rdY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619712647616,"user_tz":420,"elapsed":133316,"user":{"displayName":"Stephen Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQmD7NZJbux8rfGFCm-sLlkAnKJAAZzShruYlawQ=s64","userId":"13207354784520637281"}},"outputId":"2dc17b8d-e4fc-439c-bf95-073a23927460"},"source":["# !mmf_convert_hm --zip_file=/home/jupyter/hm.zip --password=EWryfbZyNviilcDF --bypass_checksum=1\n","!mmf_convert_hm --zip_file=/content/hm.zip --password=EWryfbZyNviilcDF --bypass_checksum=1"],"execution_count":10,"outputs":[{"output_type":"stream","text":["2021-04-29 16:08:38.042113: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n","Data folder is /root/.cache/torch/mmf/data\n","Zip path is /content/hm.zip\n","Copying /content/hm.zip\n","Unzipping /content/hm.zip\n","Extracting the zip can take time. Sit back and relax.\n","Moving train.jsonl\n","Moving dev_seen.jsonl\n","Moving test_seen.jsonl\n","Moving dev_unseen.jsonl\n","Moving test_unseen.jsonl\n","Moving img\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U_JiwZ7WMkGz","executionInfo":{"status":"ok","timestamp":1619713924809,"user_tz":420,"elapsed":588,"user":{"displayName":"Stephen Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQmD7NZJbux8rfGFCm-sLlkAnKJAAZzShruYlawQ=s64","userId":"13207354784520637281"}},"outputId":"173bef39-b601-47c2-baf6-1f6f7e62f296"},"source":["# Check how many images we have in total\n","!ls /root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/images/img/ | wc -l"],"execution_count":11,"outputs":[{"output_type":"stream","text":["12140\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pVJjgvbHel1P"},"source":["That means there are `12.140` **'uniquely named'** images in total and you might recall that the sizes of each set was the following:\n","\n","- `|train.jsonl| = 8.500`\n","- `|dev_seen.jsonl| = 500`\n","- `|dev_unseen.jsonl| = 540`\n","- `|test_seen.jsonl| = 1.000`\n","- `|test_unseen.jsonl| = 2.000`\n","\n","Well, this makes `8.500 + 500 + 540 + 1.000 + 2.000 = 12.540` in total. \\\n","> *Is there something wrong?*\\\n","> **TL;DR:** Nope. Some images in `dev_seen` are used in `dev_unseen`, too. To be specific, they have `400` common images. Hence, in total we have `12.540 - 400 = 12.140` *'unique'* images.\\\n","See <font color='orange'> <b> Extras </b> </font> --> <font color='Gold'><b> Number of 'unique' (based on file names) images </b></font> at the end of this script to see the explanation in detail."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LRU-t5d3J3Ym","executionInfo":{"status":"ok","timestamp":1619736204286,"user_tz":420,"elapsed":2581275,"user":{"displayName":"Stephen Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQmD7NZJbux8rfGFCm-sLlkAnKJAAZzShruYlawQ=s64","userId":"13207354784520637281"}},"outputId":"658d5d9b-b601-46dd-fe00-aa5e565d9afb"},"source":["# ViLBERT\n","!mmf_run config=projects/hateful_memes/configs/vilbert/defaults.yaml \\\n","    model=vilbert \\\n","    dataset=hateful_memes \\\n","    env.save_dir=\"$path\"/vilbert_tune \\\n","    checkpoint.resume_zoo=vilbert.finetuned.hateful_memes.direct \\\n","    training.max_updates=2000 \\\n","    training.batch_size=16"],"execution_count":26,"outputs":[{"output_type":"stream","text":["2021-04-29 22:00:25.725977: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[32m2021-04-29T22:00:29 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/hateful_memes/configs/vilbert/defaults.yaml\n","\u001b[32m2021-04-29T22:00:29 | mmf.utils.configuration: \u001b[0mOverriding option model to vilbert\n","\u001b[32m2021-04-29T22:00:29 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n","\u001b[32m2021-04-29T22:00:29 | mmf.utils.configuration: \u001b[0mOverriding option env.save_dir to /content/drive/MyDrive/CS7643//vilbert_tune\n","\u001b[32m2021-04-29T22:00:29 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_zoo to vilbert.finetuned.hateful_memes.direct\n","\u001b[32m2021-04-29T22:00:29 | mmf.utils.configuration: \u001b[0mOverriding option training.max_updates to 2000\n","\u001b[32m2021-04-29T22:00:29 | mmf.utils.configuration: \u001b[0mOverriding option training.batch_size to 16\n","\u001b[32m2021-04-29T22:00:29 | mmf: \u001b[0mLogging to: /content/drive/MyDrive/CS7643//vilbert_tune/train.log\n","\u001b[32m2021-04-29T22:00:29 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/hateful_memes/configs/vilbert/defaults.yaml', 'model=vilbert', 'dataset=hateful_memes', 'env.save_dir=/content/drive/MyDrive/CS7643//vilbert_tune', 'checkpoint.resume_zoo=vilbert.finetuned.hateful_memes.direct', 'training.max_updates=2000', 'training.batch_size=16'])\n","\u001b[32m2021-04-29T22:00:29 | mmf_cli.run: \u001b[0mTorch version: 1.8.1+cu102\n","\u001b[32m2021-04-29T22:00:29 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla P100-PCIE-16GB\n","\u001b[32m2021-04-29T22:00:29 | mmf_cli.run: \u001b[0mUsing seed 29438233\n","\u001b[32m2021-04-29T22:00:29 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n","\u001b[32m2021-04-29T22:00:31 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2021-04-29T22:00:31 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2021-04-29T22:00:31 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2021-04-29T22:00:31 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing ViLBERTBase: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n","- This IS expected if you are initializing ViLBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n","- This IS NOT expected if you are initializing ViLBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of ViLBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.v_embeddings.image_embeddings.weight', 'bert.v_embeddings.image_embeddings.bias', 'bert.v_embeddings.image_location_embeddings.weight', 'bert.v_embeddings.image_location_embeddings.bias', 'bert.v_embeddings.LayerNorm.weight', 'bert.v_embeddings.LayerNorm.bias', 'bert.encoder.v_layer.0.attention.self.query.weight', 'bert.encoder.v_layer.0.attention.self.query.bias', 'bert.encoder.v_layer.0.attention.self.key.weight', 'bert.encoder.v_layer.0.attention.self.key.bias', 'bert.encoder.v_layer.0.attention.self.value.weight', 'bert.encoder.v_layer.0.attention.self.value.bias', 'bert.encoder.v_layer.0.attention.output.dense.weight', 'bert.encoder.v_layer.0.attention.output.dense.bias', 'bert.encoder.v_layer.0.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.0.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.0.intermediate.dense.weight', 'bert.encoder.v_layer.0.intermediate.dense.bias', 'bert.encoder.v_layer.0.output.dense.weight', 'bert.encoder.v_layer.0.output.dense.bias', 'bert.encoder.v_layer.0.output.LayerNorm.weight', 'bert.encoder.v_layer.0.output.LayerNorm.bias', 'bert.encoder.v_layer.1.attention.self.query.weight', 'bert.encoder.v_layer.1.attention.self.query.bias', 'bert.encoder.v_layer.1.attention.self.key.weight', 'bert.encoder.v_layer.1.attention.self.key.bias', 'bert.encoder.v_layer.1.attention.self.value.weight', 'bert.encoder.v_layer.1.attention.self.value.bias', 'bert.encoder.v_layer.1.attention.output.dense.weight', 'bert.encoder.v_layer.1.attention.output.dense.bias', 'bert.encoder.v_layer.1.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.1.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.1.intermediate.dense.weight', 'bert.encoder.v_layer.1.intermediate.dense.bias', 'bert.encoder.v_layer.1.output.dense.weight', 'bert.encoder.v_layer.1.output.dense.bias', 'bert.encoder.v_layer.1.output.LayerNorm.weight', 'bert.encoder.v_layer.1.output.LayerNorm.bias', 'bert.encoder.v_layer.2.attention.self.query.weight', 'bert.encoder.v_layer.2.attention.self.query.bias', 'bert.encoder.v_layer.2.attention.self.key.weight', 'bert.encoder.v_layer.2.attention.self.key.bias', 'bert.encoder.v_layer.2.attention.self.value.weight', 'bert.encoder.v_layer.2.attention.self.value.bias', 'bert.encoder.v_layer.2.attention.output.dense.weight', 'bert.encoder.v_layer.2.attention.output.dense.bias', 'bert.encoder.v_layer.2.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.2.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.2.intermediate.dense.weight', 'bert.encoder.v_layer.2.intermediate.dense.bias', 'bert.encoder.v_layer.2.output.dense.weight', 'bert.encoder.v_layer.2.output.dense.bias', 'bert.encoder.v_layer.2.output.LayerNorm.weight', 'bert.encoder.v_layer.2.output.LayerNorm.bias', 'bert.encoder.v_layer.3.attention.self.query.weight', 'bert.encoder.v_layer.3.attention.self.query.bias', 'bert.encoder.v_layer.3.attention.self.key.weight', 'bert.encoder.v_layer.3.attention.self.key.bias', 'bert.encoder.v_layer.3.attention.self.value.weight', 'bert.encoder.v_layer.3.attention.self.value.bias', 'bert.encoder.v_layer.3.attention.output.dense.weight', 'bert.encoder.v_layer.3.attention.output.dense.bias', 'bert.encoder.v_layer.3.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.3.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.3.intermediate.dense.weight', 'bert.encoder.v_layer.3.intermediate.dense.bias', 'bert.encoder.v_layer.3.output.dense.weight', 'bert.encoder.v_layer.3.output.dense.bias', 'bert.encoder.v_layer.3.output.LayerNorm.weight', 'bert.encoder.v_layer.3.output.LayerNorm.bias', 'bert.encoder.v_layer.4.attention.self.query.weight', 'bert.encoder.v_layer.4.attention.self.query.bias', 'bert.encoder.v_layer.4.attention.self.key.weight', 'bert.encoder.v_layer.4.attention.self.key.bias', 'bert.encoder.v_layer.4.attention.self.value.weight', 'bert.encoder.v_layer.4.attention.self.value.bias', 'bert.encoder.v_layer.4.attention.output.dense.weight', 'bert.encoder.v_layer.4.attention.output.dense.bias', 'bert.encoder.v_layer.4.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.4.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.4.intermediate.dense.weight', 'bert.encoder.v_layer.4.intermediate.dense.bias', 'bert.encoder.v_layer.4.output.dense.weight', 'bert.encoder.v_layer.4.output.dense.bias', 'bert.encoder.v_layer.4.output.LayerNorm.weight', 'bert.encoder.v_layer.4.output.LayerNorm.bias', 'bert.encoder.v_layer.5.attention.self.query.weight', 'bert.encoder.v_layer.5.attention.self.query.bias', 'bert.encoder.v_layer.5.attention.self.key.weight', 'bert.encoder.v_layer.5.attention.self.key.bias', 'bert.encoder.v_layer.5.attention.self.value.weight', 'bert.encoder.v_layer.5.attention.self.value.bias', 'bert.encoder.v_layer.5.attention.output.dense.weight', 'bert.encoder.v_layer.5.attention.output.dense.bias', 'bert.encoder.v_layer.5.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.5.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.5.intermediate.dense.weight', 'bert.encoder.v_layer.5.intermediate.dense.bias', 'bert.encoder.v_layer.5.output.dense.weight', 'bert.encoder.v_layer.5.output.dense.bias', 'bert.encoder.v_layer.5.output.LayerNorm.weight', 'bert.encoder.v_layer.5.output.LayerNorm.bias', 'bert.encoder.c_layer.0.biattention.query1.weight', 'bert.encoder.c_layer.0.biattention.query1.bias', 'bert.encoder.c_layer.0.biattention.key1.weight', 'bert.encoder.c_layer.0.biattention.key1.bias', 'bert.encoder.c_layer.0.biattention.value1.weight', 'bert.encoder.c_layer.0.biattention.value1.bias', 'bert.encoder.c_layer.0.biattention.query2.weight', 'bert.encoder.c_layer.0.biattention.query2.bias', 'bert.encoder.c_layer.0.biattention.key2.weight', 'bert.encoder.c_layer.0.biattention.key2.bias', 'bert.encoder.c_layer.0.biattention.value2.weight', 'bert.encoder.c_layer.0.biattention.value2.bias', 'bert.encoder.c_layer.0.biOutput.dense1.weight', 'bert.encoder.c_layer.0.biOutput.dense1.bias', 'bert.encoder.c_layer.0.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.0.biOutput.q_dense1.weight', 'bert.encoder.c_layer.0.biOutput.q_dense1.bias', 'bert.encoder.c_layer.0.biOutput.dense2.weight', 'bert.encoder.c_layer.0.biOutput.dense2.bias', 'bert.encoder.c_layer.0.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.0.biOutput.q_dense2.weight', 'bert.encoder.c_layer.0.biOutput.q_dense2.bias', 'bert.encoder.c_layer.0.v_intermediate.dense.weight', 'bert.encoder.c_layer.0.v_intermediate.dense.bias', 'bert.encoder.c_layer.0.v_output.dense.weight', 'bert.encoder.c_layer.0.v_output.dense.bias', 'bert.encoder.c_layer.0.v_output.LayerNorm.weight', 'bert.encoder.c_layer.0.v_output.LayerNorm.bias', 'bert.encoder.c_layer.0.t_intermediate.dense.weight', 'bert.encoder.c_layer.0.t_intermediate.dense.bias', 'bert.encoder.c_layer.0.t_output.dense.weight', 'bert.encoder.c_layer.0.t_output.dense.bias', 'bert.encoder.c_layer.0.t_output.LayerNorm.weight', 'bert.encoder.c_layer.0.t_output.LayerNorm.bias', 'bert.encoder.c_layer.1.biattention.query1.weight', 'bert.encoder.c_layer.1.biattention.query1.bias', 'bert.encoder.c_layer.1.biattention.key1.weight', 'bert.encoder.c_layer.1.biattention.key1.bias', 'bert.encoder.c_layer.1.biattention.value1.weight', 'bert.encoder.c_layer.1.biattention.value1.bias', 'bert.encoder.c_layer.1.biattention.query2.weight', 'bert.encoder.c_layer.1.biattention.query2.bias', 'bert.encoder.c_layer.1.biattention.key2.weight', 'bert.encoder.c_layer.1.biattention.key2.bias', 'bert.encoder.c_layer.1.biattention.value2.weight', 'bert.encoder.c_layer.1.biattention.value2.bias', 'bert.encoder.c_layer.1.biOutput.dense1.weight', 'bert.encoder.c_layer.1.biOutput.dense1.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.1.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.1.biOutput.q_dense1.weight', 'bert.encoder.c_layer.1.biOutput.q_dense1.bias', 'bert.encoder.c_layer.1.biOutput.dense2.weight', 'bert.encoder.c_layer.1.biOutput.dense2.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.1.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.1.biOutput.q_dense2.weight', 'bert.encoder.c_layer.1.biOutput.q_dense2.bias', 'bert.encoder.c_layer.1.v_intermediate.dense.weight', 'bert.encoder.c_layer.1.v_intermediate.dense.bias', 'bert.encoder.c_layer.1.v_output.dense.weight', 'bert.encoder.c_layer.1.v_output.dense.bias', 'bert.encoder.c_layer.1.v_output.LayerNorm.weight', 'bert.encoder.c_layer.1.v_output.LayerNorm.bias', 'bert.encoder.c_layer.1.t_intermediate.dense.weight', 'bert.encoder.c_layer.1.t_intermediate.dense.bias', 'bert.encoder.c_layer.1.t_output.dense.weight', 'bert.encoder.c_layer.1.t_output.dense.bias', 'bert.encoder.c_layer.1.t_output.LayerNorm.weight', 'bert.encoder.c_layer.1.t_output.LayerNorm.bias', 'bert.encoder.c_layer.2.biattention.query1.weight', 'bert.encoder.c_layer.2.biattention.query1.bias', 'bert.encoder.c_layer.2.biattention.key1.weight', 'bert.encoder.c_layer.2.biattention.key1.bias', 'bert.encoder.c_layer.2.biattention.value1.weight', 'bert.encoder.c_layer.2.biattention.value1.bias', 'bert.encoder.c_layer.2.biattention.query2.weight', 'bert.encoder.c_layer.2.biattention.query2.bias', 'bert.encoder.c_layer.2.biattention.key2.weight', 'bert.encoder.c_layer.2.biattention.key2.bias', 'bert.encoder.c_layer.2.biattention.value2.weight', 'bert.encoder.c_layer.2.biattention.value2.bias', 'bert.encoder.c_layer.2.biOutput.dense1.weight', 'bert.encoder.c_layer.2.biOutput.dense1.bias', 'bert.encoder.c_layer.2.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.2.biOutput.q_dense1.weight', 'bert.encoder.c_layer.2.biOutput.q_dense1.bias', 'bert.encoder.c_layer.2.biOutput.dense2.weight', 'bert.encoder.c_layer.2.biOutput.dense2.bias', 'bert.encoder.c_layer.2.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.2.biOutput.q_dense2.weight', 'bert.encoder.c_layer.2.biOutput.q_dense2.bias', 'bert.encoder.c_layer.2.v_intermediate.dense.weight', 'bert.encoder.c_layer.2.v_intermediate.dense.bias', 'bert.encoder.c_layer.2.v_output.dense.weight', 'bert.encoder.c_layer.2.v_output.dense.bias', 'bert.encoder.c_layer.2.v_output.LayerNorm.weight', 'bert.encoder.c_layer.2.v_output.LayerNorm.bias', 'bert.encoder.c_layer.2.t_intermediate.dense.weight', 'bert.encoder.c_layer.2.t_intermediate.dense.bias', 'bert.encoder.c_layer.2.t_output.dense.weight', 'bert.encoder.c_layer.2.t_output.dense.bias', 'bert.encoder.c_layer.2.t_output.LayerNorm.weight', 'bert.encoder.c_layer.2.t_output.LayerNorm.bias', 'bert.encoder.c_layer.3.biattention.query1.weight', 'bert.encoder.c_layer.3.biattention.query1.bias', 'bert.encoder.c_layer.3.biattention.key1.weight', 'bert.encoder.c_layer.3.biattention.key1.bias', 'bert.encoder.c_layer.3.biattention.value1.weight', 'bert.encoder.c_layer.3.biattention.value1.bias', 'bert.encoder.c_layer.3.biattention.query2.weight', 'bert.encoder.c_layer.3.biattention.query2.bias', 'bert.encoder.c_layer.3.biattention.key2.weight', 'bert.encoder.c_layer.3.biattention.key2.bias', 'bert.encoder.c_layer.3.biattention.value2.weight', 'bert.encoder.c_layer.3.biattention.value2.bias', 'bert.encoder.c_layer.3.biOutput.dense1.weight', 'bert.encoder.c_layer.3.biOutput.dense1.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.3.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.3.biOutput.q_dense1.weight', 'bert.encoder.c_layer.3.biOutput.q_dense1.bias', 'bert.encoder.c_layer.3.biOutput.dense2.weight', 'bert.encoder.c_layer.3.biOutput.dense2.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.3.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.3.biOutput.q_dense2.weight', 'bert.encoder.c_layer.3.biOutput.q_dense2.bias', 'bert.encoder.c_layer.3.v_intermediate.dense.weight', 'bert.encoder.c_layer.3.v_intermediate.dense.bias', 'bert.encoder.c_layer.3.v_output.dense.weight', 'bert.encoder.c_layer.3.v_output.dense.bias', 'bert.encoder.c_layer.3.v_output.LayerNorm.weight', 'bert.encoder.c_layer.3.v_output.LayerNorm.bias', 'bert.encoder.c_layer.3.t_intermediate.dense.weight', 'bert.encoder.c_layer.3.t_intermediate.dense.bias', 'bert.encoder.c_layer.3.t_output.dense.weight', 'bert.encoder.c_layer.3.t_output.dense.bias', 'bert.encoder.c_layer.3.t_output.LayerNorm.weight', 'bert.encoder.c_layer.3.t_output.LayerNorm.bias', 'bert.encoder.c_layer.4.biattention.query1.weight', 'bert.encoder.c_layer.4.biattention.query1.bias', 'bert.encoder.c_layer.4.biattention.key1.weight', 'bert.encoder.c_layer.4.biattention.key1.bias', 'bert.encoder.c_layer.4.biattention.value1.weight', 'bert.encoder.c_layer.4.biattention.value1.bias', 'bert.encoder.c_layer.4.biattention.query2.weight', 'bert.encoder.c_layer.4.biattention.query2.bias', 'bert.encoder.c_layer.4.biattention.key2.weight', 'bert.encoder.c_layer.4.biattention.key2.bias', 'bert.encoder.c_layer.4.biattention.value2.weight', 'bert.encoder.c_layer.4.biattention.value2.bias', 'bert.encoder.c_layer.4.biOutput.dense1.weight', 'bert.encoder.c_layer.4.biOutput.dense1.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.4.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.4.biOutput.q_dense1.weight', 'bert.encoder.c_layer.4.biOutput.q_dense1.bias', 'bert.encoder.c_layer.4.biOutput.dense2.weight', 'bert.encoder.c_layer.4.biOutput.dense2.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.4.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.4.biOutput.q_dense2.weight', 'bert.encoder.c_layer.4.biOutput.q_dense2.bias', 'bert.encoder.c_layer.4.v_intermediate.dense.weight', 'bert.encoder.c_layer.4.v_intermediate.dense.bias', 'bert.encoder.c_layer.4.v_output.dense.weight', 'bert.encoder.c_layer.4.v_output.dense.bias', 'bert.encoder.c_layer.4.v_output.LayerNorm.weight', 'bert.encoder.c_layer.4.v_output.LayerNorm.bias', 'bert.encoder.c_layer.4.t_intermediate.dense.weight', 'bert.encoder.c_layer.4.t_intermediate.dense.bias', 'bert.encoder.c_layer.4.t_output.dense.weight', 'bert.encoder.c_layer.4.t_output.dense.bias', 'bert.encoder.c_layer.4.t_output.LayerNorm.weight', 'bert.encoder.c_layer.4.t_output.LayerNorm.bias', 'bert.encoder.c_layer.5.biattention.query1.weight', 'bert.encoder.c_layer.5.biattention.query1.bias', 'bert.encoder.c_layer.5.biattention.key1.weight', 'bert.encoder.c_layer.5.biattention.key1.bias', 'bert.encoder.c_layer.5.biattention.value1.weight', 'bert.encoder.c_layer.5.biattention.value1.bias', 'bert.encoder.c_layer.5.biattention.query2.weight', 'bert.encoder.c_layer.5.biattention.query2.bias', 'bert.encoder.c_layer.5.biattention.key2.weight', 'bert.encoder.c_layer.5.biattention.key2.bias', 'bert.encoder.c_layer.5.biattention.value2.weight', 'bert.encoder.c_layer.5.biattention.value2.bias', 'bert.encoder.c_layer.5.biOutput.dense1.weight', 'bert.encoder.c_layer.5.biOutput.dense1.bias', 'bert.encoder.c_layer.5.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.5.biOutput.q_dense1.weight', 'bert.encoder.c_layer.5.biOutput.q_dense1.bias', 'bert.encoder.c_layer.5.biOutput.dense2.weight', 'bert.encoder.c_layer.5.biOutput.dense2.bias', 'bert.encoder.c_layer.5.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.5.biOutput.q_dense2.weight', 'bert.encoder.c_layer.5.biOutput.q_dense2.bias', 'bert.encoder.c_layer.5.v_intermediate.dense.weight', 'bert.encoder.c_layer.5.v_intermediate.dense.bias', 'bert.encoder.c_layer.5.v_output.dense.weight', 'bert.encoder.c_layer.5.v_output.dense.bias', 'bert.encoder.c_layer.5.v_output.LayerNorm.weight', 'bert.encoder.c_layer.5.v_output.LayerNorm.bias', 'bert.encoder.c_layer.5.t_intermediate.dense.weight', 'bert.encoder.c_layer.5.t_intermediate.dense.bias', 'bert.encoder.c_layer.5.t_output.dense.weight', 'bert.encoder.c_layer.5.t_output.dense.bias', 'bert.encoder.c_layer.5.t_output.LayerNorm.weight', 'bert.encoder.c_layer.5.t_output.LayerNorm.bias', 'bert.t_pooler.dense.weight', 'bert.t_pooler.dense.bias', 'bert.v_pooler.dense.weight', 'bert.v_pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[32m2021-04-29T22:00:45 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n","\u001b[32m2021-04-29T22:00:45 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-29T22:00:45 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/basecontainer.py:232: UserWarning: cfg.pretty() is deprecated and will be removed in a future version.\n","Use OmegaConf.to_yaml(cfg)\n","\n","  category=UserWarning,\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-29T22:00:45 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/basecontainer.py:232: UserWarning: cfg.pretty() is deprecated and will be removed in a future version.\n","Use OmegaConf.to_yaml(cfg)\n","\n","  category=UserWarning,\n","\n","\u001b[32m2021-04-29T22:00:45 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n","[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/models/vilbert/vilbert.finetuned.hateful_memes_direct.tar.gz to /root/.cache/torch/mmf/data/models/vilbert.finetuned.hateful_memes.direct/vilbert.finetuned.hateful_memes_direct.tar.gz ]\n","Downloading vilbert.finetuned.hateful_memes_direct.tar.gz: 100% 918M/918M [00:27<00:00, 32.9MB/s]\n","[ Starting checksum for vilbert.finetuned.hateful_memes_direct.tar.gz]\n","[ Checksum successful for vilbert.finetuned.hateful_memes_direct.tar.gz]\n","Unpacking vilbert.finetuned.hateful_memes_direct.tar.gz\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-29T22:01:29 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-29T22:01:29 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-29T22:01:29 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-29T22:01:29 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-29T22:01:30 | mmf.utils.checkpoint: \u001b[0mMissing keys ['model.bert.embeddings.position_ids'] in the checkpoint.\n","If this is not your checkpoint, please open up an issue on MMF GitHub. \n","Unexpected keys if any: []\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-29T22:01:30 | py.warnings: \u001b[0m/content/mmf/mmf/utils/checkpoint.py:304: UserWarning: 'optimizer' key is not present in the checkpoint asked to be loaded. Skipping.\n","  \"'optimizer' key is not present in the \"\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-29T22:01:30 | py.warnings: \u001b[0m/content/mmf/mmf/utils/checkpoint.py:304: UserWarning: 'optimizer' key is not present in the checkpoint asked to be loaded. Skipping.\n","  \"'optimizer' key is not present in the \"\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-29T22:01:30 | py.warnings: \u001b[0m/content/mmf/mmf/utils/checkpoint.py:347: UserWarning: 'lr_scheduler' key is not present in the checkpoint asked to be loaded. Setting lr_scheduler's last_epoch to current_iteration.\n","  \"'lr_scheduler' key is not present in the \"\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-29T22:01:30 | py.warnings: \u001b[0m/content/mmf/mmf/utils/checkpoint.py:347: UserWarning: 'lr_scheduler' key is not present in the checkpoint asked to be loaded. Setting lr_scheduler's last_epoch to current_iteration.\n","  \"'lr_scheduler' key is not present in the \"\n","\n","\u001b[32m2021-04-29T22:01:30 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n","\u001b[32m2021-04-29T22:01:30 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 0\n","\u001b[32m2021-04-29T22:01:30 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 0\n","\u001b[32m2021-04-29T22:01:30 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 0\n","\u001b[32m2021-04-29T22:01:30 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n","\u001b[32m2021-04-29T22:01:30 | mmf.trainers.mmf_trainer: \u001b[0mViLBERT(\n","  (model): ViLBERTForClassification(\n","    (bert): ViLBERTBase(\n","      (embeddings): BertEmbeddings(\n","        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","        (position_embeddings): Embedding(512, 768)\n","        (token_type_embeddings): Embedding(2, 768)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (v_embeddings): BertImageFeatureEmbeddings(\n","        (image_embeddings): Linear(in_features=2048, out_features=1024, bias=True)\n","        (image_location_embeddings): Linear(in_features=5, out_features=1024, bias=True)\n","        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (encoder): BertEncoder(\n","        (layer): ModuleList(\n","          (0): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (1): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (2): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (3): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (4): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (5): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (6): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (7): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (8): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (9): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (10): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (11): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","        (v_layer): ModuleList(\n","          (0): BertImageLayer(\n","            (attention): BertImageAttention(\n","              (self): BertImageSelfAttention(\n","                (query): Linear(in_features=1024, out_features=1024, bias=True)\n","                (key): Linear(in_features=1024, out_features=1024, bias=True)\n","                (value): Linear(in_features=1024, out_features=1024, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertImageSelfOutput(\n","                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (1): BertImageLayer(\n","            (attention): BertImageAttention(\n","              (self): BertImageSelfAttention(\n","                (query): Linear(in_features=1024, out_features=1024, bias=True)\n","                (key): Linear(in_features=1024, out_features=1024, bias=True)\n","                (value): Linear(in_features=1024, out_features=1024, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertImageSelfOutput(\n","                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (2): BertImageLayer(\n","            (attention): BertImageAttention(\n","              (self): BertImageSelfAttention(\n","                (query): Linear(in_features=1024, out_features=1024, bias=True)\n","                (key): Linear(in_features=1024, out_features=1024, bias=True)\n","                (value): Linear(in_features=1024, out_features=1024, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertImageSelfOutput(\n","                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (3): BertImageLayer(\n","            (attention): BertImageAttention(\n","              (self): BertImageSelfAttention(\n","                (query): Linear(in_features=1024, out_features=1024, bias=True)\n","                (key): Linear(in_features=1024, out_features=1024, bias=True)\n","                (value): Linear(in_features=1024, out_features=1024, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertImageSelfOutput(\n","                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (4): BertImageLayer(\n","            (attention): BertImageAttention(\n","              (self): BertImageSelfAttention(\n","                (query): Linear(in_features=1024, out_features=1024, bias=True)\n","                (key): Linear(in_features=1024, out_features=1024, bias=True)\n","                (value): Linear(in_features=1024, out_features=1024, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertImageSelfOutput(\n","                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (5): BertImageLayer(\n","            (attention): BertImageAttention(\n","              (self): BertImageSelfAttention(\n","                (query): Linear(in_features=1024, out_features=1024, bias=True)\n","                (key): Linear(in_features=1024, out_features=1024, bias=True)\n","                (value): Linear(in_features=1024, out_features=1024, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertImageSelfOutput(\n","                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","        (c_layer): ModuleList(\n","          (0): BertConnectionLayer(\n","            (biattention): BertBiAttention(\n","              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (query2): Linear(in_features=768, out_features=1024, bias=True)\n","              (key2): Linear(in_features=768, out_features=1024, bias=True)\n","              (value2): Linear(in_features=768, out_features=1024, bias=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (biOutput): BertBiOutput(\n","              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (q_dropout1): Dropout(p=0.1, inplace=False)\n","              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (q_dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (v_intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (v_output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (t_intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (t_output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (1): BertConnectionLayer(\n","            (biattention): BertBiAttention(\n","              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (query2): Linear(in_features=768, out_features=1024, bias=True)\n","              (key2): Linear(in_features=768, out_features=1024, bias=True)\n","              (value2): Linear(in_features=768, out_features=1024, bias=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (biOutput): BertBiOutput(\n","              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (q_dropout1): Dropout(p=0.1, inplace=False)\n","              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (q_dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (v_intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (v_output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (t_intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (t_output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (2): BertConnectionLayer(\n","            (biattention): BertBiAttention(\n","              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (query2): Linear(in_features=768, out_features=1024, bias=True)\n","              (key2): Linear(in_features=768, out_features=1024, bias=True)\n","              (value2): Linear(in_features=768, out_features=1024, bias=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (biOutput): BertBiOutput(\n","              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (q_dropout1): Dropout(p=0.1, inplace=False)\n","              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (q_dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (v_intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (v_output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (t_intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (t_output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (3): BertConnectionLayer(\n","            (biattention): BertBiAttention(\n","              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (query2): Linear(in_features=768, out_features=1024, bias=True)\n","              (key2): Linear(in_features=768, out_features=1024, bias=True)\n","              (value2): Linear(in_features=768, out_features=1024, bias=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (biOutput): BertBiOutput(\n","              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (q_dropout1): Dropout(p=0.1, inplace=False)\n","              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (q_dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (v_intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (v_output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (t_intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (t_output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (4): BertConnectionLayer(\n","            (biattention): BertBiAttention(\n","              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (query2): Linear(in_features=768, out_features=1024, bias=True)\n","              (key2): Linear(in_features=768, out_features=1024, bias=True)\n","              (value2): Linear(in_features=768, out_features=1024, bias=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (biOutput): BertBiOutput(\n","              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (q_dropout1): Dropout(p=0.1, inplace=False)\n","              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (q_dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (v_intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (v_output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (t_intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (t_output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (5): BertConnectionLayer(\n","            (biattention): BertBiAttention(\n","              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (query2): Linear(in_features=768, out_features=1024, bias=True)\n","              (key2): Linear(in_features=768, out_features=1024, bias=True)\n","              (value2): Linear(in_features=768, out_features=1024, bias=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (biOutput): BertBiOutput(\n","              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n","              (q_dropout1): Dropout(p=0.1, inplace=False)\n","              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n","              (q_dropout2): Dropout(p=0.1, inplace=False)\n","            )\n","            (v_intermediate): BertImageIntermediate(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            )\n","            (v_output): BertImageOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (t_intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (t_output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","      (t_pooler): BertTextPooler(\n","        (dense): Linear(in_features=768, out_features=1024, bias=True)\n","        (activation): ReLU()\n","      )\n","      (v_pooler): BertImagePooler(\n","        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","        (activation): ReLU()\n","      )\n","    )\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (classifier): Sequential(\n","      (0): BertPredictionHeadTransform(\n","        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n","      )\n","      (1): Linear(in_features=1024, out_features=2, bias=True)\n","    )\n","  )\n","  (losses): Losses(\n","    (losses): ModuleList(\n","      (0): MMFLoss(\n","        (loss_criterion): CrossEntropyLoss(\n","          (loss_fn): CrossEntropyLoss()\n","        )\n","      )\n","    )\n","  )\n",")\n","\u001b[32m2021-04-29T22:01:30 | mmf.utils.general: \u001b[0mTotal Parameters: 247780354. Trained Parameters: 247780354\n","\u001b[32m2021-04-29T22:01:30 | mmf.trainers.core.training_loop: \u001b[0mStarting training...\n","\u001b[32m2021-04-29T22:03:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/2000, train/hateful_memes/cross_entropy: 0.4472, train/hateful_memes/cross_entropy/avg: 0.4472, train/total_loss: 0.4472, train/total_loss/avg: 0.4472, max mem: 7250.0, experiment: run, epoch: 1, num_updates: 100, iterations: 100, max_updates: 2000, lr: 0., ups: 0.74, time: 02m 15s 741ms, time_since_start: 03m 655ms, eta: 44m 26s 770ms\n","\u001b[32m2021-04-29T22:05:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/2000, train/hateful_memes/cross_entropy: 0.4472, train/hateful_memes/cross_entropy/avg: 0.5451, train/total_loss: 0.4472, train/total_loss/avg: 0.5451, max mem: 7250.0, experiment: run, epoch: 1, num_updates: 200, iterations: 200, max_updates: 2000, lr: 0., ups: 0.76, time: 02m 12s 009ms, time_since_start: 05m 12s 665ms, eta: 40m 56s 957ms\n","\u001b[32m2021-04-29T22:08:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/2000, train/hateful_memes/cross_entropy: 0.6429, train/hateful_memes/cross_entropy/avg: 0.8305, train/total_loss: 0.6429, train/total_loss/avg: 0.8305, max mem: 7250.0, experiment: run, epoch: 1, num_updates: 300, iterations: 300, max_updates: 2000, lr: 0., ups: 0.76, time: 02m 12s 689ms, time_since_start: 07m 25s 354ms, eta: 38m 52s 422ms\n","\u001b[32m2021-04-29T22:10:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/2000, train/hateful_memes/cross_entropy: 0.4472, train/hateful_memes/cross_entropy/avg: 0.7200, train/total_loss: 0.4472, train/total_loss/avg: 0.7200, max mem: 7250.0, experiment: run, epoch: 1, num_updates: 400, iterations: 400, max_updates: 2000, lr: 0., ups: 0.76, time: 02m 12s 361ms, time_since_start: 09m 37s 716ms, eta: 36m 29s 796ms\n","\u001b[32m2021-04-29T22:12:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/2000, train/hateful_memes/cross_entropy: 0.6108, train/hateful_memes/cross_entropy/avg: 0.6981, train/total_loss: 0.6108, train/total_loss/avg: 0.6981, max mem: 7250.0, experiment: run, epoch: 1, num_updates: 500, iterations: 500, max_updates: 2000, lr: 0., ups: 0.76, time: 02m 12s 652ms, time_since_start: 11m 50s 369ms, eta: 34m 17s 439ms\n","\u001b[32m2021-04-29T22:13:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/2000, train/hateful_memes/cross_entropy: 0.4472, train/hateful_memes/cross_entropy/avg: 0.6115, train/total_loss: 0.4472, train/total_loss/avg: 0.6115, max mem: 7250.0, experiment: run, epoch: 2, num_updates: 600, iterations: 600, max_updates: 2000, lr: 0., ups: 1.30, time: 01m 17s 071ms, time_since_start: 13m 07s 441ms, eta: 18m 35s 693ms\n","\u001b[32m2021-04-29T22:14:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/2000, train/hateful_memes/cross_entropy: 0.4472, train/hateful_memes/cross_entropy/avg: 0.5428, train/total_loss: 0.4472, train/total_loss/avg: 0.5428, max mem: 7250.0, experiment: run, epoch: 2, num_updates: 700, iterations: 700, max_updates: 2000, lr: 0., ups: 1.82, time: 55s 189ms, time_since_start: 14m 02s 631ms, eta: 12m 21s 859ms\n","\u001b[32m2021-04-29T22:15:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/2000, train/hateful_memes/cross_entropy: 0.4472, train/hateful_memes/cross_entropy/avg: 0.5419, train/total_loss: 0.4472, train/total_loss/avg: 0.5419, max mem: 7250.0, experiment: run, epoch: 2, num_updates: 800, iterations: 800, max_updates: 2000, lr: 0., ups: 1.82, time: 55s 127ms, time_since_start: 14m 57s 758ms, eta: 11m 24s 022ms\n","\u001b[32m2021-04-29T22:16:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/2000, train/hateful_memes/cross_entropy: 0.4472, train/hateful_memes/cross_entropy/avg: 0.5067, train/total_loss: 0.4472, train/total_loss/avg: 0.5067, max mem: 7250.0, experiment: run, epoch: 2, num_updates: 900, iterations: 900, max_updates: 2000, lr: 0., ups: 1.82, time: 55s 050ms, time_since_start: 15m 52s 809ms, eta: 10m 26s 147ms\n","\u001b[32m2021-04-29T22:17:32 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2021-04-29T22:17:32 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2021-04-29T22:17:56 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2021-04-29T22:18:35 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2021-04-29T22:18:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/2000, train/hateful_memes/cross_entropy: 0.3884, train/hateful_memes/cross_entropy/avg: 0.4685, train/total_loss: 0.3884, train/total_loss/avg: 0.4685, max mem: 7250.0, experiment: run, epoch: 2, num_updates: 1000, iterations: 1000, max_updates: 2000, lr: 0.00001, ups: 0.86, time: 01m 56s 722ms, time_since_start: 17m 49s 531ms, eta: 20m 06s 908ms\n","\u001b[32m2021-04-29T22:18:35 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2021-04-29T22:18:35 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2021-04-29T22:19:23 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2021-04-29T22:19:52 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n","\u001b[32m2021-04-29T22:20:23 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2021-04-29T22:21:01 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2021-04-29T22:21:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/2000, val/hateful_memes/cross_entropy: 0.8601, val/total_loss: 0.8601, val/hateful_memes/accuracy: 0.6778, val/hateful_memes/binary_f1: 0.4459, val/hateful_memes/roc_auc: 0.6733, num_updates: 1000, epoch: 2, iterations: 1000, max_updates: 2000, val_time: 02m 26s 080ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.673338\n","\u001b[32m2021-04-29T22:23:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1100/2000, train/hateful_memes/cross_entropy: 0.3884, train/hateful_memes/cross_entropy/avg: 0.4337, train/total_loss: 0.3884, train/total_loss/avg: 0.4337, max mem: 7250.0, experiment: run, epoch: 3, num_updates: 1100, iterations: 1100, max_updates: 2000, lr: 0.00001, ups: 0.82, time: 02m 02s 971ms, time_since_start: 22m 18s 586ms, eta: 19m 04s 370ms\n","\u001b[32m2021-04-29T22:25:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/2000, train/hateful_memes/cross_entropy: 0.2257, train/hateful_memes/cross_entropy/avg: 0.4106, train/total_loss: 0.2257, train/total_loss/avg: 0.4106, max mem: 7250.0, experiment: run, epoch: 3, num_updates: 1200, iterations: 1200, max_updates: 2000, lr: 0.00001, ups: 0.85, time: 01m 58s 994ms, time_since_start: 24m 17s 580ms, eta: 16m 24s 322ms\n","\u001b[32m2021-04-29T22:26:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1300/2000, train/hateful_memes/cross_entropy: 0.2257, train/hateful_memes/cross_entropy/avg: 0.3865, train/total_loss: 0.2257, train/total_loss/avg: 0.3865, max mem: 7250.0, experiment: run, epoch: 3, num_updates: 1300, iterations: 1300, max_updates: 2000, lr: 0.00001, ups: 0.86, time: 01m 56s 613ms, time_since_start: 26m 14s 194ms, eta: 14m 04s 048ms\n","\u001b[32m2021-04-29T22:28:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/2000, train/hateful_memes/cross_entropy: 0.1782, train/hateful_memes/cross_entropy/avg: 0.3635, train/total_loss: 0.1782, train/total_loss/avg: 0.3635, max mem: 7250.0, experiment: run, epoch: 3, num_updates: 1400, iterations: 1400, max_updates: 2000, lr: 0.00001, ups: 0.84, time: 01m 59s 582ms, time_since_start: 28m 13s 777ms, eta: 12m 21s 891ms\n","\u001b[32m2021-04-29T22:31:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1500/2000, train/hateful_memes/cross_entropy: 0.2069, train/hateful_memes/cross_entropy/avg: 0.3531, train/total_loss: 0.2069, train/total_loss/avg: 0.3531, max mem: 7250.0, experiment: run, epoch: 3, num_updates: 1500, iterations: 1500, max_updates: 2000, lr: 0.00001, ups: 0.83, time: 02m 01s 111ms, time_since_start: 30m 14s 888ms, eta: 10m 26s 144ms\n","\u001b[32m2021-04-29T22:33:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1600/2000, train/hateful_memes/cross_entropy: 0.1782, train/hateful_memes/cross_entropy/avg: 0.3339, train/total_loss: 0.1782, train/total_loss/avg: 0.3339, max mem: 7250.0, experiment: run, epoch: 4, num_updates: 1600, iterations: 1600, max_updates: 2000, lr: 0.00001, ups: 0.81, time: 02m 04s 065ms, time_since_start: 32m 18s 953ms, eta: 08m 33s 133ms\n","\u001b[32m2021-04-29T22:33:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1700/2000, train/hateful_memes/cross_entropy: 0.1782, train/hateful_memes/cross_entropy/avg: 0.3177, train/total_loss: 0.1782, train/total_loss/avg: 0.3177, max mem: 7250.0, experiment: run, epoch: 4, num_updates: 1700, iterations: 1700, max_updates: 2000, lr: 0.00001, ups: 1.85, time: 54s 841ms, time_since_start: 33m 13s 795ms, eta: 02m 50s 118ms\n","\u001b[32m2021-04-29T22:34:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1800/2000, train/hateful_memes/cross_entropy: 0.1782, train/hateful_memes/cross_entropy/avg: 0.3224, train/total_loss: 0.1782, train/total_loss/avg: 0.3224, max mem: 7250.0, experiment: run, epoch: 4, num_updates: 1800, iterations: 1800, max_updates: 2000, lr: 0.00001, ups: 1.85, time: 54s 843ms, time_since_start: 34m 08s 638ms, eta: 01m 53s 416ms\n","\u001b[32m2021-04-29T22:35:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1900/2000, train/hateful_memes/cross_entropy: 0.1782, train/hateful_memes/cross_entropy/avg: 0.3147, train/total_loss: 0.1782, train/total_loss/avg: 0.3147, max mem: 7250.0, experiment: run, epoch: 4, num_updates: 1900, iterations: 1900, max_updates: 2000, lr: 0.00001, ups: 1.85, time: 54s 764ms, time_since_start: 35m 03s 403ms, eta: 56s 626ms\n","\u001b[32m2021-04-29T22:36:43 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[32m2021-04-29T22:36:43 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2021-04-29T22:37:06 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2021-04-29T22:37:42 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2021-04-29T22:37:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/2000, train/hateful_memes/cross_entropy: 0.1768, train/hateful_memes/cross_entropy/avg: 0.3014, train/total_loss: 0.1768, train/total_loss/avg: 0.3014, max mem: 7250.0, experiment: run, epoch: 4, num_updates: 2000, iterations: 2000, max_updates: 2000, lr: 0., ups: 0.88, time: 01m 53s 290ms, time_since_start: 36m 56s 694ms, eta: 0ms\n","\u001b[32m2021-04-29T22:37:42 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n","\u001b[32m2021-04-29T22:37:42 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","\u001b[32m2021-04-29T22:38:29 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n","\u001b[32m2021-04-29T22:38:57 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n","\u001b[32m2021-04-29T22:39:24 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n","\u001b[32m2021-04-29T22:39:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/2000, val/hateful_memes/cross_entropy: 1.3285, val/total_loss: 1.3285, val/hateful_memes/accuracy: 0.6593, val/hateful_memes/binary_f1: 0.3699, val/hateful_memes/roc_auc: 0.6712, num_updates: 2000, epoch: 4, iterations: 2000, max_updates: 2000, val_time: 01m 42s 204ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.673338\n","\u001b[32m2021-04-29T22:39:25 | mmf.trainers.core.training_loop: \u001b[0mStepping into final validation check\n","\u001b[32m2021-04-29T22:39:25 | mmf.utils.checkpoint: \u001b[0mRestoring checkpoint\n","\u001b[32m2021-04-29T22:39:25 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n","\u001b[32m2021-04-29T22:40:17 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n","\u001b[32m2021-04-29T22:40:17 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 1000\n","\u001b[32m2021-04-29T22:40:17 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 1000\n","\u001b[32m2021-04-29T22:40:17 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 2\n","\u001b[32m2021-04-29T22:40:26 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on test set\n","\u001b[32m2021-04-29T22:40:26 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","  0% 0/125 [00:00<?, ?it/s]\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-29T22:40:32 | py.warnings: \u001b[0m/content/mmf/mmf/modules/losses.py:109: UserWarning: Sample list has not field 'targets', are you sure that your ImDB has labels? you may have wanted to run with evaluation.predict=true\n","  \"Sample list has not field 'targets', are you \"\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-29T22:40:32 | py.warnings: \u001b[0m/content/mmf/mmf/modules/losses.py:109: UserWarning: Sample list has not field 'targets', are you sure that your ImDB has labels? you may have wanted to run with evaluation.predict=true\n","  \"Sample list has not field 'targets', are you \"\n","\n","  1% 1/125 [00:06<12:44,  6.17s/it]\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-29T22:40:32 | py.warnings: \u001b[0m/content/mmf/mmf/common/report.py:164: UserWarning: targets not found in report. Metrics calculation might not work as expected.\n","  + \"might not work as expected.\"\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-29T22:40:32 | py.warnings: \u001b[0m/content/mmf/mmf/common/report.py:164: UserWarning: targets not found in report. Metrics calculation might not work as expected.\n","  + \"might not work as expected.\"\n","\n","100% 125/125 [02:49<00:00,  1.36s/it]\n","Traceback (most recent call last):\n","  File \"/usr/local/bin/mmf_run\", line 33, in <module>\n","    sys.exit(load_entry_point('mmf', 'console_scripts', 'mmf_run')())\n","  File \"/content/mmf/mmf_cli/run.py\", line 133, in run\n","    main(configuration, predict=predict)\n","  File \"/content/mmf/mmf_cli/run.py\", line 56, in main\n","    trainer.train()\n","  File \"/content/mmf/mmf/trainers/mmf_trainer.py\", line 144, in train\n","    self.inference()\n","  File \"/content/mmf/mmf/trainers/mmf_trainer.py\", line 162, in inference\n","    report, meter = self.evaluation_loop(dataset, use_tqdm=True)\n","  File \"/content/mmf/mmf/trainers/core/evaluation_loop.py\", line 91, in evaluation_loop\n","    combined_report.metrics = self.metrics(combined_report, combined_report)\n","  File \"/content/mmf/mmf/modules/metrics.py\", line 156, in __call__\n","    sample_list, model_output, *args, **kwargs\n","  File \"/content/mmf/mmf/modules/metrics.py\", line 221, in _calculate_with_checks\n","    value = self.calculate(*args, **kwargs)\n","  File \"/content/mmf/mmf/modules/metrics.py\", line 254, in calculate\n","    expected = sample_list[\"targets\"]\n","KeyError: 'targets'\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Hwhi8nS66xd7"},"source":["# ViLBERT CC\n","!mmf_run config=projects/hateful_memes/configs/vilbert/from_cc.yaml \\\n","    model=vilbert \\\n","    dataset=hateful_memes \\\n","    env.save_dir=\"$path\"/vilbert_tune \\\n","    checkpoint.resume_zoo=vilbert.finetuned.hateful_memes.from_cc_original \\\n","    training.max_updates=2000 \\\n","    training.batch_size=16"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uhKvYHtWHlyr"},"source":["## Submit a prediction\n","\n","Now, we will use a pretrained model from MMF to submit a prediction to DrivenData. Run the command in the next block and at the end it will output the path to the csv file generated. Download and upload that file to [DrivenData's submission page](https://www.drivendata.org/competitions/64/hateful-memes/submissions/)."]},{"cell_type":"code","metadata":{"id":"ZlPz8R7QWXIp"},"source":["!mmf_predict config=projects/hateful_memes/configs/mmbt/defaults.yaml \\\n","  model=mmbt \\\n","  dataset=hateful_memes \\\n","  run_type=test \\\n","  checkpoint.resume_pretrained=False \\\n","  dataset_config.hateful_memes.annotations.val[0]=hateful_memes/defaults/annotations/dev_seen.jsonl \\\n","  dataset_config.hateful_memes.annotations.test[0]=hateful_memes/defaults/annotations/test_seen.jsonl"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eNUoAtfO1E-_","executionInfo":{"status":"ok","timestamp":1619736322748,"user_tz":420,"elapsed":109236,"user":{"displayName":"Stephen Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQmD7NZJbux8rfGFCm-sLlkAnKJAAZzShruYlawQ=s64","userId":"13207354784520637281"}},"outputId":"079a57ac-471b-4c8b-9624-2f7934f2e096"},"source":["# predictions on the validation set\n","!mmf_predict config=\"$path\"/vilbert_tune/config.yaml \\\n","  model=vilbert \\\n","  dataset=hateful_memes \\\n","  run_type=val \\\n","  checkpoint.resume_file=\"$path\"/vilbert_tune/best.ckpt \\\n","  checkpoint.resume_pretrained=False \\\n","  dataset_config.hateful_memes.annotations.val[0]=hateful_memes/defaults/annotations/dev_seen.jsonl \\\n","  dataset_config.hateful_memes.annotations.test[0]=hateful_memes/defaults/annotations/test_seen.jsonl"],"execution_count":27,"outputs":[{"output_type":"stream","text":["2021-04-29 22:43:40.411443: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n","\u001b[32m2021-04-29T22:43:48 | mmf.utils.configuration: \u001b[0mOverriding option config to /content/drive/MyDrive/CS7643//vilbert_tune/config.yaml\n","\u001b[32m2021-04-29T22:43:48 | mmf.utils.configuration: \u001b[0mOverriding option model to vilbert\n","\u001b[32m2021-04-29T22:43:48 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n","\u001b[32m2021-04-29T22:43:48 | mmf.utils.configuration: \u001b[0mOverriding option run_type to val\n","\u001b[32m2021-04-29T22:43:48 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_file to /content/drive/MyDrive/CS7643//vilbert_tune/best.ckpt\n","\u001b[32m2021-04-29T22:43:48 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_pretrained to False\n","\u001b[32m2021-04-29T22:43:48 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.val[0] to hateful_memes/defaults/annotations/dev_seen.jsonl\n","\u001b[32m2021-04-29T22:43:48 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.test[0] to hateful_memes/defaults/annotations/test_seen.jsonl\n","\u001b[32m2021-04-29T22:43:48 | mmf.utils.configuration: \u001b[0mOverriding option evaluation.predict to true\n","\u001b[32m2021-04-29T22:43:48 | mmf: \u001b[0mLogging to: /content/drive/MyDrive/CS7643//vilbert_tune/train.log\n","\u001b[32m2021-04-29T22:43:48 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=/content/drive/MyDrive/CS7643//vilbert_tune/config.yaml', 'model=vilbert', 'dataset=hateful_memes', 'run_type=val', 'checkpoint.resume_file=/content/drive/MyDrive/CS7643//vilbert_tune/best.ckpt', 'checkpoint.resume_pretrained=False', 'dataset_config.hateful_memes.annotations.val[0]=hateful_memes/defaults/annotations/dev_seen.jsonl', 'dataset_config.hateful_memes.annotations.test[0]=hateful_memes/defaults/annotations/test_seen.jsonl', 'evaluation.predict=true'])\n","\u001b[32m2021-04-29T22:43:48 | mmf_cli.run: \u001b[0mTorch version: 1.8.1+cu102\n","\u001b[32m2021-04-29T22:43:48 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla P100-PCIE-16GB\n","\u001b[32m2021-04-29T22:43:48 | mmf_cli.run: \u001b[0mUsing seed 29438233\n","\u001b[32m2021-04-29T22:43:48 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n","\u001b[32m2021-04-29T22:43:51 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2021-04-29T22:43:51 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2021-04-29T22:43:51 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n","\u001b[32m2021-04-29T22:43:51 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing ViLBERTBase: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n","- This IS expected if you are initializing ViLBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n","- This IS NOT expected if you are initializing ViLBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of ViLBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.v_embeddings.image_embeddings.weight', 'bert.v_embeddings.image_embeddings.bias', 'bert.v_embeddings.image_location_embeddings.weight', 'bert.v_embeddings.image_location_embeddings.bias', 'bert.v_embeddings.LayerNorm.weight', 'bert.v_embeddings.LayerNorm.bias', 'bert.encoder.v_layer.0.attention.self.query.weight', 'bert.encoder.v_layer.0.attention.self.query.bias', 'bert.encoder.v_layer.0.attention.self.key.weight', 'bert.encoder.v_layer.0.attention.self.key.bias', 'bert.encoder.v_layer.0.attention.self.value.weight', 'bert.encoder.v_layer.0.attention.self.value.bias', 'bert.encoder.v_layer.0.attention.output.dense.weight', 'bert.encoder.v_layer.0.attention.output.dense.bias', 'bert.encoder.v_layer.0.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.0.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.0.intermediate.dense.weight', 'bert.encoder.v_layer.0.intermediate.dense.bias', 'bert.encoder.v_layer.0.output.dense.weight', 'bert.encoder.v_layer.0.output.dense.bias', 'bert.encoder.v_layer.0.output.LayerNorm.weight', 'bert.encoder.v_layer.0.output.LayerNorm.bias', 'bert.encoder.v_layer.1.attention.self.query.weight', 'bert.encoder.v_layer.1.attention.self.query.bias', 'bert.encoder.v_layer.1.attention.self.key.weight', 'bert.encoder.v_layer.1.attention.self.key.bias', 'bert.encoder.v_layer.1.attention.self.value.weight', 'bert.encoder.v_layer.1.attention.self.value.bias', 'bert.encoder.v_layer.1.attention.output.dense.weight', 'bert.encoder.v_layer.1.attention.output.dense.bias', 'bert.encoder.v_layer.1.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.1.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.1.intermediate.dense.weight', 'bert.encoder.v_layer.1.intermediate.dense.bias', 'bert.encoder.v_layer.1.output.dense.weight', 'bert.encoder.v_layer.1.output.dense.bias', 'bert.encoder.v_layer.1.output.LayerNorm.weight', 'bert.encoder.v_layer.1.output.LayerNorm.bias', 'bert.encoder.v_layer.2.attention.self.query.weight', 'bert.encoder.v_layer.2.attention.self.query.bias', 'bert.encoder.v_layer.2.attention.self.key.weight', 'bert.encoder.v_layer.2.attention.self.key.bias', 'bert.encoder.v_layer.2.attention.self.value.weight', 'bert.encoder.v_layer.2.attention.self.value.bias', 'bert.encoder.v_layer.2.attention.output.dense.weight', 'bert.encoder.v_layer.2.attention.output.dense.bias', 'bert.encoder.v_layer.2.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.2.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.2.intermediate.dense.weight', 'bert.encoder.v_layer.2.intermediate.dense.bias', 'bert.encoder.v_layer.2.output.dense.weight', 'bert.encoder.v_layer.2.output.dense.bias', 'bert.encoder.v_layer.2.output.LayerNorm.weight', 'bert.encoder.v_layer.2.output.LayerNorm.bias', 'bert.encoder.v_layer.3.attention.self.query.weight', 'bert.encoder.v_layer.3.attention.self.query.bias', 'bert.encoder.v_layer.3.attention.self.key.weight', 'bert.encoder.v_layer.3.attention.self.key.bias', 'bert.encoder.v_layer.3.attention.self.value.weight', 'bert.encoder.v_layer.3.attention.self.value.bias', 'bert.encoder.v_layer.3.attention.output.dense.weight', 'bert.encoder.v_layer.3.attention.output.dense.bias', 'bert.encoder.v_layer.3.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.3.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.3.intermediate.dense.weight', 'bert.encoder.v_layer.3.intermediate.dense.bias', 'bert.encoder.v_layer.3.output.dense.weight', 'bert.encoder.v_layer.3.output.dense.bias', 'bert.encoder.v_layer.3.output.LayerNorm.weight', 'bert.encoder.v_layer.3.output.LayerNorm.bias', 'bert.encoder.v_layer.4.attention.self.query.weight', 'bert.encoder.v_layer.4.attention.self.query.bias', 'bert.encoder.v_layer.4.attention.self.key.weight', 'bert.encoder.v_layer.4.attention.self.key.bias', 'bert.encoder.v_layer.4.attention.self.value.weight', 'bert.encoder.v_layer.4.attention.self.value.bias', 'bert.encoder.v_layer.4.attention.output.dense.weight', 'bert.encoder.v_layer.4.attention.output.dense.bias', 'bert.encoder.v_layer.4.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.4.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.4.intermediate.dense.weight', 'bert.encoder.v_layer.4.intermediate.dense.bias', 'bert.encoder.v_layer.4.output.dense.weight', 'bert.encoder.v_layer.4.output.dense.bias', 'bert.encoder.v_layer.4.output.LayerNorm.weight', 'bert.encoder.v_layer.4.output.LayerNorm.bias', 'bert.encoder.v_layer.5.attention.self.query.weight', 'bert.encoder.v_layer.5.attention.self.query.bias', 'bert.encoder.v_layer.5.attention.self.key.weight', 'bert.encoder.v_layer.5.attention.self.key.bias', 'bert.encoder.v_layer.5.attention.self.value.weight', 'bert.encoder.v_layer.5.attention.self.value.bias', 'bert.encoder.v_layer.5.attention.output.dense.weight', 'bert.encoder.v_layer.5.attention.output.dense.bias', 'bert.encoder.v_layer.5.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.5.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.5.intermediate.dense.weight', 'bert.encoder.v_layer.5.intermediate.dense.bias', 'bert.encoder.v_layer.5.output.dense.weight', 'bert.encoder.v_layer.5.output.dense.bias', 'bert.encoder.v_layer.5.output.LayerNorm.weight', 'bert.encoder.v_layer.5.output.LayerNorm.bias', 'bert.encoder.c_layer.0.biattention.query1.weight', 'bert.encoder.c_layer.0.biattention.query1.bias', 'bert.encoder.c_layer.0.biattention.key1.weight', 'bert.encoder.c_layer.0.biattention.key1.bias', 'bert.encoder.c_layer.0.biattention.value1.weight', 'bert.encoder.c_layer.0.biattention.value1.bias', 'bert.encoder.c_layer.0.biattention.query2.weight', 'bert.encoder.c_layer.0.biattention.query2.bias', 'bert.encoder.c_layer.0.biattention.key2.weight', 'bert.encoder.c_layer.0.biattention.key2.bias', 'bert.encoder.c_layer.0.biattention.value2.weight', 'bert.encoder.c_layer.0.biattention.value2.bias', 'bert.encoder.c_layer.0.biOutput.dense1.weight', 'bert.encoder.c_layer.0.biOutput.dense1.bias', 'bert.encoder.c_layer.0.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.0.biOutput.q_dense1.weight', 'bert.encoder.c_layer.0.biOutput.q_dense1.bias', 'bert.encoder.c_layer.0.biOutput.dense2.weight', 'bert.encoder.c_layer.0.biOutput.dense2.bias', 'bert.encoder.c_layer.0.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.0.biOutput.q_dense2.weight', 'bert.encoder.c_layer.0.biOutput.q_dense2.bias', 'bert.encoder.c_layer.0.v_intermediate.dense.weight', 'bert.encoder.c_layer.0.v_intermediate.dense.bias', 'bert.encoder.c_layer.0.v_output.dense.weight', 'bert.encoder.c_layer.0.v_output.dense.bias', 'bert.encoder.c_layer.0.v_output.LayerNorm.weight', 'bert.encoder.c_layer.0.v_output.LayerNorm.bias', 'bert.encoder.c_layer.0.t_intermediate.dense.weight', 'bert.encoder.c_layer.0.t_intermediate.dense.bias', 'bert.encoder.c_layer.0.t_output.dense.weight', 'bert.encoder.c_layer.0.t_output.dense.bias', 'bert.encoder.c_layer.0.t_output.LayerNorm.weight', 'bert.encoder.c_layer.0.t_output.LayerNorm.bias', 'bert.encoder.c_layer.1.biattention.query1.weight', 'bert.encoder.c_layer.1.biattention.query1.bias', 'bert.encoder.c_layer.1.biattention.key1.weight', 'bert.encoder.c_layer.1.biattention.key1.bias', 'bert.encoder.c_layer.1.biattention.value1.weight', 'bert.encoder.c_layer.1.biattention.value1.bias', 'bert.encoder.c_layer.1.biattention.query2.weight', 'bert.encoder.c_layer.1.biattention.query2.bias', 'bert.encoder.c_layer.1.biattention.key2.weight', 'bert.encoder.c_layer.1.biattention.key2.bias', 'bert.encoder.c_layer.1.biattention.value2.weight', 'bert.encoder.c_layer.1.biattention.value2.bias', 'bert.encoder.c_layer.1.biOutput.dense1.weight', 'bert.encoder.c_layer.1.biOutput.dense1.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.1.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.1.biOutput.q_dense1.weight', 'bert.encoder.c_layer.1.biOutput.q_dense1.bias', 'bert.encoder.c_layer.1.biOutput.dense2.weight', 'bert.encoder.c_layer.1.biOutput.dense2.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.1.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.1.biOutput.q_dense2.weight', 'bert.encoder.c_layer.1.biOutput.q_dense2.bias', 'bert.encoder.c_layer.1.v_intermediate.dense.weight', 'bert.encoder.c_layer.1.v_intermediate.dense.bias', 'bert.encoder.c_layer.1.v_output.dense.weight', 'bert.encoder.c_layer.1.v_output.dense.bias', 'bert.encoder.c_layer.1.v_output.LayerNorm.weight', 'bert.encoder.c_layer.1.v_output.LayerNorm.bias', 'bert.encoder.c_layer.1.t_intermediate.dense.weight', 'bert.encoder.c_layer.1.t_intermediate.dense.bias', 'bert.encoder.c_layer.1.t_output.dense.weight', 'bert.encoder.c_layer.1.t_output.dense.bias', 'bert.encoder.c_layer.1.t_output.LayerNorm.weight', 'bert.encoder.c_layer.1.t_output.LayerNorm.bias', 'bert.encoder.c_layer.2.biattention.query1.weight', 'bert.encoder.c_layer.2.biattention.query1.bias', 'bert.encoder.c_layer.2.biattention.key1.weight', 'bert.encoder.c_layer.2.biattention.key1.bias', 'bert.encoder.c_layer.2.biattention.value1.weight', 'bert.encoder.c_layer.2.biattention.value1.bias', 'bert.encoder.c_layer.2.biattention.query2.weight', 'bert.encoder.c_layer.2.biattention.query2.bias', 'bert.encoder.c_layer.2.biattention.key2.weight', 'bert.encoder.c_layer.2.biattention.key2.bias', 'bert.encoder.c_layer.2.biattention.value2.weight', 'bert.encoder.c_layer.2.biattention.value2.bias', 'bert.encoder.c_layer.2.biOutput.dense1.weight', 'bert.encoder.c_layer.2.biOutput.dense1.bias', 'bert.encoder.c_layer.2.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.2.biOutput.q_dense1.weight', 'bert.encoder.c_layer.2.biOutput.q_dense1.bias', 'bert.encoder.c_layer.2.biOutput.dense2.weight', 'bert.encoder.c_layer.2.biOutput.dense2.bias', 'bert.encoder.c_layer.2.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.2.biOutput.q_dense2.weight', 'bert.encoder.c_layer.2.biOutput.q_dense2.bias', 'bert.encoder.c_layer.2.v_intermediate.dense.weight', 'bert.encoder.c_layer.2.v_intermediate.dense.bias', 'bert.encoder.c_layer.2.v_output.dense.weight', 'bert.encoder.c_layer.2.v_output.dense.bias', 'bert.encoder.c_layer.2.v_output.LayerNorm.weight', 'bert.encoder.c_layer.2.v_output.LayerNorm.bias', 'bert.encoder.c_layer.2.t_intermediate.dense.weight', 'bert.encoder.c_layer.2.t_intermediate.dense.bias', 'bert.encoder.c_layer.2.t_output.dense.weight', 'bert.encoder.c_layer.2.t_output.dense.bias', 'bert.encoder.c_layer.2.t_output.LayerNorm.weight', 'bert.encoder.c_layer.2.t_output.LayerNorm.bias', 'bert.encoder.c_layer.3.biattention.query1.weight', 'bert.encoder.c_layer.3.biattention.query1.bias', 'bert.encoder.c_layer.3.biattention.key1.weight', 'bert.encoder.c_layer.3.biattention.key1.bias', 'bert.encoder.c_layer.3.biattention.value1.weight', 'bert.encoder.c_layer.3.biattention.value1.bias', 'bert.encoder.c_layer.3.biattention.query2.weight', 'bert.encoder.c_layer.3.biattention.query2.bias', 'bert.encoder.c_layer.3.biattention.key2.weight', 'bert.encoder.c_layer.3.biattention.key2.bias', 'bert.encoder.c_layer.3.biattention.value2.weight', 'bert.encoder.c_layer.3.biattention.value2.bias', 'bert.encoder.c_layer.3.biOutput.dense1.weight', 'bert.encoder.c_layer.3.biOutput.dense1.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.3.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.3.biOutput.q_dense1.weight', 'bert.encoder.c_layer.3.biOutput.q_dense1.bias', 'bert.encoder.c_layer.3.biOutput.dense2.weight', 'bert.encoder.c_layer.3.biOutput.dense2.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.3.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.3.biOutput.q_dense2.weight', 'bert.encoder.c_layer.3.biOutput.q_dense2.bias', 'bert.encoder.c_layer.3.v_intermediate.dense.weight', 'bert.encoder.c_layer.3.v_intermediate.dense.bias', 'bert.encoder.c_layer.3.v_output.dense.weight', 'bert.encoder.c_layer.3.v_output.dense.bias', 'bert.encoder.c_layer.3.v_output.LayerNorm.weight', 'bert.encoder.c_layer.3.v_output.LayerNorm.bias', 'bert.encoder.c_layer.3.t_intermediate.dense.weight', 'bert.encoder.c_layer.3.t_intermediate.dense.bias', 'bert.encoder.c_layer.3.t_output.dense.weight', 'bert.encoder.c_layer.3.t_output.dense.bias', 'bert.encoder.c_layer.3.t_output.LayerNorm.weight', 'bert.encoder.c_layer.3.t_output.LayerNorm.bias', 'bert.encoder.c_layer.4.biattention.query1.weight', 'bert.encoder.c_layer.4.biattention.query1.bias', 'bert.encoder.c_layer.4.biattention.key1.weight', 'bert.encoder.c_layer.4.biattention.key1.bias', 'bert.encoder.c_layer.4.biattention.value1.weight', 'bert.encoder.c_layer.4.biattention.value1.bias', 'bert.encoder.c_layer.4.biattention.query2.weight', 'bert.encoder.c_layer.4.biattention.query2.bias', 'bert.encoder.c_layer.4.biattention.key2.weight', 'bert.encoder.c_layer.4.biattention.key2.bias', 'bert.encoder.c_layer.4.biattention.value2.weight', 'bert.encoder.c_layer.4.biattention.value2.bias', 'bert.encoder.c_layer.4.biOutput.dense1.weight', 'bert.encoder.c_layer.4.biOutput.dense1.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.4.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.4.biOutput.q_dense1.weight', 'bert.encoder.c_layer.4.biOutput.q_dense1.bias', 'bert.encoder.c_layer.4.biOutput.dense2.weight', 'bert.encoder.c_layer.4.biOutput.dense2.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.4.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.4.biOutput.q_dense2.weight', 'bert.encoder.c_layer.4.biOutput.q_dense2.bias', 'bert.encoder.c_layer.4.v_intermediate.dense.weight', 'bert.encoder.c_layer.4.v_intermediate.dense.bias', 'bert.encoder.c_layer.4.v_output.dense.weight', 'bert.encoder.c_layer.4.v_output.dense.bias', 'bert.encoder.c_layer.4.v_output.LayerNorm.weight', 'bert.encoder.c_layer.4.v_output.LayerNorm.bias', 'bert.encoder.c_layer.4.t_intermediate.dense.weight', 'bert.encoder.c_layer.4.t_intermediate.dense.bias', 'bert.encoder.c_layer.4.t_output.dense.weight', 'bert.encoder.c_layer.4.t_output.dense.bias', 'bert.encoder.c_layer.4.t_output.LayerNorm.weight', 'bert.encoder.c_layer.4.t_output.LayerNorm.bias', 'bert.encoder.c_layer.5.biattention.query1.weight', 'bert.encoder.c_layer.5.biattention.query1.bias', 'bert.encoder.c_layer.5.biattention.key1.weight', 'bert.encoder.c_layer.5.biattention.key1.bias', 'bert.encoder.c_layer.5.biattention.value1.weight', 'bert.encoder.c_layer.5.biattention.value1.bias', 'bert.encoder.c_layer.5.biattention.query2.weight', 'bert.encoder.c_layer.5.biattention.query2.bias', 'bert.encoder.c_layer.5.biattention.key2.weight', 'bert.encoder.c_layer.5.biattention.key2.bias', 'bert.encoder.c_layer.5.biattention.value2.weight', 'bert.encoder.c_layer.5.biattention.value2.bias', 'bert.encoder.c_layer.5.biOutput.dense1.weight', 'bert.encoder.c_layer.5.biOutput.dense1.bias', 'bert.encoder.c_layer.5.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.5.biOutput.q_dense1.weight', 'bert.encoder.c_layer.5.biOutput.q_dense1.bias', 'bert.encoder.c_layer.5.biOutput.dense2.weight', 'bert.encoder.c_layer.5.biOutput.dense2.bias', 'bert.encoder.c_layer.5.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.5.biOutput.q_dense2.weight', 'bert.encoder.c_layer.5.biOutput.q_dense2.bias', 'bert.encoder.c_layer.5.v_intermediate.dense.weight', 'bert.encoder.c_layer.5.v_intermediate.dense.bias', 'bert.encoder.c_layer.5.v_output.dense.weight', 'bert.encoder.c_layer.5.v_output.dense.bias', 'bert.encoder.c_layer.5.v_output.LayerNorm.weight', 'bert.encoder.c_layer.5.v_output.LayerNorm.bias', 'bert.encoder.c_layer.5.t_intermediate.dense.weight', 'bert.encoder.c_layer.5.t_intermediate.dense.bias', 'bert.encoder.c_layer.5.t_output.dense.weight', 'bert.encoder.c_layer.5.t_output.dense.bias', 'bert.encoder.c_layer.5.t_output.LayerNorm.weight', 'bert.encoder.c_layer.5.t_output.LayerNorm.bias', 'bert.t_pooler.dense.weight', 'bert.t_pooler.dense.bias', 'bert.v_pooler.dense.weight', 'bert.v_pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[32m2021-04-29T22:44:08 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n","\u001b[32m2021-04-29T22:44:08 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-29T22:44:08 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/basecontainer.py:232: UserWarning: cfg.pretty() is deprecated and will be removed in a future version.\n","Use OmegaConf.to_yaml(cfg)\n","\n","  category=UserWarning,\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-29T22:44:08 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/basecontainer.py:232: UserWarning: cfg.pretty() is deprecated and will be removed in a future version.\n","Use OmegaConf.to_yaml(cfg)\n","\n","  category=UserWarning,\n","\n","\u001b[32m2021-04-29T22:44:08 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-29T22:44:38 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-29T22:44:38 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n","\u001b[32m2021-04-29T22:44:38 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n","\u001b[32m2021-04-29T22:44:38 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 1000\n","\u001b[32m2021-04-29T22:44:38 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 1000\n","\u001b[32m2021-04-29T22:44:38 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 2\n","\u001b[32m2021-04-29T22:44:38 | mmf.trainers.core.evaluation_loop: \u001b[0mStarting val inference predictions\n","\u001b[32m2021-04-29T22:44:38 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n","100% 32/32 [00:42<00:00,  1.32s/it]\n","\u001b[32m2021-04-29T22:45:21 | mmf.common.test_reporter: \u001b[0mWrote predictions for hateful_memes to /content/drive/MyDrive/CS7643/vilbert_tune/hateful_memes_vilbert_29438233/reports/hateful_memes_run_val_2021-04-29T22:45:21.csv\n","\u001b[32m2021-04-29T22:45:21 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished predicting. Loaded 32\n","\u001b[32m2021-04-29T22:45:21 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"u_793CbuwRgZ"},"source":["# predictions on the test set\n","!mmf_predict config=\"$path\"/vilbert_tune/config.yaml \\\n","  model=vilbert \\\n","  dataset=hateful_memes \\\n","  run_type=test \\\n","  checkpoint.resume_file=\"$path\"/vilbert_tune/best.ckpt \\\n","  checkpoint.resume_pretrained=False \\\n","  dataset_config.hateful_memes.annotations.val[0]=hateful_memes/defaults/annotations/dev_seen.jsonl \\\n","  dataset_config.hateful_memes.annotations.test[0]=hateful_memes/defaults/annotations/test_seen.jsonl"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mDDnTlQZV46f"},"source":["## Train an existing model\n","\n","We will use MMF to train an existing baseline from MMF's model zoo on the Hateful Memes dataset. Run the next code cell to start training MMBT-Grid model on the dataset. You can adjust the batch size, maximum number of updates, log and evaluation interval among other things by using command line overrides. Read more about MMF's configuration system at https://github.com/facebookresearch/mmf/tree/master/projects/hateful_memes#reproducing-baselines"]},{"cell_type":"code","metadata":{"id":"1nwebqtdWOfZ"},"source":["!mmf_run config=projects/hateful_memes/configs/mmbt/defaults.yaml \\\n","  model=mmbt \\\n","  dataset=hateful_memes \\\n","  training.log_interval=50 \\\n","  training.max_updates=3000 \\\n","  training.batch_size=16 \\\n","  training.evaluation_interval=500"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1mB-z-6XWdBd"},"source":["## Build your own model\n","\n","Using MMF's encoders, modules and utilities, we can easily build a custom model. In this example, we are building a fusion model which fuses ResNet pooled grid features with fasttext embedding vectors to classify a meme as hateful or not hateful. \n","\n","Steps involved in building the model are:\n","\n","1. Create a new processor to get fasttext sentence embeddings. (Read more on processors [here]())\n","2. Create new model using encoders from MMF.\n","3. Move hardcoded stuff from model to configuration."]},{"cell_type":"code","metadata":{"id":"x2yjX5JxIu2C"},"source":["import torch \n","\n","# We will inherit the FastText Processor already present in MMF\n","from mmf.datasets.processors import FastTextProcessor\n","# registry is needed to register processor and model to be MMF discoverable\n","from mmf.common.registry import registry\n","\n","# Register the processor so that MMF can discover it\n","@registry.register_processor(\"fasttext_sentence_vector\")\n","class FastTextSentenceVectorProcessor(FastTextProcessor):\n","    # Override the call method\n","    def __call__(self, item):\n","        # This function is present in FastTextProcessor class and loads\n","        # fasttext bin\n","        self._load_fasttext_model(self.model_file)\n","        if \"text\" in item:\n","            text = item[\"text\"]\n","        elif \"tokens\" in item:\n","            text = \" \".join(item[\"tokens\"])\n","\n","        # Get a sentence vector for sentence and convert it to torch tensor\n","        sentence_vector = torch.tensor(\n","            self.model.get_sentence_vector(text),\n","            dtype=torch.float\n","        )\n","\n","        # Return back a dict\n","        return {\n","            \"text\": sentence_vector\n","        }\n","    \n","    # Make dataset builder happy, return a random number\n","    def get_vocab_size(self):\n","        return None"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rlB4n0nwKWZn"},"source":["import torch\n","\n","# registry is need to register our new model so as to be MMF discoverable\n","from mmf.common.registry import registry\n","# All model using MMF need to inherit BaseModel\n","from mmf.models.base_model import BaseModel\n","# ProjectionEmbedding will act as proxy encoder for FastText Sentence Vector\n","from mmf.modules.embeddings import ProjectionEmbedding\n","# Builder methods for image encoder and classifier\n","from mmf.utils.build import build_classifier_layer, build_image_encoder\n","\n","# Register the model for MMF, \"concat_vl\" key would be used to find the model\n","@registry.register_model(\"concat_vl\")\n","class LanguageAndVisionConcat(BaseModel):\n","    # All models in MMF get first argument as config which contains all\n","    # of the information you stored in this model's config (hyperparameters)\n","    def __init__(self, config, *args, **kwargs):\n","        # This is not needed in most cases as it just calling parent's init\n","        # with same parameters. But to explain how config is initialized we \n","        # have kept this\n","        super().__init__(config, *args, **kwargs)\n","    \n","    # This classmethod tells MMF where to look for default config of this model\n","    @classmethod\n","    def config_path(cls):\n","        # Relative to user dir root\n","        return \"/content/hm_example_mmf/configs/models/concat_vl.yaml\"\n","    \n","    # Each method need to define a build method where the model's modules\n","    # are actually build and assigned to the model\n","    def build(self):\n","        \"\"\"\n","        Config's image_encoder attribute will used to build an MMF image\n","        encoder. This config in yaml will look like:\n","\n","        # \"type\" parameter specifies the type of encoder we are using here. \n","        # In this particular case, we are using resnet152\n","        type: resnet152\n","      \n","        # Parameters are passed to underlying encoder class by \n","        # build_image_encoder\n","        params:\n","          # Specifies whether to use a pretrained version\n","          pretrained: true \n","          # Pooling type, use max to use AdaptiveMaxPool2D\n","          pool_type: avg \n","      \n","          # Number of output features from the encoder, -1 for original\n","          # otherwise, supports between 1 to 9\n","          num_output_features: 1 \n","        \"\"\"\n","        self.vision_module = build_image_encoder(self.config.image_encoder)\n","\n","        \"\"\"\n","        For classifer, configuration would look like:\n","        # Specifies the type of the classifier, in this case mlp\n","        type: mlp\n","        # Parameter to the classifier passed through build_classifier_layer\n","        params:\n","          # Dimension of the tensor coming into the classifier\n","          in_dim: 512\n","          # Dimension of the tensor going out of the classifier\n","          out_dim: 2\n","          # Number of MLP layers in the classifier\n","          num_layers: 0\n","        \"\"\"\n","        self.classifier = build_classifier_layer(self.config.classifier)\n","        \n","        # ProjectionEmbeddings takes in params directly as it is module\n","        # So, pass in kwargs, which are in_dim, out_dim and module\n","        # whose value would be \"linear\" as we want linear layer\n","        self.language_module = ProjectionEmbedding(\n","            **self.config.text_encoder.params\n","        )\n","        # Dropout value will come from config now\n","        self.dropout = torch.nn.Dropout(self.config.dropout)\n","        # Same as Projection Embedding, fusion's layer params (which are param \n","        # for linear layer) will come from config now\n","        self.fusion = torch.nn.Linear(**self.config.fusion.params)\n","        self.relu = torch.nn.ReLU()\n","\n","    # Each model in MMF gets a dict called sample_list which contains\n","    # all of the necessary information returned from the image\n","    def forward(self, sample_list):\n","        # Text input features will be in \"text\" key\n","        text = sample_list[\"text\"]\n","        # Similarly, image input will be in \"image\" key\n","        image = sample_list[\"image\"]\n","\n","        text_features = self.relu(self.language_module(text))\n","        image_features = self.relu(self.vision_module(image))\n","\n","        # Concatenate the features returned from two modality encoders\n","        combined = torch.cat([text_features, image_features.squeeze()], dim=1)\n","\n","        # Pass through the fusion layer, relu and dropout\n","        fused = self.dropout(self.relu(self.fusion(combined)))\n","\n","        # Pass final tensor from classifier to get scores\n","        logits = self.classifier(fused)\n","\n","        # For loss calculations (automatically done by MMF based on loss defined\n","        # in the config), we need to return a dict with \"scores\" key as logits\n","        output = {\"scores\": logits}\n","\n","        # MMF will automatically calculate loss\n","        return output"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G8VCzWStDwkJ"},"source":["Now, we will install the example repo that we have already created on top of MMF and contains code in this colab. We do this so that we don't have to build configs again from scratch"]},{"cell_type":"code","metadata":{"id":"bjvxZYBXTrRG"},"source":["!git clone https://github.com/apsdehal/hm_example_mmf /content/hm_example_mmf"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"taXGqCxQXJbo"},"source":["## Train your model\n","\n","In this step, we will train the model we just built. A dot list can be passed as either a dict or a list to the run to override the configuration parameters."]},{"cell_type":"code","metadata":{"id":"9Aci1mtsURL9"},"source":["import sys\n","from mmf_cli.run import run\n","opts = opts=[\n","    \"config='/content/hm_example_mmf/configs/experiments/defaults.yaml'\", \n","    \"model=concat_vl\", \n","    \"dataset=hateful_memes\", \n","    \"training.num_workers=0\"\n","]\n","run(opts=opts)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lTAshObTm1ey"},"source":["## Using your module\n","\n","Since, we have cloned the repo that contains the example we built in this colab notebook we can use it also to run the training from command line by using the `env.user_dir` option or by overriding the environment variable `MMF_USER_DIR`. Expand the cell below the next code cell to see how it can be done."]},{"cell_type":"code","metadata":{"id":"Rwggw7XLUYuO"},"source":["!MMF_USER_DIR=\"/content/hm_example_mmf\" mmf_run \\\n","  config=\"configs/experiments/defaults.yaml\" \\\n","  model=concat_vl \\\n","  dataset=hateful_memes \\\n","  training.num_workers=0"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q-Y2aJC_nRpy"},"source":["## Conclusion and Further Steps\n","\n","In this colab notebook, we learned how we can use MMF to train and predict already existing models in MMF's zoo. We also learned how we can build custom models using various modules and goodies provided in MMF easily.\n","\n","If you have any issues, feedback or comments, please reach us out at mmf@fb.com or open up an issue at [GitHub](https://github.com/facebookresearch/mmf/issues/new/choose). We are also accepting PRs if you want to add your cool model to MMF and we are always open to community contributions.\n","\n","At Facebook AI, we’ll continuously improve and expand on the multimodal capabilities available through MMF, and we welcome contributions from the community as well to build this resource. We hope MMF will be the framework of choice and be a catalyst for research in this area by providing a powerful, versatile platform for multimodal research. "]}]}