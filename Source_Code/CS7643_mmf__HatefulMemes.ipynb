{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "CS7643_mmf_ HatefulMemes.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f8f354d41290471bb16e594edb96ef4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f9f889609c9146ed8ccdfcfedb8e75eb",
              "IPY_MODEL_5ebb862f16e642fa8f24ffdc8b3d14cb"
            ],
            "layout": "IPY_MODEL_d444436836164d2cb470521725b0b169"
          }
        },
        "f9f889609c9146ed8ccdfcfedb8e75eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4876536620aa44a4a0b448f554d3b1f5",
            "max": 241530880,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_45f46f25466840c5b6653d05e395abfb",
            "value": 241530880
          }
        },
        "5ebb862f16e642fa8f24ffdc8b3d14cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff03394f803d4033978dac3cc947195d",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_15bfadcb8dba41879e4b84a7f0a08ea3",
            "value": " 230M/230M [33:47&lt;00:00, 119kB/s]"
          }
        },
        "d444436836164d2cb470521725b0b169": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4876536620aa44a4a0b448f554d3b1f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45f46f25466840c5b6653d05e395abfb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "ff03394f803d4033978dac3cc947195d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15bfadcb8dba41879e4b84a7f0a08ea3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwCc6zN1BkIE"
      },
      "source": [
        "# This notebook is created based on the MMF Colab Demo template - https://colab.research.google.com/github/facebookresearch/mmf/blob/notebooks/notebooks/mmf_hm_example.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7Eo9ZqTDW3I"
      },
      "source": [
        "## Download MMF\n",
        "\n",
        "In this section, we will download the MMF package and required dependencies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTvGiwu5aE91"
      },
      "source": [
        "### Prerequisites \n",
        "Please enable GPU in this notebook: Runtime > Change runtime type > Hardware Accelerator > Set to GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uxh_vli1Drky"
      },
      "source": [
        "First we will install the MMF package and required dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JrgCvqorS2j8",
        "outputId": "6ed1430a-86f9-4d9b-b886-469d6a3d41da"
      },
      "source": [
        "#!pip install --pre --upgrade mmf\n",
        "\n",
        "# Remove the pre-installed mmf\n",
        "!rm -rf /content/mmf/\n",
        "!rm -rf /root/.cache/torch/mmf/\n",
        "\n",
        "# Update PyYAML and imagug\n",
        "!pip uninstall -y PyYAML\n",
        "!pip install PyYAML==5.1.2\n",
        "!pip uninstall -y imgaug && pip uninstall -y albumentations && pip install git+https://github.com/aleju/imgaug.git\n",
        "\n",
        "# Install mmf from source\n",
        "!git clone https://github.com/facebookresearch/mmf.git\n",
        "!cd /content/mmf/\n",
        "!pip install --editable /content/mmf/"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling PyYAML-3.13:\n",
            "  Successfully uninstalled PyYAML-3.13\n",
            "Collecting PyYAML==5.1.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/e8/b3212641ee2718d556df0f23f78de8303f068fe29cdaa7a91018849582fe/PyYAML-5.1.2.tar.gz (265kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 266kB 11.4MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: PyYAML\n",
            "  Building wheel for PyYAML (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyYAML: filename=PyYAML-5.1.2-cp37-cp37m-linux_x86_64.whl size=44103 sha256=d73d574bd620659f00dc6edcbbd45968761cf7436ea2c85a62bae0162ac1ecf2\n",
            "  Stored in directory: /root/.cache/pip/wheels/d9/45/dd/65f0b38450c47cf7e5312883deb97d065e030c5cca0a365030\n",
            "Successfully built PyYAML\n",
            "Installing collected packages: PyYAML\n",
            "Successfully installed PyYAML-5.1.2\n",
            "Uninstalling imgaug-0.2.9:\n",
            "  Successfully uninstalled imgaug-0.2.9\n",
            "Uninstalling albumentations-0.1.12:\n",
            "  Successfully uninstalled albumentations-0.1.12\n",
            "Collecting git+https://github.com/aleju/imgaug.git\n",
            "  Cloning https://github.com/aleju/imgaug.git to /tmp/pip-req-build-veo6pl7e\n",
            "  Running command git clone -q https://github.com/aleju/imgaug.git /tmp/pip-req-build-veo6pl7e\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from imgaug==0.4.0) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from imgaug==0.4.0) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from imgaug==0.4.0) (1.4.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from imgaug==0.4.0) (7.1.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from imgaug==0.4.0) (3.2.2)\n",
            "Requirement already satisfied: scikit-image>=0.14.2 in /usr/local/lib/python3.7/dist-packages (from imgaug==0.4.0) (0.16.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from imgaug==0.4.0) (4.1.2.30)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.7/dist-packages (from imgaug==0.4.0) (1.7.1)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.7/dist-packages (from imgaug==0.4.0) (2.4.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug==0.4.0) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug==0.4.0) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug==0.4.0) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug==0.4.0) (1.3.1)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->imgaug==0.4.0) (1.1.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->imgaug==0.4.0) (2.5.1)\n",
            "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.0->scikit-image>=0.14.2->imgaug==0.4.0) (4.4.2)\n",
            "Building wheels for collected packages: imgaug\n",
            "  Building wheel for imgaug (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for imgaug: filename=imgaug-0.4.0-cp37-none-any.whl size=971106 sha256=822acfcbf51a16f8e0d809d667c61fb7894a3ecc925e53291280a60881e97b5a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-h1w2jgr9/wheels/65/3d/94/ee32cbeaa29c473a4db74c2d21904ac747311fdca4732665f0\n",
            "Successfully built imgaug\n",
            "Installing collected packages: imgaug\n",
            "Successfully installed imgaug-0.4.0\n",
            "Cloning into 'mmf'...\n",
            "remote: Enumerating objects: 19000, done.\u001b[K\n",
            "remote: Counting objects: 100% (1106/1106), done.\u001b[K\n",
            "remote: Compressing objects: 100% (556/556), done.\u001b[K\n",
            "remote: Total 19000 (delta 608), reused 930 (delta 503), pack-reused 17894\u001b[K\n",
            "Receiving objects: 100% (19000/19000), 14.59 MiB | 19.50 MiB/s, done.\n",
            "Resolving deltas: 100% (11966/11966), done.\n",
            "Obtaining file:///content/mmf\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting lmdb==0.98\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c0/5c/d56dbc2532ecf14fa004c543927500c0f645eaca8bd7ec39420c7546396a/lmdb-0.98.tar.gz (869kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 870kB 10.6MB/s \n",
            "\u001b[?25hCollecting tqdm<4.50.0,>=4.43.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/d5/f220e0c69b2f346b5649b66abebb391df1a00a59997a7ccf823325bd7a3e/tqdm-4.49.0-py2.py3-none-any.whl (69kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71kB 7.8MB/s \n",
            "\u001b[?25hCollecting datasets==1.2.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/06/9b/d097f2238fc3c028495cf5f8c65378972b9f1b2cbb27f3c57c7219195aa9/datasets-1.2.1-py3-none-any.whl (159kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 163kB 12.9MB/s \n",
            "\u001b[?25hCollecting GitPython==3.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d3/2f/6a366d56c9b1355b0880be9ea66b166cb3536392638d8d91413ec66305ad/GitPython-3.1.0-py3-none-any.whl (450kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 460kB 14.9MB/s \n",
            "\u001b[?25hCollecting transformers==3.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2c/4e/4f1ede0fd7a36278844a277f8d53c21f88f37f3754abf76a5d6224f76d4a/transformers-3.4.0-py3-none-any.whl (1.3MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.3MB 40.6MB/s \n",
            "\u001b[?25hCollecting torchvision<=0.9.1,>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/93/8a/82062a33b5eb7f696bf23f8ccf04bf6fc81d1a4972740fb21c2569ada0a6/torchvision-0.9.1-cp37-cp37m-manylinux1_x86_64.whl (17.4MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17.4MB 333kB/s \n",
            "\u001b[?25hRequirement already satisfied: pycocotools==2.0.2 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (2.0.2)\n",
            "Collecting fasttext==0.9.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/61/2e01f1397ec533756c1d893c22d9d5ed3fce3a6e4af1976e0d86bb13ea97/fasttext-0.9.1.tar.gz (57kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61kB 7.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (1.19.5)\n",
            "Collecting torchtext==0.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/ef/54b8da26f37787f5c670ae2199329e7dccf195c060b25628d99e587dac51/torchtext-0.5.0-py3-none-any.whl (73kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 81kB 8.1MB/s \n",
            "\u001b[?25hCollecting nltk==3.4.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/1d/d925cfb4f324ede997f6d47bea4d9babba51b49e87a767c170b77005889d/nltk-3.4.5.zip (1.5MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.5MB 34.0MB/s \n",
            "\u001b[?25hCollecting torch<=1.8.1,>=1.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/74/6fc9dee50f7c93d6b7d9644554bdc9692f3023fa5d1de779666e6bf8ae76/torch-1.8.1-cp37-cp37m-manylinux1_x86_64.whl (804.1MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 804.1MB 23kB/s \n",
            "\u001b[?25hRequirement already satisfied: editdistance==0.5.3 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (0.5.3)\n",
            "Collecting iopath==0.1.7\n",
            "  Downloading https://files.pythonhosted.org/packages/e3/d5/1c70fea7632640e8a9fb5a176676e555238119b3e7ee8b6dc49980ec5769/iopath-0.1.7-py3-none-any.whl\n",
            "Requirement already satisfied: requests==2.23.0 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (2.23.0)\n",
            "Collecting matplotlib==3.3.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/23/3d/db9a6b3c83c9511301152dbb64a029c3a4313c86eaef12c237b13ecf91d6/matplotlib-3.3.4-cp37-cp37m-manylinux1_x86_64.whl (11.5MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11.6MB 21.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor==1.1.0 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (1.1.0)\n",
            "Requirement already satisfied: sklearn==0.0 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (0.0)\n",
            "Collecting omegaconf==2.0.6\n",
            "  Downloading https://files.pythonhosted.org/packages/d0/eb/9d63ce09dd8aa85767c65668d5414958ea29648a0eec80a4a7d311ec2684/omegaconf-2.0.6-py3-none-any.whl\n",
            "Collecting pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@7a48db5\n",
            "  Cloning https://github.com/PyTorchLightning/pytorch-lightning (to revision 7a48db5) to /tmp/pip-install-9zosj7oi/pytorch-lightning\n",
            "  Running command git clone -q https://github.com/PyTorchLightning/pytorch-lightning /tmp/pip-install-9zosj7oi/pytorch-lightning\n",
            "\u001b[33m  WARNING: Did not find branch or tag '7a48db5', assuming revision or ref.\u001b[0m\n",
            "  Running command git checkout -q 7a48db5\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting demjson==2.2.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/96/67/6db789e2533158963d4af689f961b644ddd9200615b8ce92d6cad695c65a/demjson-2.2.4.tar.gz (131kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 133kB 40.1MB/s \n",
            "\u001b[?25hCollecting ftfy==5.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/e2/3b51c53dffb1e52d9210ebc01f1fb9f2f6eba9b3201fa971fd3946643c71/ftfy-5.8.tar.gz (64kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71kB 7.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1->mmf==1.0.0rc12) (0.3.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1->mmf==1.0.0rc12) (1.1.5)\n",
            "Collecting xxhash\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/4f/0a862cad26aa2ed7a7cd87178cbbfa824fc1383e472d63596a0d018374e7/xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 245kB 40.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1->mmf==1.0.0rc12) (3.10.1)\n",
            "Requirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1->mmf==1.0.0rc12) (3.0.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1->mmf==1.0.0rc12) (0.70.11.1)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/e8/f414d1a4f0bbc668ed441f74f44c116d9816833a48bf81d22b697090dba8/gitdb-4.0.7-py3-none-any.whl (63kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71kB 7.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0->mmf==1.0.0rc12) (3.12.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0->mmf==1.0.0rc12) (3.0.12)\n",
            "Collecting tokenizers==0.9.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/e7/edf655ae34925aeaefb7b7fcc3dd0887d2a1203ee6b0df4d1170d1a19d4f/tokenizers-0.9.2-cp37-cp37m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.9MB 29.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0->mmf==1.0.0rc12) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 901kB 38.2MB/s \n",
            "\u001b[?25hCollecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.2MB 35.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0->mmf==1.0.0rc12) (20.9)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision<=0.9.1,>=0.7.0->mmf==1.0.0rc12) (7.1.2)\n",
            "Requirement already satisfied: cython>=0.27.3 in /usr/local/lib/python3.7/dist-packages (from pycocotools==2.0.2->mmf==1.0.0rc12) (0.29.22)\n",
            "Requirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.7/dist-packages (from pycocotools==2.0.2->mmf==1.0.0rc12) (56.0.0)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.7/dist-packages (from fasttext==0.9.1->mmf==1.0.0rc12) (2.6.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchtext==0.5.0->mmf==1.0.0rc12) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<=1.8.1,>=1.6.0->mmf==1.0.0rc12) (3.7.4.3)\n",
            "Collecting portalocker\n",
            "  Downloading https://files.pythonhosted.org/packages/68/33/cb524f4de298509927b90aa5ee34767b9a2b93e663cf354b2a3efa2b4acd/portalocker-2.3.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests==2.23.0->mmf==1.0.0rc12) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests==2.23.0->mmf==1.0.0rc12) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests==2.23.0->mmf==1.0.0rc12) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests==2.23.0->mmf==1.0.0rc12) (1.24.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.4->mmf==1.0.0rc12) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.4->mmf==1.0.0rc12) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.4->mmf==1.0.0rc12) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.4->mmf==1.0.0rc12) (0.10.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn==0.0->mmf==1.0.0rc12) (0.22.2.post1)\n",
            "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.7/dist-packages (from omegaconf==2.0.6->mmf==1.0.0rc12) (5.1.2)\n",
            "Requirement already satisfied: tensorboard!=2.5.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@7a48db5->mmf==1.0.0rc12) (2.4.1)\n",
            "Collecting fsspec[http]>=2021.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/91/2ef649137816850fa4f4c97c6f2eabb1a79bf0aa2c8ed198e387e373455e/fsspec-2021.4.0-py3-none-any.whl (108kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 112kB 42.1MB/s \n",
            "\u001b[?25hCollecting pyDeprecate==0.3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/14/52/aa227a0884df71ed1957649085adf2b8bc2a1816d037c2f18b3078854516/pyDeprecate-0.3.0-py3-none-any.whl\n",
            "Collecting torchmetrics>=0.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/99/dc59248df9a50349d537ffb3403c1bdc1fa69077109d46feaa0843488001/torchmetrics-0.3.1-py3-none-any.whl (271kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 276kB 40.2MB/s \n",
            "\u001b[?25hCollecting future>=0.17.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 829kB 40.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy==5.8->mmf==1.0.0rc12) (0.2.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets==1.2.1->mmf==1.0.0rc12) (2018.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets==1.2.1->mmf==1.0.0rc12) (3.4.1)\n",
            "Collecting smmap<5,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/68/ee/d540eb5e5996eb81c26ceffac6ee49041d473bc5125f2aa995cf51ec1cf1/smmap-4.0.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.4.0->mmf==1.0.0rc12) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.4.0->mmf==1.0.0rc12) (1.0.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn==0.0->mmf==1.0.0rc12) (1.4.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@7a48db5->mmf==1.0.0rc12) (1.32.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@7a48db5->mmf==1.0.0rc12) (1.0.1)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@7a48db5->mmf==1.0.0rc12) (0.36.2)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@7a48db5->mmf==1.0.0rc12) (0.12.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@7a48db5->mmf==1.0.0rc12) (1.28.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@7a48db5->mmf==1.0.0rc12) (0.4.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@7a48db5->mmf==1.0.0rc12) (1.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@7a48db5->mmf==1.0.0rc12) (3.3.4)\n",
            "Collecting aiohttp; extra == \"http\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/c0/5890b4c8b04a79b7360e8fe4490feb0bb3ab179743f199f0e6220cebd568/aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.3MB 35.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@7a48db5->mmf==1.0.0rc12) (4.2.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@7a48db5->mmf==1.0.0rc12) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@7a48db5->mmf==1.0.0rc12) (4.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@7a48db5->mmf==1.0.0rc12) (1.3.0)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f1/62/046834c5fc998c88ab2ef722f5d42122230a632212c8afa76418324f53ff/yarl-1.6.3-cp37-cp37m-manylinux2014_x86_64.whl (294kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 296kB 32.6MB/s \n",
            "\u001b[?25hCollecting async-timeout<4.0,>=3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e1/1e/5a4441be21b0726c4464f3f23c8b19628372f606755a9d2e46c187e65ec4/async_timeout-3.0.1-py3-none-any.whl\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=2021.4.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@7a48db5->mmf==1.0.0rc12) (20.3.0)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/a6/4123b8165acbe773d1a8dc8e3f0d1edea16d29f7de018eda769abb56bd30/multidict-5.1.0-cp37-cp37m-manylinux2014_x86_64.whl (142kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143kB 43.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@7a48db5->mmf==1.0.0rc12) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning@ git+https://github.com/PyTorchLightning/pytorch-lightning@7a48db5->mmf==1.0.0rc12) (3.1.0)\n",
            "Building wheels for collected packages: pytorch-lightning\n",
            "  Building wheel for pytorch-lightning (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytorch-lightning: filename=pytorch_lightning-1.3.0rc2-cp37-none-any.whl size=796717 sha256=f00470442bcde017e2b67cfcc1d3ef080578ac854117e257a8b332e4ab8b9645\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-uh8b_h30/wheels/c7/f2/4f/cc90dc40152a3d5d29cb8b9b7ee83f1af9f72fe9af306cbcc2\n",
            "Successfully built pytorch-lightning\n",
            "Building wheels for collected packages: lmdb, fasttext, nltk, demjson, ftfy, future\n",
            "  Building wheel for lmdb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lmdb: filename=lmdb-0.98-cp37-cp37m-linux_x86_64.whl size=219692 sha256=88eba6a703079c6b134ac7575105cce6a080cebcbdb3f31df94598b4a638357c\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/97/8c/7721e4b6b0ac723c6cc45ecca60599a80f75e2367330647390\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.1-cp37-cp37m-linux_x86_64.whl size=2462650 sha256=4c3c2eab0683648959ec642b64546489a16cbabd8ee754d36e2b0c2d1c0abc0e\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/f0/04/caa82c912aee89ce76358ff954f3f0729b7577c8ff23a292e3\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.4.5-cp37-none-any.whl size=1449906 sha256=51d0de276a687d1cde3687004459daec19d435ffda03d5838379af4bb937da87\n",
            "  Stored in directory: /root/.cache/pip/wheels/96/86/f6/68ab24c23f207c0077381a5e3904b2815136b879538a24b483\n",
            "  Building wheel for demjson (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for demjson: filename=demjson-2.2.4-cp37-none-any.whl size=73546 sha256=01122db86c9998503dc9a1c4ccf843e86e00b61df8167c697e5b3d99180b05ca\n",
            "  Stored in directory: /root/.cache/pip/wheels/c5/d2/ab/a54fb5ea53ac3badba098160e8452fa126a51febda80440ded\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-5.8-cp37-none-any.whl size=45613 sha256=f09bc3d6cbe28d6a1974d4a07f21373554e912f0806823bad55502c55a32ddc0\n",
            "  Stored in directory: /root/.cache/pip/wheels/ba/c0/ef/f28c4da5ac84a4e06ac256ca9182fc34fa57fefffdbc68425b\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-cp37-none-any.whl size=491058 sha256=b2bfa4222ab5d4aae5991923ee6d963ef6190da23e7879a58c3b97eeeedc947d\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\n",
            "Successfully built lmdb fasttext nltk demjson ftfy future\n",
            "Installing collected packages: lmdb, tqdm, xxhash, datasets, smmap, gitdb, GitPython, tokenizers, sacremoses, sentencepiece, transformers, torch, torchvision, fasttext, torchtext, nltk, portalocker, iopath, matplotlib, omegaconf, multidict, yarl, async-timeout, aiohttp, fsspec, pyDeprecate, torchmetrics, future, pytorch-lightning, demjson, ftfy, mmf\n",
            "  Found existing installation: lmdb 0.99\n",
            "    Uninstalling lmdb-0.99:\n",
            "      Successfully uninstalled lmdb-0.99\n",
            "  Found existing installation: tqdm 4.41.1\n",
            "    Uninstalling tqdm-4.41.1:\n",
            "      Successfully uninstalled tqdm-4.41.1\n",
            "  Found existing installation: torch 1.8.1+cu101\n",
            "    Uninstalling torch-1.8.1+cu101:\n",
            "      Successfully uninstalled torch-1.8.1+cu101\n",
            "  Found existing installation: torchvision 0.9.1+cu101\n",
            "    Uninstalling torchvision-0.9.1+cu101:\n",
            "      Successfully uninstalled torchvision-0.9.1+cu101\n",
            "  Found existing installation: torchtext 0.9.1\n",
            "    Uninstalling torchtext-0.9.1:\n",
            "      Successfully uninstalled torchtext-0.9.1\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "  Found existing installation: matplotlib 3.2.2\n",
            "    Uninstalling matplotlib-3.2.2:\n",
            "      Successfully uninstalled matplotlib-3.2.2\n",
            "  Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "  Running setup.py develop for mmf\n",
            "Successfully installed GitPython-3.1.0 aiohttp-3.7.4.post0 async-timeout-3.0.1 datasets-1.2.1 demjson-2.2.4 fasttext-0.9.1 fsspec-2021.4.0 ftfy-5.8 future-0.18.2 gitdb-4.0.7 iopath-0.1.7 lmdb-0.98 matplotlib-3.3.4 mmf multidict-5.1.0 nltk-3.4.5 omegaconf-2.0.6 portalocker-2.3.0 pyDeprecate-0.3.0 pytorch-lightning-1.3.0rc2 sacremoses-0.0.45 sentencepiece-0.1.95 smmap-4.0.0 tokenizers-0.9.2 torch-1.8.1 torchmetrics-0.3.1 torchtext-0.5.0 torchvision-0.9.1 tqdm-4.49.0 transformers-3.4.0 xxhash-2.0.2 yarl-1.6.3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYyXt9dzEBEU"
      },
      "source": [
        "## Download dataset\n",
        "\n",
        "We will now download the Hateful Memes dataset. You will require two things to download the datasets: (i) URL (ii) Password to the zip file. To get both of these follow these steps:\n",
        "\n",
        "1. Go to [DrivenData challenge page](https://www.drivendata.org/competitions/64/hateful-memes/)\n",
        "2. Register, read and acknowledge the agreements for data access.\n",
        "3. Go to the [data page](https://www.drivendata.org/competitions/64/hateful-memes/data), right click on the \"Hateful Memes challenge dataset\" link and \"Copy Link Address\" as shown in the image. This will copy the URL for the zip file to your clipboard which you will use in the next step.\n",
        "![data](https://i.imgur.com/JQx2hPm.png)\n",
        "4. Also, note the password provided in the description.\n",
        "5. Run the next code block, fill in the URL and the zipfile's password when prompted.\n",
        "\n",
        "The code blocks after that will download, convert and visualize the dataset.\n",
        "\n",
        "URL of Dataset: https://drivendata-competition-fb-hateful-memes-data.s3.amazonaws.com/XjiOc5ycDBRRNwbhRlgH.zip?AWSAccessKeyId=AKIARVBOBDCY4MWEDJKS&Signature=FpmkioFlEFPvW%2FMtmwfZIgJ%2BGCE%3D&Expires=1618941090\n",
        "\n",
        "Password: EWryfbZyNviilcDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulosPHAE-eto"
      },
      "source": [
        "from getpass import getpass, getuser\n",
        "#url = getpass(\"Enter the Hateful Memes data URL:\")\n",
        "#password = getpass(\"Enter ZIP file's Password:\")\n",
        "#url = 'https://drivendata-competition-fb-hateful-memes-data.s3.amazonaws.com/XjiOc5ycDBRRNwbhRlgH.zip?AWSAccessKeyId=AKIARVBOBDCY4MWEDJKS&Signature=FpmkioFlEFPvW%2FMtmwfZIgJ%2BGCE%3D&Expires=1618941090'\n",
        "password = 'EWryfbZyNviilcDF'\n",
        "url = 'https://drivendata-competition-fb-hateful-memes-data.s3.amazonaws.com/XjiOc5ycDBRRNwbhRlgH.zip?AWSAccessKeyId=AKIARVBOBDCY4MWEDJKS&Signature=ey9vLRX9%2FMRFZRKyFOIlJiJtjmo%3D&Expires=1620143289'"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiux2MWzFRPz"
      },
      "source": [
        "This will actually download the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5Y8wI6BoNN6",
        "outputId": "5cd68f25-9a49-4ccf-af1a-b43474d886d7"
      },
      "source": [
        "!curl -o /content/hm.zip \"$url\" -H 'Referer: https://www.drivendata.org/competitions/64/hateful-memes/data/' --compressed"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   295    0   295    0     0    470      0 --:--:-- --:--:-- --:--:--   470\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPYBxsyRFUUb"
      },
      "source": [
        "The next command will convert the zip file into required MMF format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsMmmOB3_rdY",
        "outputId": "de1b1654-2703-4874-8eb5-a9eecbdd0e02"
      },
      "source": [
        "!mmf_convert_hm --zip_file /content/hm.zip --password $password --bypass_checksum=1"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-30 19:08:28.644195: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "Data folder is /root/.cache/torch/mmf/data\n",
            "Zip path is /content/hm.zip\n",
            "Copying /content/hm.zip\n",
            "Unzipping /content/hm.zip\n",
            "Extracting the zip can take time. Sit back and relax.\n",
            "Moving train.jsonl\n",
            "Moving dev_seen.jsonl\n",
            "Moving test_seen.jsonl\n",
            "Moving dev_unseen.jsonl\n",
            "Moving test_unseen.jsonl\n",
            "Moving img\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tv6kSPrpBmR8"
      },
      "source": [
        "Remove hm.zip to save same some space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1UVDHnqBqPA",
        "outputId": "5b905752-7d43-4468-ad8c-bc2fafaee00d"
      },
      "source": [
        "!rm /content/hm.zip\n",
        "!rm /root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/images/hm.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove '/content/hm.zip': No such file or directory\n",
            "rm: cannot remove '/root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/images/hm.zip': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yf9VH6rmqyoK"
      },
      "source": [
        "Test/evaluate ready models(.pth)/check points"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YplgojJophTk"
      },
      "source": [
        "#!ls /content/drive/MyDrive/pth/BachSize32_MaxUpdates15000/\n",
        "#!rm -rf /content/save/hateful*\n",
        "#!cp -rf /content/save/visual_bert_final_VB_lr5e6.pth /content/drive/MyDrive/pth/b/\n",
        "#!cp -rf /content/save/VB_b32_m15000_4LayerBert_hateful_memes_visual_bert_1909805/ /content/drive/MyDrive/pth/b/\n",
        "\n",
        "# Set the dir for pth files\n",
        "#dir = '/content/drive/MyDrive/pth/BachSize32_MaxUpdates15000/'\n",
        "#dir = '/content/drive/MyDrive/pth/BachSize32_MaxUpdates15000_ClsNumLayers4_UnimodalOnly/' \n",
        "#=========Image-Grid=======================\n",
        "# Predict val/test\n",
        "#!mmf_predict config=projects/hateful_memes/configs/unimodal/image.yaml model=unimodal_image dataset=hateful_memes run_type=val checkpoint.resume_file=$dir'unimodal_image_final.pth' checkpoint.resume_pretrained=False\n",
        "#!mmf_predict config=projects/hateful_memes/configs/unimodal/image.yaml model=unimodal_image dataset=hateful_memes run_type=test checkpoint.resume_file=$dir'unimodal_image_final.pth' checkpoint.resume_pretrained=False\n",
        "\n",
        "# Run on val with trained model\n",
        "#!mmf_run config=projects/hateful_memes/configs/unimodal/image.yaml model=unimodal_image dataset=hateful_memes run_type=val checkpoint.resume_file=$dir'unimodal_image_final.pth' checkpoint.resume_pretrained=False\n",
        "\n",
        "#==========Text BERT ======================\n",
        "#!mmf_predict config=projects/hateful_memes/configs/unimodal/bert.yaml model=unimodal_text dataset=hateful_memes run_type=val checkpoint.resume_file=$dir'unimodal_text_final.pth' checkpoint.resume_pretrained=False\n",
        "#!mmf_predict config=projects/hateful_memes/configs/unimodal/bert.yaml model=unimodal_text dataset=hateful_memes run_type=test checkpoint.resume_file=$dir'unimodal_text_final.pth' checkpoint.resume_pretrained=False\n",
        "#!mmf_run config=projects/hateful_memes/configs/unimodal/bert.yaml model=unimodal_text dataset=hateful_memes run_type=val checkpoint.resume_file=$dir'unimodal_text_final.pth' checkpoint.resume_pretrained=False\n",
        "\n",
        "#==========Visual BERT ======================\n",
        "#!mmf_predict config=projects/hateful_memes/configs/visual_bert/direct.yaml model=visual_bert dataset=hateful_memes run_type=val checkpoint.resume_file=$dir'visual_bert_final.pth' checkpoint.resume_pretrained=False dataset_config.hateful_memes.annotations.val[0]=hateful_memes/defaults/annotations/dev_seen.jsonl dataset_config.hateful_memes.annotations.test[0]=hateful_memes/defaults/annotations/test_seen.jsonl\n",
        "#!mmf_predict config=projects/hateful_memes/configs/visual_bert/direct.yaml model=visual_bert dataset=hateful_memes run_type=test checkpoint.resume_file=$dir'visual_bert_final.pth' checkpoint.resume_pretrained=False dataset_config.hateful_memes.annotations.val[0]=hateful_memes/defaults/annotations/dev_seen.jsonl dataset_config.hateful_memes.annotations.test[0]=hateful_memes/defaults/annotations/test_seen.jsonl\n",
        "#!mmf_run config=projects/hateful_memes/configs/visual_bert/direct.yaml model=visual_bert dataset=hateful_memes run_type=val checkpoint.resume_file=$dir'visual_bert_final.pth' checkpoint.resume_pretrained=False\n",
        "\n",
        "#==========Visual BERT COCO ======================\n",
        "#!mmf_predict config=projects/hateful_memes/configs/visual_bert/from_coco.yaml model=visual_bert dataset=hateful_memes run_type=val checkpoint.resume_file=$dir'visual_bert_COCO_final.pth' checkpoint.resume_pretrained=False\n",
        "#!mmf_predict config=projects/hateful_memes/configs/visual_bert/from_coco.yaml model=visual_bert dataset=hateful_memes run_type=test checkpoint.resume_file=$dir'visual_bert_COCO_final.pth' checkpoint.resume_pretrained=False dataset_config.hateful_memes.annotations.val[0]=hateful_memes/defaults/annotations/dev_seen.jsonl dataset_config.hateful_memes.annotations.test[0]=hateful_memes/defaults/annotations/test_seen.jsonl\n",
        "#!mmf_run config=projects/hateful_memes/configs/visual_bert/from_coco.yaml model=visual_bert dataset=hateful_memes run_type=val checkpoint.resume_file=/content/save/visual_bert_final_coco5e6.pth checkpoint.resume_pretrained=False training.max_updates=6000\n",
        "\n",
        "# =========Pretrained baseline=====================\n",
        "#!mmf_run config=projects/hateful_memes/configs/unimodal/image.yaml model=unimodal_image dataset=hateful_memes run_type=val checkpoint.resume_zoo=unimodal_image.hateful_memes.images checkpoint.resume_pretrained=False\n",
        "#!mmf_run config=projects/hateful_memes/configs/unimodal/bert.yaml model=unimodal_text dataset=hateful_memes run_type=val checkpoint.resume_zoo=unimodal_text.hateful_memes.bert checkpoint.resume_pretrained=False\n",
        "#!mmf_run config=projects/hateful_memes/configs/visual_bert/direct.yaml model=visual_bert dataset=hateful_memes run_type=val checkpoint.resume_zoo=visual_bert.finetuned.hateful_memes.direct checkpoint.resume_pretrained=False\n",
        "#!mmf_run config=projects/hateful_memes/configs/visual_bert/from_coco.yaml model=visual_bert dataset=hateful_memes run_type=val checkpoint.resume_zoo=visual_bert.finetuned.hateful_memes.from_coco checkpoint.resume_pretrained=False\n",
        "\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKDldaLrAgfp"
      },
      "source": [
        "Dataset analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TyT6p2uwrTkE",
        "outputId": "38a8d044-e3ce-426f-dc80-b07579af3321"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UuYDQsd2Anp6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "755ccd94-4958-433e-cb74-aa33efbe6d73"
      },
      "source": [
        "#import sys\n",
        "#sys.path.append(\"/content/mmf/mmf\")\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "dataset_loc = '/root/.cache/torch/mmf/data/datasets/'\n",
        "train_ds = dataset_loc + 'hateful_memes/defaults/annotations/train.jsonl'\n",
        "val_ds = dataset_loc + 'hateful_memes/defaults/annotations/dev_unseen.jsonl'\n",
        "test_ds = dataset_loc +'hateful_memes/defaults/annotations/test_unseen.jsonl'\n",
        "\n",
        "val_ds2 = dataset_loc + 'hateful_memes/defaults/annotations/dev_seen.jsonl'\n",
        "test_ds2 = dataset_loc +'hateful_memes/defaults/annotations/test_seen.jsonl'\n",
        "print(train_ds)\n",
        "#!cat $val_ds\n",
        "train_df = pd.read_json(train_ds, lines=True)\n",
        "val_df = pd.read_json(val_ds, lines=True)\n",
        "test_df = pd.read_json(test_ds, lines=True)\n",
        "\n",
        "val_df2 = pd.read_json(val_ds2, lines=True)\n",
        "test_df2 = pd.read_json(test_ds2, lines=True)\n",
        "\n",
        "print(train_df.shape, val_df.shape, test_df.shape, val_df2.shape, test_df2.shape)\n",
        "print(val_df.iloc[0:10,0:3].values)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/annotations/train.jsonl\n",
            "(8500, 4) (540, 4) (2000, 3) (500, 4) (1000, 3)\n",
            "[[76432 'img/76432.png' 0]\n",
            " [14270 'img/14270.png' 0]\n",
            " [56947 'img/56947.png' 0]\n",
            " [35174 'img/35174.png' 0]\n",
            " [39264 'img/39264.png' 0]\n",
            " [18564 'img/18564.png' 0]\n",
            " [42361 'img/42361.png' 0]\n",
            " [29067 'img/29067.png' 0]\n",
            " [86471 'img/86471.png' 0]\n",
            " [51940 'img/51940.png' 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpjGUHXZGGRw"
      },
      "source": [
        "## Configure and tune pretrained model\n",
        "\n",
        "Refer to https://github.com/facebookresearch/mmf/tree/master/projects/hateful_memes \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fk6Kx8BN4Y5S"
      },
      "source": [
        "\n",
        "#===Type: Unimodal===\n",
        "#MODEL = 'Text BERT'\n",
        "#REPLACE_WITH_BASELINE_CONFIG = 'projects/hateful_memes/configs/unimodal/bert.yaml'\n",
        "#REPLACE_WITH_MODEL_KEY = 'unimodal_text'\n",
        "#REPLACE_WITH_PRETRAINED_ZOO_KEY = 'unimodal_text.hateful_memes.bert'\n",
        "\n",
        "#MODEL = 'Image-Grid'\n",
        "#REPLACE_WITH_BASELINE_CONFIG = 'projects/hateful_memes/configs/unimodal/image.yaml'\n",
        "#REPLACE_WITH_MODEL_KEY = 'unimodal_image'\n",
        "#REPLACE_WITH_PRETRAINED_ZOO_KEY = 'unimodal_image.hateful_memes.images'\n",
        "\n",
        "#===Type: Multimodal (Unimodal Pretraining)===\n",
        "#MODEL = 'Visual BERT'\n",
        "#REPLACE_WITH_BASELINE_CONFIG = 'projects/hateful_memes/configs/visual_bert/direct.yaml'\n",
        "#REPLACE_WITH_MODEL_KEY = 'visual_bert'\n",
        "#REPLACE_WITH_PRETRAINED_ZOO_KEY = 'visual_bert.finetuned.hateful_memes.direct'\n",
        "\n",
        "#===Type: Multimodal (Multimodal Pretraining)===\n",
        "#MODEL = 'Visual BERT COCO'\n",
        "REPLACE_WITH_BASELINE_CONFIG = 'projects/hateful_memes/configs/visual_bert/from_coco.yaml'\n",
        "REPLACE_WITH_MODEL_KEY = 'visual_bert'\n",
        "REPLACE_WITH_PRETRAINED_ZOO_KEY = 'visual_bert.finetuned.hateful_memes.from_coco'"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4Coy5DqGgwN"
      },
      "source": [
        "Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ozBVQHoGi5B",
        "outputId": "03e28082-4ece-46ea-b81e-d32d2c9a05ba"
      },
      "source": [
        "!mmf_run config=$REPLACE_WITH_BASELINE_CONFIG model=$REPLACE_WITH_MODEL_KEY dataset=hateful_memes training.log_interval=50 \\\n",
        "  training.max_updates=10000 \\\n",
        "  training.batch_size=32 \\\n",
        "  training.evaluation_interval=500 evaluation.predict=true training.fp16=True"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-05-01 03:31:25.961621: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "\u001b[32m2021-05-01T03:31:33 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/hateful_memes/configs/visual_bert/from_coco.yaml\n",
            "\u001b[32m2021-05-01T03:31:33 | mmf.utils.configuration: \u001b[0mOverriding option model to visual_bert\n",
            "\u001b[32m2021-05-01T03:31:33 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n",
            "\u001b[32m2021-05-01T03:31:33 | mmf.utils.configuration: \u001b[0mOverriding option training.log_interval to 50\n",
            "\u001b[32m2021-05-01T03:31:33 | mmf.utils.configuration: \u001b[0mOverriding option training.max_updates to 10000\n",
            "\u001b[32m2021-05-01T03:31:33 | mmf.utils.configuration: \u001b[0mOverriding option training.batch_size to 32\n",
            "\u001b[32m2021-05-01T03:31:33 | mmf.utils.configuration: \u001b[0mOverriding option training.evaluation_interval to 500\n",
            "\u001b[32m2021-05-01T03:31:33 | mmf.utils.configuration: \u001b[0mOverriding option evaluation.predict to true\n",
            "\u001b[32m2021-05-01T03:31:33 | mmf.utils.configuration: \u001b[0mOverriding option training.fp16 to True\n",
            "\u001b[32m2021-05-01T03:31:33 | mmf: \u001b[0mLogging to: ./save/train.log\n",
            "\u001b[32m2021-05-01T03:31:33 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/hateful_memes/configs/visual_bert/from_coco.yaml', 'model=visual_bert', 'dataset=hateful_memes', 'training.log_interval=50', 'training.max_updates=10000', 'training.batch_size=32', 'training.evaluation_interval=500', 'evaluation.predict=true', 'training.fp16=True'])\n",
            "\u001b[32m2021-05-01T03:31:33 | mmf_cli.run: \u001b[0mTorch version: 1.8.1+cu102\n",
            "\u001b[32m2021-05-01T03:31:33 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla P100-PCIE-16GB\n",
            "\u001b[32m2021-05-01T03:31:33 | mmf_cli.run: \u001b[0mUsing seed 33189342\n",
            "\u001b[32m2021-05-01T03:31:33 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-01T03:31:35 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-01T03:31:35 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2021-05-01T03:31:35 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
            "\u001b[32m2021-05-01T03:31:35 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
            "\u001b[32m2021-05-01T03:31:36 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
            "\u001b[32m2021-05-01T03:31:36 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
            "Some weights of VisualBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.token_type_embeddings_visual.weight', 'bert.embeddings.position_embeddings_visual.weight', 'bert.embeddings.projection.weight', 'bert.embeddings.projection.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[32m2021-05-01T03:31:52 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
            "\u001b[32m2021-05-01T03:31:52 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-01T03:31:52 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/basecontainer.py:232: UserWarning: cfg.pretty() is deprecated and will be removed in a future version.\n",
            "Use OmegaConf.to_yaml(cfg)\n",
            "\n",
            "  category=UserWarning,\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-01T03:31:52 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/basecontainer.py:232: UserWarning: cfg.pretty() is deprecated and will be removed in a future version.\n",
            "Use OmegaConf.to_yaml(cfg)\n",
            "\n",
            "  category=UserWarning,\n",
            "\n",
            "\u001b[32m2021-05-01T03:31:52 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
            "[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/models/visual_bert/visual_bert.pretrained.coco_train_val.tar.gz to /root/.cache/torch/mmf/data/models/visual_bert.pretrained.coco.defaults/visual_bert.pretrained.coco_train_val.tar.gz ]\n",
            "Downloading visual_bert.pretrained.coco_train_val.tar.gz: 100% 415M/415M [01:03<00:00, 6.54MB/s]\n",
            "[ Starting checksum for visual_bert.pretrained.coco_train_val.tar.gz]\n",
            "[ Checksum successful for visual_bert.pretrained.coco_train_val.tar.gz]\n",
            "Unpacking visual_bert.pretrained.coco_train_val.tar.gz\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-01T03:33:04 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-01T03:33:04 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-01T03:33:04 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-01T03:33:04 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.word_embeddings.weight from model.bert.embeddings.word_embeddings.weight\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings.weight from model.bert.embeddings.position_embeddings.weight\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings.weight from model.bert.embeddings.token_type_embeddings.weight\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.weight from model.bert.embeddings.LayerNorm.weight\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.bias from model.bert.embeddings.LayerNorm.bias\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings_visual.weight from model.bert.embeddings.token_type_embeddings_visual.weight\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings_visual.weight from model.bert.embeddings.position_embeddings_visual.weight\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.weight from model.bert.embeddings.projection.weight\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.bias from model.bert.embeddings.projection.bias\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.weight from model.bert.encoder.layer.0.attention.self.query.weight\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.bias from model.bert.encoder.layer.0.attention.self.query.bias\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.weight from model.bert.encoder.layer.0.attention.self.key.weight\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.bias from model.bert.encoder.layer.0.attention.self.key.bias\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.weight from model.bert.encoder.layer.0.attention.self.value.weight\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.bias from model.bert.encoder.layer.0.attention.self.value.bias\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.weight from model.bert.encoder.layer.0.attention.output.dense.weight\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.bias from model.bert.encoder.layer.0.attention.output.dense.bias\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.weight from model.bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.bias from model.bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.weight from model.bert.encoder.layer.0.intermediate.dense.weight\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.bias from model.bert.encoder.layer.0.intermediate.dense.bias\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.weight from model.bert.encoder.layer.0.output.dense.weight\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.bias from model.bert.encoder.layer.0.output.dense.bias\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.weight from model.bert.encoder.layer.0.output.LayerNorm.weight\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.bias from model.bert.encoder.layer.0.output.LayerNorm.bias\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.weight from model.bert.encoder.layer.1.attention.self.query.weight\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.bias from model.bert.encoder.layer.1.attention.self.query.bias\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.weight from model.bert.encoder.layer.1.attention.self.key.weight\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.bias from model.bert.encoder.layer.1.attention.self.key.bias\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.weight from model.bert.encoder.layer.1.attention.self.value.weight\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.bias from model.bert.encoder.layer.1.attention.self.value.bias\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.weight from model.bert.encoder.layer.1.attention.output.dense.weight\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.bias from model.bert.encoder.layer.1.attention.output.dense.bias\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.weight from model.bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.bias from model.bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.weight from model.bert.encoder.layer.1.intermediate.dense.weight\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.bias from model.bert.encoder.layer.1.intermediate.dense.bias\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.weight from model.bert.encoder.layer.1.output.dense.weight\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.bias from model.bert.encoder.layer.1.output.dense.bias\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.weight from model.bert.encoder.layer.1.output.LayerNorm.weight\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.bias from model.bert.encoder.layer.1.output.LayerNorm.bias\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.weight from model.bert.encoder.layer.2.attention.self.query.weight\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.bias from model.bert.encoder.layer.2.attention.self.query.bias\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.weight from model.bert.encoder.layer.2.attention.self.key.weight\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.bias from model.bert.encoder.layer.2.attention.self.key.bias\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.weight from model.bert.encoder.layer.2.attention.self.value.weight\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.bias from model.bert.encoder.layer.2.attention.self.value.bias\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.weight from model.bert.encoder.layer.2.attention.output.dense.weight\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.bias from model.bert.encoder.layer.2.attention.output.dense.bias\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.weight from model.bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.bias from model.bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.weight from model.bert.encoder.layer.2.intermediate.dense.weight\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.bias from model.bert.encoder.layer.2.intermediate.dense.bias\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.weight from model.bert.encoder.layer.2.output.dense.weight\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.bias from model.bert.encoder.layer.2.output.dense.bias\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.weight from model.bert.encoder.layer.2.output.LayerNorm.weight\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.bias from model.bert.encoder.layer.2.output.LayerNorm.bias\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.weight from model.bert.encoder.layer.3.attention.self.query.weight\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.bias from model.bert.encoder.layer.3.attention.self.query.bias\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.weight from model.bert.encoder.layer.3.attention.self.key.weight\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.bias from model.bert.encoder.layer.3.attention.self.key.bias\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.weight from model.bert.encoder.layer.3.attention.self.value.weight\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.bias from model.bert.encoder.layer.3.attention.self.value.bias\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.weight from model.bert.encoder.layer.3.attention.output.dense.weight\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.bias from model.bert.encoder.layer.3.attention.output.dense.bias\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.weight from model.bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.bias from model.bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.weight from model.bert.encoder.layer.3.intermediate.dense.weight\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.bias from model.bert.encoder.layer.3.intermediate.dense.bias\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.weight from model.bert.encoder.layer.3.output.dense.weight\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.bias from model.bert.encoder.layer.3.output.dense.bias\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.weight from model.bert.encoder.layer.3.output.LayerNorm.weight\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.bias from model.bert.encoder.layer.3.output.LayerNorm.bias\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.weight from model.bert.encoder.layer.4.attention.self.query.weight\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.bias from model.bert.encoder.layer.4.attention.self.query.bias\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.weight from model.bert.encoder.layer.4.attention.self.key.weight\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.bias from model.bert.encoder.layer.4.attention.self.key.bias\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.weight from model.bert.encoder.layer.4.attention.self.value.weight\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.bias from model.bert.encoder.layer.4.attention.self.value.bias\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.weight from model.bert.encoder.layer.4.attention.output.dense.weight\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.bias from model.bert.encoder.layer.4.attention.output.dense.bias\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.weight from model.bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.bias from model.bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.weight from model.bert.encoder.layer.4.intermediate.dense.weight\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.bias from model.bert.encoder.layer.4.intermediate.dense.bias\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.weight from model.bert.encoder.layer.4.output.dense.weight\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.bias from model.bert.encoder.layer.4.output.dense.bias\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.weight from model.bert.encoder.layer.4.output.LayerNorm.weight\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.bias from model.bert.encoder.layer.4.output.LayerNorm.bias\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.weight from model.bert.encoder.layer.5.attention.self.query.weight\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.bias from model.bert.encoder.layer.5.attention.self.query.bias\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.weight from model.bert.encoder.layer.5.attention.self.key.weight\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.bias from model.bert.encoder.layer.5.attention.self.key.bias\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.weight from model.bert.encoder.layer.5.attention.self.value.weight\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.bias from model.bert.encoder.layer.5.attention.self.value.bias\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.weight from model.bert.encoder.layer.5.attention.output.dense.weight\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.bias from model.bert.encoder.layer.5.attention.output.dense.bias\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.weight from model.bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.bias from model.bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.weight from model.bert.encoder.layer.5.intermediate.dense.weight\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.bias from model.bert.encoder.layer.5.intermediate.dense.bias\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.weight from model.bert.encoder.layer.5.output.dense.weight\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.bias from model.bert.encoder.layer.5.output.dense.bias\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.weight from model.bert.encoder.layer.5.output.LayerNorm.weight\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.bias from model.bert.encoder.layer.5.output.LayerNorm.bias\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.weight from model.bert.encoder.layer.6.attention.self.query.weight\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.bias from model.bert.encoder.layer.6.attention.self.query.bias\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.weight from model.bert.encoder.layer.6.attention.self.key.weight\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.bias from model.bert.encoder.layer.6.attention.self.key.bias\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.weight from model.bert.encoder.layer.6.attention.self.value.weight\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.bias from model.bert.encoder.layer.6.attention.self.value.bias\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.weight from model.bert.encoder.layer.6.attention.output.dense.weight\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.bias from model.bert.encoder.layer.6.attention.output.dense.bias\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.weight from model.bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.bias from model.bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.weight from model.bert.encoder.layer.6.intermediate.dense.weight\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.bias from model.bert.encoder.layer.6.intermediate.dense.bias\n",
            "\u001b[32m2021-05-01T03:33:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.weight from model.bert.encoder.layer.6.output.dense.weight\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.bias from model.bert.encoder.layer.6.output.dense.bias\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.weight from model.bert.encoder.layer.6.output.LayerNorm.weight\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.bias from model.bert.encoder.layer.6.output.LayerNorm.bias\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.weight from model.bert.encoder.layer.7.attention.self.query.weight\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.bias from model.bert.encoder.layer.7.attention.self.query.bias\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.weight from model.bert.encoder.layer.7.attention.self.key.weight\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.bias from model.bert.encoder.layer.7.attention.self.key.bias\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.weight from model.bert.encoder.layer.7.attention.self.value.weight\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.bias from model.bert.encoder.layer.7.attention.self.value.bias\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.weight from model.bert.encoder.layer.7.attention.output.dense.weight\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.bias from model.bert.encoder.layer.7.attention.output.dense.bias\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.weight from model.bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.bias from model.bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.weight from model.bert.encoder.layer.7.intermediate.dense.weight\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.bias from model.bert.encoder.layer.7.intermediate.dense.bias\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.weight from model.bert.encoder.layer.7.output.dense.weight\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.bias from model.bert.encoder.layer.7.output.dense.bias\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.weight from model.bert.encoder.layer.7.output.LayerNorm.weight\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.bias from model.bert.encoder.layer.7.output.LayerNorm.bias\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.weight from model.bert.encoder.layer.8.attention.self.query.weight\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.bias from model.bert.encoder.layer.8.attention.self.query.bias\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.weight from model.bert.encoder.layer.8.attention.self.key.weight\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.bias from model.bert.encoder.layer.8.attention.self.key.bias\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.weight from model.bert.encoder.layer.8.attention.self.value.weight\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.bias from model.bert.encoder.layer.8.attention.self.value.bias\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.weight from model.bert.encoder.layer.8.attention.output.dense.weight\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.bias from model.bert.encoder.layer.8.attention.output.dense.bias\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.weight from model.bert.encoder.layer.8.attention.output.LayerNorm.weight\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.bias from model.bert.encoder.layer.8.attention.output.LayerNorm.bias\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.weight from model.bert.encoder.layer.8.intermediate.dense.weight\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.bias from model.bert.encoder.layer.8.intermediate.dense.bias\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.weight from model.bert.encoder.layer.8.output.dense.weight\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.bias from model.bert.encoder.layer.8.output.dense.bias\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.weight from model.bert.encoder.layer.8.output.LayerNorm.weight\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.bias from model.bert.encoder.layer.8.output.LayerNorm.bias\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.weight from model.bert.encoder.layer.9.attention.self.query.weight\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.bias from model.bert.encoder.layer.9.attention.self.query.bias\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.weight from model.bert.encoder.layer.9.attention.self.key.weight\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.bias from model.bert.encoder.layer.9.attention.self.key.bias\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.weight from model.bert.encoder.layer.9.attention.self.value.weight\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.bias from model.bert.encoder.layer.9.attention.self.value.bias\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.weight from model.bert.encoder.layer.9.attention.output.dense.weight\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.bias from model.bert.encoder.layer.9.attention.output.dense.bias\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.weight from model.bert.encoder.layer.9.attention.output.LayerNorm.weight\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.bias from model.bert.encoder.layer.9.attention.output.LayerNorm.bias\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.weight from model.bert.encoder.layer.9.intermediate.dense.weight\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.bias from model.bert.encoder.layer.9.intermediate.dense.bias\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.weight from model.bert.encoder.layer.9.output.dense.weight\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.bias from model.bert.encoder.layer.9.output.dense.bias\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.weight from model.bert.encoder.layer.9.output.LayerNorm.weight\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.bias from model.bert.encoder.layer.9.output.LayerNorm.bias\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.weight from model.bert.encoder.layer.10.attention.self.query.weight\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.bias from model.bert.encoder.layer.10.attention.self.query.bias\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.weight from model.bert.encoder.layer.10.attention.self.key.weight\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.bias from model.bert.encoder.layer.10.attention.self.key.bias\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.weight from model.bert.encoder.layer.10.attention.self.value.weight\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.bias from model.bert.encoder.layer.10.attention.self.value.bias\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.weight from model.bert.encoder.layer.10.attention.output.dense.weight\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.bias from model.bert.encoder.layer.10.attention.output.dense.bias\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.weight from model.bert.encoder.layer.10.attention.output.LayerNorm.weight\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.bias from model.bert.encoder.layer.10.attention.output.LayerNorm.bias\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.weight from model.bert.encoder.layer.10.intermediate.dense.weight\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.bias from model.bert.encoder.layer.10.intermediate.dense.bias\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.weight from model.bert.encoder.layer.10.output.dense.weight\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.bias from model.bert.encoder.layer.10.output.dense.bias\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.weight from model.bert.encoder.layer.10.output.LayerNorm.weight\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.bias from model.bert.encoder.layer.10.output.LayerNorm.bias\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.weight from model.bert.encoder.layer.11.attention.self.query.weight\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.bias from model.bert.encoder.layer.11.attention.self.query.bias\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.weight from model.bert.encoder.layer.11.attention.self.key.weight\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.bias from model.bert.encoder.layer.11.attention.self.key.bias\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.weight from model.bert.encoder.layer.11.attention.self.value.weight\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.bias from model.bert.encoder.layer.11.attention.self.value.bias\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.weight from model.bert.encoder.layer.11.attention.output.dense.weight\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.bias from model.bert.encoder.layer.11.attention.output.dense.bias\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.weight from model.bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.bias from model.bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.weight from model.bert.encoder.layer.11.intermediate.dense.weight\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.bias from model.bert.encoder.layer.11.intermediate.dense.bias\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.weight from model.bert.encoder.layer.11.output.dense.weight\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.bias from model.bert.encoder.layer.11.output.dense.bias\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.weight from model.bert.encoder.layer.11.output.LayerNorm.weight\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.bias from model.bert.encoder.layer.11.output.LayerNorm.bias\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.weight from model.bert.pooler.dense.weight\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.bias from model.bert.pooler.dense.bias\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mPretrained model loaded\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 0\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 0\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 0\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.trainers.mmf_trainer: \u001b[0mVisualBERT(\n",
            "  (model): VisualBERTForClassification(\n",
            "    (bert): VisualBERTBase(\n",
            "      (embeddings): BertVisioLinguisticEmbeddings(\n",
            "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "        (position_embeddings): Embedding(512, 768)\n",
            "        (token_type_embeddings): Embedding(2, 768)\n",
            "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (token_type_embeddings_visual): Embedding(2, 768)\n",
            "        (position_embeddings_visual): Embedding(512, 768)\n",
            "        (projection): Linear(in_features=2048, out_features=768, bias=True)\n",
            "      )\n",
            "      (encoder): BertEncoderJit(\n",
            "        (layer): ModuleList(\n",
            "          (0): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (2): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (3): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (4): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (5): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (6): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (7): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (8): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (9): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (10): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (11): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pooler): BertPooler(\n",
            "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (activation): Tanh()\n",
            "      )\n",
            "    )\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "    (classifier): Sequential(\n",
            "      (0): BertPredictionHeadTransform(\n",
            "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      )\n",
            "      (1): Linear(in_features=768, out_features=2, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (losses): Losses(\n",
            "    (losses): ModuleList(\n",
            "      (0): MMFLoss(\n",
            "        (loss_criterion): CrossEntropyLoss(\n",
            "          (loss_fn): CrossEntropyLoss()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.utils.general: \u001b[0mTotal Parameters: 112044290. Trained Parameters: 112044290\n",
            "\u001b[32m2021-05-01T03:33:05 | mmf.trainers.core.training_loop: \u001b[0mStarting training...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-01T03:33:17 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-01T03:33:17 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "\n",
            "\u001b[32m2021-05-01T03:35:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 50/10000, train/hateful_memes/cross_entropy: 0.9257, train/hateful_memes/cross_entropy/avg: 0.9257, train/total_loss: 0.9257, train/total_loss/avg: 0.9257, max mem: 8104.0, experiment: run, epoch: 1, num_updates: 50, iterations: 50, max_updates: 10000, lr: 0., ups: 0.36, time: 02m 20s 734ms, time_since_start: 03m 33s 104ms, eta: 08h 02m 38s 352ms\n",
            "\u001b[32m2021-05-01T03:37:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/10000, train/hateful_memes/cross_entropy: 0.7635, train/hateful_memes/cross_entropy/avg: 0.8446, train/total_loss: 0.7635, train/total_loss/avg: 0.8446, max mem: 8104.0, experiment: run, epoch: 1, num_updates: 100, iterations: 100, max_updates: 10000, lr: 0., ups: 0.39, time: 02m 08s 582ms, time_since_start: 05m 41s 686ms, eta: 07h 18m 44s 948ms\n",
            "\u001b[32m2021-05-01T03:39:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 150/10000, train/hateful_memes/cross_entropy: 0.7635, train/hateful_memes/cross_entropy/avg: 0.7956, train/total_loss: 0.7635, train/total_loss/avg: 0.7956, max mem: 8104.0, experiment: run, epoch: 1, num_updates: 150, iterations: 150, max_updates: 10000, lr: 0., ups: 0.37, time: 02m 16s 526ms, time_since_start: 07m 58s 213ms, eta: 07h 43m 30s 212ms\n",
            "\u001b[32m2021-05-01T03:41:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/10000, train/hateful_memes/cross_entropy: 0.6977, train/hateful_memes/cross_entropy/avg: 0.7599, train/total_loss: 0.6977, train/total_loss/avg: 0.7599, max mem: 8104.0, experiment: run, epoch: 1, num_updates: 200, iterations: 200, max_updates: 10000, lr: 0., ups: 0.39, time: 02m 08s 531ms, time_since_start: 10m 06s 745ms, eta: 07h 14m 08s 714ms\n",
            "\u001b[32m2021-05-01T03:44:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 250/10000, train/hateful_memes/cross_entropy: 0.6977, train/hateful_memes/cross_entropy/avg: 0.7402, train/total_loss: 0.6977, train/total_loss/avg: 0.7402, max mem: 8104.0, experiment: run, epoch: 1, num_updates: 250, iterations: 250, max_updates: 10000, lr: 0., ups: 0.36, time: 02m 17s 545ms, time_since_start: 12m 24s 290ms, eta: 07h 42m 13s 302ms\n",
            "\u001b[32m2021-05-01T03:45:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/10000, train/hateful_memes/cross_entropy: 0.6610, train/hateful_memes/cross_entropy/avg: 0.7144, train/total_loss: 0.6610, train/total_loss/avg: 0.7144, max mem: 8104.0, experiment: run, epoch: 2, num_updates: 300, iterations: 300, max_updates: 10000, lr: 0., ups: 0.51, time: 01m 39s 503ms, time_since_start: 14m 03s 794ms, eta: 05h 32m 40s 018ms\n",
            "\u001b[32m2021-05-01T03:47:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 350/10000, train/hateful_memes/cross_entropy: 0.6610, train/hateful_memes/cross_entropy/avg: 0.7016, train/total_loss: 0.6610, train/total_loss/avg: 0.7016, max mem: 8104.0, experiment: run, epoch: 2, num_updates: 350, iterations: 350, max_updates: 10000, lr: 0., ups: 0.45, time: 01m 50s 194ms, time_since_start: 15m 53s 988ms, eta: 06h 06m 30s 608ms\n",
            "\u001b[32m2021-05-01T03:50:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/10000, train/hateful_memes/cross_entropy: 0.6529, train/hateful_memes/cross_entropy/avg: 0.6823, train/total_loss: 0.6529, train/total_loss/avg: 0.6823, max mem: 8104.0, experiment: run, epoch: 2, num_updates: 400, iterations: 400, max_updates: 10000, lr: 0., ups: 0.37, time: 02m 15s 290ms, time_since_start: 18m 09s 279ms, eta: 07h 27m 39s 026ms\n",
            "\u001b[32m2021-05-01T03:52:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 450/10000, train/hateful_memes/cross_entropy: 0.6529, train/hateful_memes/cross_entropy/avg: 0.6711, train/total_loss: 0.6529, train/total_loss/avg: 0.6711, max mem: 8104.0, experiment: run, epoch: 2, num_updates: 450, iterations: 450, max_updates: 10000, lr: 0., ups: 0.39, time: 02m 08s 754ms, time_since_start: 20m 18s 034ms, eta: 07h 03m 48s 267ms\n",
            "\u001b[32m2021-05-01T03:54:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/10000, train/hateful_memes/cross_entropy: 0.6248, train/hateful_memes/cross_entropy/avg: 0.6629, train/total_loss: 0.6248, train/total_loss/avg: 0.6629, max mem: 8104.0, experiment: run, epoch: 2, num_updates: 500, iterations: 500, max_updates: 10000, lr: 0., ups: 0.37, time: 02m 15s 708ms, time_since_start: 22m 33s 742ms, eta: 07h 24m 21s 318ms\n",
            "\u001b[32m2021-05-01T03:54:26 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2021-05-01T03:54:26 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[32m2021-05-01T03:55:20 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-05-01T03:55:26 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
            "\u001b[32m2021-05-01T03:55:40 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-05-01T03:55:55 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-05-01T03:55:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/10000, val/hateful_memes/cross_entropy: 0.6689, val/total_loss: 0.6689, val/hateful_memes/accuracy: 0.6222, val/hateful_memes/binary_f1: 0.0000, val/hateful_memes/roc_auc: 0.4648, num_updates: 500, epoch: 2, iterations: 500, max_updates: 10000, val_time: 01m 29s 140ms, best_update: 500, best_iteration: 500, best_val/hateful_memes/roc_auc: 0.464809\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-01T03:57:00 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-01T03:57:00 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2021-05-01T03:57:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 550/10000, train/hateful_memes/cross_entropy: 0.6529, train/hateful_memes/cross_entropy/avg: 0.6630, train/total_loss: 0.6529, train/total_loss/avg: 0.6630, max mem: 8122.0, experiment: run, epoch: 3, num_updates: 550, iterations: 550, max_updates: 10000, lr: 0., ups: 0.45, time: 01m 50s 936ms, time_since_start: 25m 53s 843ms, eta: 06h 01m 19s 961ms\n",
            "\u001b[32m2021-05-01T03:59:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/10000, train/hateful_memes/cross_entropy: 0.6261, train/hateful_memes/cross_entropy/avg: 0.6600, train/total_loss: 0.6261, train/total_loss/avg: 0.6600, max mem: 8122.0, experiment: run, epoch: 3, num_updates: 600, iterations: 600, max_updates: 10000, lr: 0., ups: 0.46, time: 01m 48s 587ms, time_since_start: 27m 42s 430ms, eta: 05h 51m 48s 492ms\n",
            "\u001b[32m2021-05-01T04:01:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 650/10000, train/hateful_memes/cross_entropy: 0.6529, train/hateful_memes/cross_entropy/avg: 0.6622, train/total_loss: 0.6529, train/total_loss/avg: 0.6622, max mem: 8122.0, experiment: run, epoch: 3, num_updates: 650, iterations: 650, max_updates: 10000, lr: 0., ups: 0.41, time: 02m 01s 318ms, time_since_start: 29m 43s 749ms, eta: 06h 30m 57s 926ms\n",
            "\u001b[32m2021-05-01T04:03:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/10000, train/hateful_memes/cross_entropy: 0.6505, train/hateful_memes/cross_entropy/avg: 0.6614, train/total_loss: 0.6505, train/total_loss/avg: 0.6614, max mem: 8122.0, experiment: run, epoch: 3, num_updates: 700, iterations: 700, max_updates: 10000, lr: 0., ups: 0.42, time: 01m 59s 997ms, time_since_start: 31m 43s 747ms, eta: 06h 24m 38s 475ms\n",
            "\u001b[32m2021-05-01T04:05:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 750/10000, train/hateful_memes/cross_entropy: 0.6529, train/hateful_memes/cross_entropy/avg: 0.6710, train/total_loss: 0.6529, train/total_loss/avg: 0.6710, max mem: 8122.0, experiment: run, epoch: 3, num_updates: 750, iterations: 750, max_updates: 10000, lr: 0., ups: 0.37, time: 02m 15s 500ms, time_since_start: 33m 59s 247ms, eta: 07h 11m 59s 810ms\n",
            "\u001b[32m2021-05-01T04:07:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/10000, train/hateful_memes/cross_entropy: 0.6505, train/hateful_memes/cross_entropy/avg: 0.6678, train/total_loss: 0.6505, train/total_loss/avg: 0.6678, max mem: 8122.0, experiment: run, epoch: 4, num_updates: 800, iterations: 800, max_updates: 10000, lr: 0., ups: 0.41, time: 02m 02s 870ms, time_since_start: 36m 02s 118ms, eta: 06h 29m 36s 911ms\n",
            "\u001b[32m2021-05-01T04:09:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 850/10000, train/hateful_memes/cross_entropy: 0.6505, train/hateful_memes/cross_entropy/avg: 0.6654, train/total_loss: 0.6505, train/total_loss/avg: 0.6654, max mem: 8122.0, experiment: run, epoch: 4, num_updates: 850, iterations: 850, max_updates: 10000, lr: 0., ups: 0.62, time: 01m 21s 316ms, time_since_start: 37m 23s 434ms, eta: 04h 16m 26s 833ms\n",
            "\u001b[32m2021-05-01T04:11:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/10000, train/hateful_memes/cross_entropy: 0.6288, train/hateful_memes/cross_entropy/avg: 0.6634, train/total_loss: 0.6288, train/total_loss/avg: 0.6634, max mem: 8122.0, experiment: run, epoch: 4, num_updates: 900, iterations: 900, max_updates: 10000, lr: 0., ups: 0.48, time: 01m 44s 946ms, time_since_start: 39m 08s 380ms, eta: 05h 29m 09s 644ms\n",
            "\u001b[32m2021-05-01T04:13:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 950/10000, train/hateful_memes/cross_entropy: 0.6288, train/hateful_memes/cross_entropy/avg: 0.6549, train/total_loss: 0.6288, train/total_loss/avg: 0.6549, max mem: 8122.0, experiment: run, epoch: 4, num_updates: 950, iterations: 950, max_updates: 10000, lr: 0., ups: 0.39, time: 02m 08s 743ms, time_since_start: 41m 17s 124ms, eta: 06h 41m 34s 935ms\n",
            "\u001b[32m2021-05-01T04:15:18 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[32m2021-05-01T04:15:18 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-05-01T04:15:26 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-05-01T04:15:39 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-05-01T04:15:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/10000, train/hateful_memes/cross_entropy: 0.6268, train/hateful_memes/cross_entropy/avg: 0.6469, train/total_loss: 0.6268, train/total_loss/avg: 0.6469, max mem: 8122.0, experiment: run, epoch: 4, num_updates: 1000, iterations: 1000, max_updates: 10000, lr: 0., ups: 0.34, time: 02m 29s 341ms, time_since_start: 43m 46s 466ms, eta: 07h 43m 15s 415ms\n",
            "\u001b[32m2021-05-01T04:15:39 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2021-05-01T04:15:39 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-01T04:15:39 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-01T04:15:39 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2021-05-01T04:16:31 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-05-01T04:16:37 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
            "\u001b[32m2021-05-01T04:16:50 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-05-01T04:17:06 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-05-01T04:17:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/10000, val/hateful_memes/cross_entropy: 0.6731, val/total_loss: 0.6731, val/hateful_memes/accuracy: 0.6222, val/hateful_memes/binary_f1: 0.1500, val/hateful_memes/roc_auc: 0.5434, num_updates: 1000, epoch: 4, iterations: 1000, max_updates: 10000, val_time: 01m 27s 298ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.543397\n",
            "\u001b[32m2021-05-01T04:19:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1050/10000, train/hateful_memes/cross_entropy: 0.6268, train/hateful_memes/cross_entropy/avg: 0.6461, train/total_loss: 0.6268, train/total_loss/avg: 0.6461, max mem: 8122.0, experiment: run, epoch: 4, num_updates: 1050, iterations: 1050, max_updates: 10000, lr: 0., ups: 0.43, time: 01m 57s 625ms, time_since_start: 47m 11s 391ms, eta: 06h 02m 50s 760ms\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-01T04:19:36 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-01T04:19:36 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2021-05-01T04:20:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1100/10000, train/hateful_memes/cross_entropy: 0.6261, train/hateful_memes/cross_entropy/avg: 0.6451, train/total_loss: 0.6261, train/total_loss/avg: 0.6451, max mem: 8122.0, experiment: run, epoch: 5, num_updates: 1100, iterations: 1100, max_updates: 10000, lr: 0., ups: 0.48, time: 01m 44s 383ms, time_since_start: 48m 55s 775ms, eta: 05h 20m 12s 046ms\n",
            "\u001b[32m2021-05-01T04:22:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1150/10000, train/hateful_memes/cross_entropy: 0.6257, train/hateful_memes/cross_entropy/avg: 0.6419, train/total_loss: 0.6257, train/total_loss/avg: 0.6419, max mem: 8122.0, experiment: run, epoch: 5, num_updates: 1150, iterations: 1150, max_updates: 10000, lr: 0., ups: 0.47, time: 01m 46s 283ms, time_since_start: 50m 42s 059ms, eta: 05h 24m 11s 775ms\n",
            "\u001b[32m2021-05-01T04:24:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/10000, train/hateful_memes/cross_entropy: 0.6257, train/hateful_memes/cross_entropy/avg: 0.6415, train/total_loss: 0.6257, train/total_loss/avg: 0.6415, max mem: 8122.0, experiment: run, epoch: 5, num_updates: 1200, iterations: 1200, max_updates: 10000, lr: 0., ups: 0.40, time: 02m 06s 022ms, time_since_start: 52m 48s 081ms, eta: 06h 22m 14s 086ms\n",
            "\u001b[32m2021-05-01T04:26:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1250/10000, train/hateful_memes/cross_entropy: 0.6248, train/hateful_memes/cross_entropy/avg: 0.6351, train/total_loss: 0.6248, train/total_loss/avg: 0.6351, max mem: 8122.0, experiment: run, epoch: 5, num_updates: 1250, iterations: 1250, max_updates: 10000, lr: 0., ups: 0.38, time: 02m 11s 105ms, time_since_start: 54m 59s 187ms, eta: 06h 35m 23s 566ms\n",
            "\u001b[32m2021-05-01T04:29:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1300/10000, train/hateful_memes/cross_entropy: 0.6248, train/hateful_memes/cross_entropy/avg: 0.6281, train/total_loss: 0.6248, train/total_loss/avg: 0.6281, max mem: 8122.0, experiment: run, epoch: 5, num_updates: 1300, iterations: 1300, max_updates: 10000, lr: 0., ups: 0.38, time: 02m 12s 092ms, time_since_start: 57m 11s 280ms, eta: 06h 36m 05s 585ms\n",
            "\u001b[32m2021-05-01T04:30:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1350/10000, train/hateful_memes/cross_entropy: 0.6194, train/hateful_memes/cross_entropy/avg: 0.6224, train/total_loss: 0.6194, train/total_loss/avg: 0.6224, max mem: 8122.0, experiment: run, epoch: 6, num_updates: 1350, iterations: 1350, max_updates: 10000, lr: 0., ups: 0.48, time: 01m 44s 699ms, time_since_start: 58m 55s 980ms, eta: 05h 12m 08s 938ms\n",
            "\u001b[32m2021-05-01T04:32:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/10000, train/hateful_memes/cross_entropy: 0.6194, train/hateful_memes/cross_entropy/avg: 0.6208, train/total_loss: 0.6194, train/total_loss/avg: 0.6208, max mem: 8122.0, experiment: run, epoch: 6, num_updates: 1400, iterations: 1400, max_updates: 10000, lr: 0., ups: 0.58, time: 01m 26s 803ms, time_since_start: 01h 22s 783ms, eta: 04h 17m 17s 824ms\n",
            "\u001b[32m2021-05-01T04:34:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1450/10000, train/hateful_memes/cross_entropy: 0.6194, train/hateful_memes/cross_entropy/avg: 0.6186, train/total_loss: 0.6194, train/total_loss/avg: 0.6186, max mem: 8122.0, experiment: run, epoch: 6, num_updates: 1450, iterations: 1450, max_updates: 10000, lr: 0., ups: 0.42, time: 02m 611ms, time_since_start: 01h 02m 23s 395ms, eta: 05h 55m 25s 840ms\n",
            "\u001b[32m2021-05-01T04:36:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1500/10000, train/hateful_memes/cross_entropy: 0.6257, train/hateful_memes/cross_entropy/avg: 0.6193, train/total_loss: 0.6257, train/total_loss/avg: 0.6193, max mem: 8122.0, experiment: run, epoch: 6, num_updates: 1500, iterations: 1500, max_updates: 10000, lr: 0., ups: 0.38, time: 02m 12s 483ms, time_since_start: 01h 04m 35s 878ms, eta: 06h 28m 07s 967ms\n",
            "\u001b[32m2021-05-01T04:36:28 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2021-05-01T04:36:28 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[32m2021-05-01T04:37:24 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-05-01T04:37:29 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
            "\u001b[32m2021-05-01T04:37:42 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-05-01T04:38:00 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-05-01T04:38:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1500/10000, val/hateful_memes/cross_entropy: 0.7137, val/total_loss: 0.7137, val/hateful_memes/accuracy: 0.6222, val/hateful_memes/binary_f1: 0.3200, val/hateful_memes/roc_auc: 0.6160, num_updates: 1500, epoch: 6, iterations: 1500, max_updates: 10000, val_time: 01m 31s 279ms, best_update: 1500, best_iteration: 1500, best_val/hateful_memes/roc_auc: 0.615985\n",
            "\u001b[32m2021-05-01T04:39:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1550/10000, train/hateful_memes/cross_entropy: 0.6194, train/hateful_memes/cross_entropy/avg: 0.6134, train/total_loss: 0.6194, train/total_loss/avg: 0.6134, max mem: 8122.0, experiment: run, epoch: 6, num_updates: 1550, iterations: 1550, max_updates: 10000, lr: 0., ups: 0.42, time: 01m 58s 610ms, time_since_start: 01h 08m 05s 798ms, eta: 05h 45m 26s 680ms\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-01T04:41:54 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-01T04:41:54 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2021-05-01T04:42:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1600/10000, train/hateful_memes/cross_entropy: 0.5782, train/hateful_memes/cross_entropy/avg: 0.6052, train/total_loss: 0.5782, train/total_loss/avg: 0.6052, max mem: 8122.0, experiment: run, epoch: 7, num_updates: 1600, iterations: 1600, max_updates: 10000, lr: 0., ups: 0.40, time: 02m 05s 365ms, time_since_start: 01h 10m 11s 163ms, eta: 06h 02m 57s 456ms\n",
            "\u001b[32m2021-05-01T04:43:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1650/10000, train/hateful_memes/cross_entropy: 0.5704, train/hateful_memes/cross_entropy/avg: 0.6035, train/total_loss: 0.5704, train/total_loss/avg: 0.6035, max mem: 8122.0, experiment: run, epoch: 7, num_updates: 1650, iterations: 1650, max_updates: 10000, lr: 0., ups: 0.54, time: 01m 32s 059ms, time_since_start: 01h 11m 43s 222ms, eta: 04h 24m 56s 573ms\n",
            "\u001b[32m2021-05-01T04:45:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1700/10000, train/hateful_memes/cross_entropy: 0.5566, train/hateful_memes/cross_entropy/avg: 0.5973, train/total_loss: 0.5566, train/total_loss/avg: 0.5973, max mem: 8122.0, experiment: run, epoch: 7, num_updates: 1700, iterations: 1700, max_updates: 10000, lr: 0., ups: 0.43, time: 01m 57s 761ms, time_since_start: 01h 13m 40s 984ms, eta: 05h 36m 53s 137ms\n",
            "\u001b[32m2021-05-01T04:47:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1750/10000, train/hateful_memes/cross_entropy: 0.5511, train/hateful_memes/cross_entropy/avg: 0.5901, train/total_loss: 0.5511, train/total_loss/avg: 0.5901, max mem: 8122.0, experiment: run, epoch: 7, num_updates: 1750, iterations: 1750, max_updates: 10000, lr: 0., ups: 0.40, time: 02m 04s 899ms, time_since_start: 01h 15m 45s 884ms, eta: 05h 55m 09s 132ms\n",
            "\u001b[32m2021-05-01T04:49:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1800/10000, train/hateful_memes/cross_entropy: 0.5023, train/hateful_memes/cross_entropy/avg: 0.5849, train/total_loss: 0.5023, train/total_loss/avg: 0.5849, max mem: 8122.0, experiment: run, epoch: 7, num_updates: 1800, iterations: 1800, max_updates: 10000, lr: 0., ups: 0.38, time: 02m 11s 155ms, time_since_start: 01h 17m 57s 039ms, eta: 06h 10m 40s 845ms\n",
            "\u001b[32m2021-05-01T04:52:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1850/10000, train/hateful_memes/cross_entropy: 0.4948, train/hateful_memes/cross_entropy/avg: 0.5815, train/total_loss: 0.4948, train/total_loss/avg: 0.5815, max mem: 8122.0, experiment: run, epoch: 7, num_updates: 1850, iterations: 1850, max_updates: 10000, lr: 0., ups: 0.38, time: 02m 10s 448ms, time_since_start: 01h 20m 07s 488ms, eta: 06h 06m 26s 058ms\n",
            "\u001b[32m2021-05-01T04:53:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1900/10000, train/hateful_memes/cross_entropy: 0.4834, train/hateful_memes/cross_entropy/avg: 0.5746, train/total_loss: 0.4834, train/total_loss/avg: 0.5746, max mem: 8122.0, experiment: run, epoch: 8, num_updates: 1900, iterations: 1900, max_updates: 10000, lr: 0., ups: 0.53, time: 01m 34s 304ms, time_since_start: 01h 21m 41s 792ms, eta: 04h 23m 16s 685ms\n",
            "\u001b[32m2021-05-01T04:55:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1950/10000, train/hateful_memes/cross_entropy: 0.4739, train/hateful_memes/cross_entropy/avg: 0.5708, train/total_loss: 0.4739, train/total_loss/avg: 0.5708, max mem: 8122.0, experiment: run, epoch: 8, num_updates: 1950, iterations: 1950, max_updates: 10000, lr: 0., ups: 0.52, time: 01m 36s 130ms, time_since_start: 01h 23m 17s 922ms, eta: 04h 26m 43s 164ms\n",
            "\u001b[32m2021-05-01T04:57:19 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[32m2021-05-01T04:57:19 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-05-01T04:57:28 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-05-01T04:57:40 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-05-01T04:57:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/10000, train/hateful_memes/cross_entropy: 0.4595, train/hateful_memes/cross_entropy/avg: 0.5653, train/total_loss: 0.4595, train/total_loss/avg: 0.5653, max mem: 8122.0, experiment: run, epoch: 8, num_updates: 2000, iterations: 2000, max_updates: 10000, lr: 0.00001, ups: 0.33, time: 02m 30s 235ms, time_since_start: 01h 25m 48s 158ms, eta: 06h 54m 15s 014ms\n",
            "\u001b[32m2021-05-01T04:57:40 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2021-05-01T04:57:41 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-01T04:57:41 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-01T04:57:41 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2021-05-01T04:58:27 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-05-01T04:58:32 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
            "\u001b[32m2021-05-01T04:58:45 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-05-01T04:59:01 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-05-01T04:59:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/10000, val/hateful_memes/cross_entropy: 0.8307, val/total_loss: 0.8307, val/hateful_memes/accuracy: 0.6500, val/hateful_memes/binary_f1: 0.3803, val/hateful_memes/roc_auc: 0.6591, num_updates: 2000, epoch: 8, iterations: 2000, max_updates: 10000, val_time: 01m 21s 078ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.659103\n",
            "\u001b[32m2021-05-01T05:01:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2050/10000, train/hateful_memes/cross_entropy: 0.4531, train/hateful_memes/cross_entropy/avg: 0.5616, train/total_loss: 0.4531, train/total_loss/avg: 0.5616, max mem: 8122.0, experiment: run, epoch: 8, num_updates: 2050, iterations: 2050, max_updates: 10000, lr: 0., ups: 0.42, time: 01m 58s 507ms, time_since_start: 01h 29m 07s 756ms, eta: 05h 24m 43s 370ms\n",
            "\u001b[32m2021-05-01T05:03:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2100/10000, train/hateful_memes/cross_entropy: 0.4531, train/hateful_memes/cross_entropy/avg: 0.5608, train/total_loss: 0.4531, train/total_loss/avg: 0.5608, max mem: 8122.0, experiment: run, epoch: 8, num_updates: 2100, iterations: 2100, max_updates: 10000, lr: 0., ups: 0.37, time: 02m 15s 495ms, time_since_start: 01h 31m 23s 251ms, eta: 06h 08m 56s 228ms\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-01T05:04:21 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-01T05:04:21 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2021-05-01T05:04:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2150/10000, train/hateful_memes/cross_entropy: 0.4351, train/hateful_memes/cross_entropy/avg: 0.5543, train/total_loss: 0.4351, train/total_loss/avg: 0.5543, max mem: 8122.0, experiment: run, epoch: 9, num_updates: 2150, iterations: 2150, max_updates: 10000, lr: 0., ups: 0.50, time: 01m 41s 263ms, time_since_start: 01h 33m 04s 515ms, eta: 04h 33m 58s 904ms\n",
            "\u001b[32m2021-05-01T05:06:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2200/10000, train/hateful_memes/cross_entropy: 0.4272, train/hateful_memes/cross_entropy/avg: 0.5465, train/total_loss: 0.4272, train/total_loss/avg: 0.5465, max mem: 8122.0, experiment: run, epoch: 9, num_updates: 2200, iterations: 2200, max_updates: 10000, lr: 0., ups: 0.54, time: 01m 32s 548ms, time_since_start: 01h 34m 37s 063ms, eta: 04h 08m 48s 374ms\n",
            "\u001b[32m2021-05-01T05:08:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2250/10000, train/hateful_memes/cross_entropy: 0.4161, train/hateful_memes/cross_entropy/avg: 0.5421, train/total_loss: 0.4161, train/total_loss/avg: 0.5421, max mem: 8122.0, experiment: run, epoch: 9, num_updates: 2250, iterations: 2250, max_updates: 10000, lr: 0., ups: 0.45, time: 01m 52s 878ms, time_since_start: 01h 36m 29s 942ms, eta: 05h 01m 31s 099ms\n",
            "\u001b[32m2021-05-01T05:10:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2300/10000, train/hateful_memes/cross_entropy: 0.4031, train/hateful_memes/cross_entropy/avg: 0.5360, train/total_loss: 0.4031, train/total_loss/avg: 0.5360, max mem: 8122.0, experiment: run, epoch: 9, num_updates: 2300, iterations: 2300, max_updates: 10000, lr: 0., ups: 0.38, time: 02m 12s 025ms, time_since_start: 01h 38m 41s 968ms, eta: 05h 50m 23s 288ms\n",
            "\u001b[32m2021-05-01T05:12:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2350/10000, train/hateful_memes/cross_entropy: 0.3912, train/hateful_memes/cross_entropy/avg: 0.5326, train/total_loss: 0.3912, train/total_loss/avg: 0.5326, max mem: 8122.0, experiment: run, epoch: 9, num_updates: 2350, iterations: 2350, max_updates: 10000, lr: 0., ups: 0.39, time: 02m 08s 179ms, time_since_start: 01h 40m 50s 147ms, eta: 05h 37m 58s 204ms\n",
            "\u001b[32m2021-05-01T05:14:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2400/10000, train/hateful_memes/cross_entropy: 0.3755, train/hateful_memes/cross_entropy/avg: 0.5248, train/total_loss: 0.3755, train/total_loss/avg: 0.5248, max mem: 8122.0, experiment: run, epoch: 10, num_updates: 2400, iterations: 2400, max_updates: 10000, lr: 0., ups: 0.40, time: 02m 05s 673ms, time_since_start: 01h 42m 55s 821ms, eta: 05h 29m 11s 850ms\n",
            "\u001b[32m2021-05-01T05:16:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2450/10000, train/hateful_memes/cross_entropy: 0.3755, train/hateful_memes/cross_entropy/avg: 0.5224, train/total_loss: 0.3755, train/total_loss/avg: 0.5224, max mem: 8122.0, experiment: run, epoch: 10, num_updates: 2450, iterations: 2450, max_updates: 10000, lr: 0., ups: 0.62, time: 01m 20s 676ms, time_since_start: 01h 44m 16s 498ms, eta: 03h 29m 56s 418ms\n",
            "\u001b[32m2021-05-01T05:17:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2500/10000, train/hateful_memes/cross_entropy: 0.3509, train/hateful_memes/cross_entropy/avg: 0.5177, train/total_loss: 0.3509, train/total_loss/avg: 0.5177, max mem: 8122.0, experiment: run, epoch: 10, num_updates: 2500, iterations: 2500, max_updates: 10000, lr: 0., ups: 0.48, time: 01m 44s 759ms, time_since_start: 01h 46m 01s 257ms, eta: 04h 30m 48s 171ms\n",
            "\u001b[32m2021-05-01T05:17:54 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2021-05-01T05:17:54 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[32m2021-05-01T05:18:53 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-05-01T05:18:59 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
            "\u001b[32m2021-05-01T05:19:12 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-05-01T05:19:30 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-05-01T05:19:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2500/10000, val/hateful_memes/cross_entropy: 0.8641, val/total_loss: 0.8641, val/hateful_memes/accuracy: 0.6741, val/hateful_memes/binary_f1: 0.5111, val/hateful_memes/roc_auc: 0.6891, num_updates: 2500, epoch: 10, iterations: 2500, max_updates: 10000, val_time: 01m 36s 572ms, best_update: 2500, best_iteration: 2500, best_val/hateful_memes/roc_auc: 0.689103\n",
            "\u001b[32m2021-05-01T05:21:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2550/10000, train/hateful_memes/cross_entropy: 0.3503, train/hateful_memes/cross_entropy/avg: 0.5136, train/total_loss: 0.3503, train/total_loss/avg: 0.5136, max mem: 8122.0, experiment: run, epoch: 10, num_updates: 2550, iterations: 2550, max_updates: 10000, lr: 0., ups: 0.43, time: 01m 57s 428ms, time_since_start: 01h 49m 35s 259ms, eta: 05h 01m 31s 745ms\n",
            "\u001b[32m2021-05-01T05:23:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2600/10000, train/hateful_memes/cross_entropy: 0.3476, train/hateful_memes/cross_entropy/avg: 0.5098, train/total_loss: 0.3476, train/total_loss/avg: 0.5098, max mem: 8122.0, experiment: run, epoch: 10, num_updates: 2600, iterations: 2600, max_updates: 10000, lr: 0., ups: 0.37, time: 02m 14s 259ms, time_since_start: 01h 51m 49s 519ms, eta: 05h 42m 26s 059ms\n",
            "\u001b[32m2021-05-01T05:25:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2650/10000, train/hateful_memes/cross_entropy: 0.3452, train/hateful_memes/cross_entropy/avg: 0.5058, train/total_loss: 0.3452, train/total_loss/avg: 0.5058, max mem: 8122.0, experiment: run, epoch: 10, num_updates: 2650, iterations: 2650, max_updates: 10000, lr: 0., ups: 0.38, time: 02m 12s 652ms, time_since_start: 01h 54m 02s 171ms, eta: 05h 36m 02s 851ms\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-01T05:26:13 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-01T05:26:13 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2021-05-01T05:27:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2700/10000, train/hateful_memes/cross_entropy: 0.3181, train/hateful_memes/cross_entropy/avg: 0.5022, train/total_loss: 0.3181, train/total_loss/avg: 0.5022, max mem: 8122.0, experiment: run, epoch: 11, num_updates: 2700, iterations: 2700, max_updates: 10000, lr: 0., ups: 0.58, time: 01m 26s 293ms, time_since_start: 01h 55m 28s 465ms, eta: 03h 37m 07s 188ms\n",
            "\u001b[32m2021-05-01T05:29:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2750/10000, train/hateful_memes/cross_entropy: 0.3173, train/hateful_memes/cross_entropy/avg: 0.4964, train/total_loss: 0.3173, train/total_loss/avg: 0.4964, max mem: 8122.0, experiment: run, epoch: 11, num_updates: 2750, iterations: 2750, max_updates: 10000, lr: 0., ups: 0.50, time: 01m 41s 645ms, time_since_start: 01h 57m 10s 110ms, eta: 04h 13m 59s 691ms\n",
            "\u001b[32m2021-05-01T05:30:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2800/10000, train/hateful_memes/cross_entropy: 0.3143, train/hateful_memes/cross_entropy/avg: 0.4889, train/total_loss: 0.3143, train/total_loss/avg: 0.4889, max mem: 8122.0, experiment: run, epoch: 11, num_updates: 2800, iterations: 2800, max_updates: 10000, lr: 0., ups: 0.43, time: 01m 56s 057ms, time_since_start: 01h 59m 06s 167ms, eta: 04h 48m 432ms\n",
            "\u001b[32m2021-05-01T05:33:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2850/10000, train/hateful_memes/cross_entropy: 0.3143, train/hateful_memes/cross_entropy/avg: 0.4862, train/total_loss: 0.3143, train/total_loss/avg: 0.4862, max mem: 8122.0, experiment: run, epoch: 11, num_updates: 2850, iterations: 2850, max_updates: 10000, lr: 0., ups: 0.37, time: 02m 15s 529ms, time_since_start: 02h 01m 21s 697ms, eta: 05h 33m 59s 739ms\n",
            "\u001b[32m2021-05-01T05:35:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2900/10000, train/hateful_memes/cross_entropy: 0.3085, train/hateful_memes/cross_entropy/avg: 0.4795, train/total_loss: 0.3085, train/total_loss/avg: 0.4795, max mem: 8122.0, experiment: run, epoch: 11, num_updates: 2900, iterations: 2900, max_updates: 10000, lr: 0., ups: 0.39, time: 02m 09s 773ms, time_since_start: 02h 03m 31s 470ms, eta: 05h 17m 34s 328ms\n",
            "\u001b[32m2021-05-01T05:37:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2950/10000, train/hateful_memes/cross_entropy: 0.2961, train/hateful_memes/cross_entropy/avg: 0.4738, train/total_loss: 0.2961, train/total_loss/avg: 0.4738, max mem: 8122.0, experiment: run, epoch: 12, num_updates: 2950, iterations: 2950, max_updates: 10000, lr: 0., ups: 0.49, time: 01m 43s 435ms, time_since_start: 02h 05m 14s 906ms, eta: 04h 11m 20s 314ms\n",
            "\u001b[32m2021-05-01T05:38:37 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[32m2021-05-01T05:38:37 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-05-01T05:38:45 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-05-01T05:38:59 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-05-01T05:38:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/10000, train/hateful_memes/cross_entropy: 0.2868, train/hateful_memes/cross_entropy/avg: 0.4666, train/total_loss: 0.2868, train/total_loss/avg: 0.4666, max mem: 8122.0, experiment: run, epoch: 12, num_updates: 3000, iterations: 3000, max_updates: 10000, lr: 0., ups: 0.45, time: 01m 51s 864ms, time_since_start: 02h 07m 06s 770ms, eta: 04h 29m 53s 479ms\n",
            "\u001b[32m2021-05-01T05:38:59 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2021-05-01T05:38:59 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-01T05:38:59 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-01T05:38:59 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2021-05-01T05:39:46 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-05-01T05:39:51 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-05-01T05:40:04 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-05-01T05:40:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/10000, val/hateful_memes/cross_entropy: 1.1120, val/total_loss: 1.1120, val/hateful_memes/accuracy: 0.6574, val/hateful_memes/binary_f1: 0.4272, val/hateful_memes/roc_auc: 0.6826, num_updates: 3000, epoch: 12, iterations: 3000, max_updates: 10000, val_time: 01m 05s 076ms, best_update: 2500, best_iteration: 2500, best_val/hateful_memes/roc_auc: 0.689103\n",
            "\u001b[32m2021-05-01T05:42:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3050/10000, train/hateful_memes/cross_entropy: 0.2868, train/hateful_memes/cross_entropy/avg: 0.4643, train/total_loss: 0.2868, train/total_loss/avg: 0.4643, max mem: 8122.0, experiment: run, epoch: 12, num_updates: 3050, iterations: 3050, max_updates: 10000, lr: 0., ups: 0.43, time: 01m 56s 686ms, time_since_start: 02h 10m 08s 538ms, eta: 04h 39m 30s 866ms\n",
            "\u001b[32m2021-05-01T05:44:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3100/10000, train/hateful_memes/cross_entropy: 0.2849, train/hateful_memes/cross_entropy/avg: 0.4614, train/total_loss: 0.2849, train/total_loss/avg: 0.4614, max mem: 8122.0, experiment: run, epoch: 12, num_updates: 3100, iterations: 3100, max_updates: 10000, lr: 0., ups: 0.39, time: 02m 07s 748ms, time_since_start: 02h 12m 16s 286ms, eta: 05h 03m 48s 740ms\n",
            "\u001b[32m2021-05-01T05:46:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3150/10000, train/hateful_memes/cross_entropy: 0.2849, train/hateful_memes/cross_entropy/avg: 0.4567, train/total_loss: 0.2849, train/total_loss/avg: 0.4567, max mem: 8122.0, experiment: run, epoch: 12, num_updates: 3150, iterations: 3150, max_updates: 10000, lr: 0., ups: 0.37, time: 02m 14s 453ms, time_since_start: 02h 14m 30s 740ms, eta: 05h 17m 26s 393ms\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-01T05:48:07 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-01T05:48:07 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2021-05-01T05:48:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3200/10000, train/hateful_memes/cross_entropy: 0.2849, train/hateful_memes/cross_entropy/avg: 0.4502, train/total_loss: 0.2849, train/total_loss/avg: 0.4502, max mem: 8122.0, experiment: run, epoch: 13, num_updates: 3200, iterations: 3200, max_updates: 10000, lr: 0., ups: 0.42, time: 01m 59s 668ms, time_since_start: 02h 16m 30s 409ms, eta: 04h 40m 28s 286ms\n",
            "\u001b[32m2021-05-01T05:49:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3250/10000, train/hateful_memes/cross_entropy: 0.2596, train/hateful_memes/cross_entropy/avg: 0.4453, train/total_loss: 0.2596, train/total_loss/avg: 0.4453, max mem: 8122.0, experiment: run, epoch: 13, num_updates: 3250, iterations: 3250, max_updates: 10000, lr: 0., ups: 0.62, time: 01m 21s 841ms, time_since_start: 02h 17m 52s 250ms, eta: 03h 10m 24s 190ms\n",
            "\u001b[32m2021-05-01T05:51:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3300/10000, train/hateful_memes/cross_entropy: 0.1806, train/hateful_memes/cross_entropy/avg: 0.4388, train/total_loss: 0.1806, train/total_loss/avg: 0.4388, max mem: 8122.0, experiment: run, epoch: 13, num_updates: 3300, iterations: 3300, max_updates: 10000, lr: 0., ups: 0.48, time: 01m 45s 600ms, time_since_start: 02h 19m 37s 850ms, eta: 04h 03m 51s 592ms\n",
            "\u001b[32m2021-05-01T05:53:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3350/10000, train/hateful_memes/cross_entropy: 0.1675, train/hateful_memes/cross_entropy/avg: 0.4346, train/total_loss: 0.1675, train/total_loss/avg: 0.4346, max mem: 8122.0, experiment: run, epoch: 13, num_updates: 3350, iterations: 3350, max_updates: 10000, lr: 0., ups: 0.38, time: 02m 10s 031ms, time_since_start: 02h 21m 47s 881ms, eta: 04h 58m 02s 127ms\n",
            "\u001b[32m2021-05-01T05:55:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3400/10000, train/hateful_memes/cross_entropy: 0.1675, train/hateful_memes/cross_entropy/avg: 0.4293, train/total_loss: 0.1675, train/total_loss/avg: 0.4293, max mem: 8122.0, experiment: run, epoch: 13, num_updates: 3400, iterations: 3400, max_updates: 10000, lr: 0., ups: 0.38, time: 02m 11s 768ms, time_since_start: 02h 23m 59s 650ms, eta: 04h 59m 44s 801ms\n",
            "\u001b[32m2021-05-01T05:58:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3450/10000, train/hateful_memes/cross_entropy: 0.1623, train/hateful_memes/cross_entropy/avg: 0.4237, train/total_loss: 0.1623, train/total_loss/avg: 0.4237, max mem: 8122.0, experiment: run, epoch: 13, num_updates: 3450, iterations: 3450, max_updates: 10000, lr: 0., ups: 0.37, time: 02m 15s 919ms, time_since_start: 02h 26m 15s 569ms, eta: 05h 06m 50s 798ms\n",
            "\u001b[32m2021-05-01T05:59:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3500/10000, train/hateful_memes/cross_entropy: 0.1448, train/hateful_memes/cross_entropy/avg: 0.4179, train/total_loss: 0.1448, train/total_loss/avg: 0.4179, max mem: 8122.0, experiment: run, epoch: 14, num_updates: 3500, iterations: 3500, max_updates: 10000, lr: 0., ups: 0.64, time: 01m 18s 246ms, time_since_start: 02h 27m 33s 815ms, eta: 02h 55m 17s 868ms\n",
            "\u001b[32m2021-05-01T05:59:26 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2021-05-01T05:59:26 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[32m2021-05-01T06:00:18 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-05-01T06:00:23 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
            "\u001b[32m2021-05-01T06:00:38 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-05-01T06:00:53 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-05-01T06:00:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3500/10000, val/hateful_memes/cross_entropy: 1.2269, val/total_loss: 1.2269, val/hateful_memes/accuracy: 0.6796, val/hateful_memes/binary_f1: 0.4508, val/hateful_memes/roc_auc: 0.6977, num_updates: 3500, epoch: 14, iterations: 3500, max_updates: 10000, val_time: 01m 26s 653ms, best_update: 3500, best_iteration: 3500, best_val/hateful_memes/roc_auc: 0.697721\n",
            "\u001b[32m2021-05-01T06:02:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3550/10000, train/hateful_memes/cross_entropy: 0.1281, train/hateful_memes/cross_entropy/avg: 0.4132, train/total_loss: 0.1281, train/total_loss/avg: 0.4132, max mem: 8122.0, experiment: run, epoch: 14, num_updates: 3550, iterations: 3550, max_updates: 10000, lr: 0., ups: 0.45, time: 01m 50s 332ms, time_since_start: 02h 30m 50s 804ms, eta: 04h 05m 16s 803ms\n",
            "\u001b[32m2021-05-01T06:04:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3600/10000, train/hateful_memes/cross_entropy: 0.0971, train/hateful_memes/cross_entropy/avg: 0.4081, train/total_loss: 0.0971, train/total_loss/avg: 0.4081, max mem: 8122.0, experiment: run, epoch: 14, num_updates: 3600, iterations: 3600, max_updates: 10000, lr: 0., ups: 0.41, time: 02m 03s 350ms, time_since_start: 02h 32m 54s 155ms, eta: 04h 32m 05s 735ms\n",
            "\u001b[32m2021-05-01T06:06:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3650/10000, train/hateful_memes/cross_entropy: 0.0791, train/hateful_memes/cross_entropy/avg: 0.4033, train/total_loss: 0.0791, train/total_loss/avg: 0.4033, max mem: 8122.0, experiment: run, epoch: 14, num_updates: 3650, iterations: 3650, max_updates: 10000, lr: 0., ups: 0.40, time: 02m 04s 611ms, time_since_start: 02h 34m 58s 766ms, eta: 04h 32m 43s 696ms\n",
            "\u001b[32m2021-05-01T06:08:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3700/10000, train/hateful_memes/cross_entropy: 0.0791, train/hateful_memes/cross_entropy/avg: 0.4001, train/total_loss: 0.0791, train/total_loss/avg: 0.4001, max mem: 8122.0, experiment: run, epoch: 14, num_updates: 3700, iterations: 3700, max_updates: 10000, lr: 0., ups: 0.40, time: 02m 04s 192ms, time_since_start: 02h 37m 02s 958ms, eta: 04h 29m 40s 254ms\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-01T06:09:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-01T06:09:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2021-05-01T06:10:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3750/10000, train/hateful_memes/cross_entropy: 0.0749, train/hateful_memes/cross_entropy/avg: 0.3954, train/total_loss: 0.0749, train/total_loss/avg: 0.3954, max mem: 8122.0, experiment: run, epoch: 15, num_updates: 3750, iterations: 3750, max_updates: 10000, lr: 0., ups: 0.52, time: 01m 37s 541ms, time_since_start: 02h 38m 40s 500ms, eta: 03h 30m 07s 304ms\n",
            "\u001b[32m2021-05-01T06:11:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3800/10000, train/hateful_memes/cross_entropy: 0.0791, train/hateful_memes/cross_entropy/avg: 0.3933, train/total_loss: 0.0791, train/total_loss/avg: 0.3933, max mem: 8122.0, experiment: run, epoch: 15, num_updates: 3800, iterations: 3800, max_updates: 10000, lr: 0., ups: 0.58, time: 01m 26s 449ms, time_since_start: 02h 40m 06s 949ms, eta: 03h 04m 44s 166ms\n",
            "\u001b[32m2021-05-01T06:13:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3850/10000, train/hateful_memes/cross_entropy: 0.0726, train/hateful_memes/cross_entropy/avg: 0.3890, train/total_loss: 0.0726, train/total_loss/avg: 0.3890, max mem: 8122.0, experiment: run, epoch: 15, num_updates: 3850, iterations: 3850, max_updates: 10000, lr: 0., ups: 0.44, time: 01m 54s 154ms, time_since_start: 02h 42m 01s 104ms, eta: 04h 01m 58s 452ms\n",
            "\u001b[32m2021-05-01T06:15:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3900/10000, train/hateful_memes/cross_entropy: 0.0726, train/hateful_memes/cross_entropy/avg: 0.3856, train/total_loss: 0.0726, train/total_loss/avg: 0.3856, max mem: 8122.0, experiment: run, epoch: 15, num_updates: 3900, iterations: 3900, max_updates: 10000, lr: 0., ups: 0.40, time: 02m 05s 717ms, time_since_start: 02h 44m 06s 822ms, eta: 04h 24m 19s 003ms\n",
            "\u001b[32m2021-05-01T06:18:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3950/10000, train/hateful_memes/cross_entropy: 0.0646, train/hateful_memes/cross_entropy/avg: 0.3815, train/total_loss: 0.0646, train/total_loss/avg: 0.3815, max mem: 8122.0, experiment: run, epoch: 15, num_updates: 3950, iterations: 3950, max_updates: 10000, lr: 0., ups: 0.37, time: 02m 16s 332ms, time_since_start: 02h 46m 23s 154ms, eta: 04h 44m 17s 113ms\n",
            "\u001b[32m2021-05-01T06:20:10 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[32m2021-05-01T06:20:10 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-05-01T06:20:18 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-05-01T06:20:31 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-05-01T06:20:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4000/10000, train/hateful_memes/cross_entropy: 0.0646, train/hateful_memes/cross_entropy/avg: 0.3771, train/total_loss: 0.0646, train/total_loss/avg: 0.3771, max mem: 8122.0, experiment: run, epoch: 16, num_updates: 4000, iterations: 4000, max_updates: 10000, lr: 0., ups: 0.37, time: 02m 15s 142ms, time_since_start: 02h 48m 38s 297ms, eta: 04h 39m 28s 553ms\n",
            "\u001b[32m2021-05-01T06:20:31 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2021-05-01T06:20:33 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-01T06:20:33 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-01T06:20:33 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2021-05-01T06:21:18 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-05-01T06:21:24 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-05-01T06:21:36 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-05-01T06:21:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4000/10000, val/hateful_memes/cross_entropy: 1.4463, val/total_loss: 1.4463, val/hateful_memes/accuracy: 0.6741, val/hateful_memes/binary_f1: 0.4395, val/hateful_memes/roc_auc: 0.6865, num_updates: 4000, epoch: 16, iterations: 4000, max_updates: 10000, val_time: 01m 05s 364ms, best_update: 3500, best_iteration: 3500, best_val/hateful_memes/roc_auc: 0.697721\n",
            "\u001b[32m2021-05-01T06:23:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4050/10000, train/hateful_memes/cross_entropy: 0.0645, train/hateful_memes/cross_entropy/avg: 0.3728, train/total_loss: 0.0645, train/total_loss/avg: 0.3728, max mem: 8122.0, experiment: run, epoch: 16, num_updates: 4050, iterations: 4050, max_updates: 10000, lr: 0., ups: 0.46, time: 01m 48s 718ms, time_since_start: 02h 51m 32s 383ms, eta: 03h 42m 57s 325ms\n",
            "\u001b[32m2021-05-01T06:25:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4100/10000, train/hateful_memes/cross_entropy: 0.0606, train/hateful_memes/cross_entropy/avg: 0.3688, train/total_loss: 0.0606, train/total_loss/avg: 0.3688, max mem: 8122.0, experiment: run, epoch: 16, num_updates: 4100, iterations: 4100, max_updates: 10000, lr: 0., ups: 0.41, time: 02m 01s 670ms, time_since_start: 02h 53m 34s 053ms, eta: 04h 07m 25s 206ms\n",
            "\u001b[32m2021-05-01T06:27:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4150/10000, train/hateful_memes/cross_entropy: 0.0606, train/hateful_memes/cross_entropy/avg: 0.3654, train/total_loss: 0.0606, train/total_loss/avg: 0.3654, max mem: 8122.0, experiment: run, epoch: 16, num_updates: 4150, iterations: 4150, max_updates: 10000, lr: 0., ups: 0.42, time: 02m 593ms, time_since_start: 02h 55m 34s 646ms, eta: 04h 03m 09s 151ms\n",
            "\u001b[32m2021-05-01T06:29:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4200/10000, train/hateful_memes/cross_entropy: 0.0606, train/hateful_memes/cross_entropy/avg: 0.3616, train/total_loss: 0.0606, train/total_loss/avg: 0.3616, max mem: 8122.0, experiment: run, epoch: 16, num_updates: 4200, iterations: 4200, max_updates: 10000, lr: 0., ups: 0.40, time: 02m 05s 206ms, time_since_start: 02h 57m 39s 853ms, eta: 04h 10m 17s 822ms\n",
            "\u001b[32m2021-05-01T06:31:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4250/10000, train/hateful_memes/cross_entropy: 0.0580, train/hateful_memes/cross_entropy/avg: 0.3580, train/total_loss: 0.0580, train/total_loss/avg: 0.3580, max mem: 8122.0, experiment: run, epoch: 16, num_updates: 4250, iterations: 4250, max_updates: 10000, lr: 0., ups: 0.41, time: 02m 03s 104ms, time_since_start: 02h 59m 42s 958ms, eta: 04h 03m 58s 396ms\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-01T06:31:43 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-01T06:31:43 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2021-05-01T06:32:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4300/10000, train/hateful_memes/cross_entropy: 0.0606, train/hateful_memes/cross_entropy/avg: 0.3549, train/total_loss: 0.0606, train/total_loss/avg: 0.3549, max mem: 8122.0, experiment: run, epoch: 17, num_updates: 4300, iterations: 4300, max_updates: 10000, lr: 0., ups: 0.62, time: 01m 21s 070ms, time_since_start: 03h 01m 04s 029ms, eta: 02h 39m 16s 325ms\n",
            "\u001b[32m2021-05-01T06:34:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4350/10000, train/hateful_memes/cross_entropy: 0.0580, train/hateful_memes/cross_entropy/avg: 0.3513, train/total_loss: 0.0580, train/total_loss/avg: 0.3513, max mem: 8122.0, experiment: run, epoch: 17, num_updates: 4350, iterations: 4350, max_updates: 10000, lr: 0., ups: 0.52, time: 01m 37s 628ms, time_since_start: 03h 02m 41s 657ms, eta: 03h 10m 07s 095ms\n",
            "\u001b[32m2021-05-01T06:36:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4400/10000, train/hateful_memes/cross_entropy: 0.0494, train/hateful_memes/cross_entropy/avg: 0.3475, train/total_loss: 0.0494, train/total_loss/avg: 0.3475, max mem: 8122.0, experiment: run, epoch: 17, num_updates: 4400, iterations: 4400, max_updates: 10000, lr: 0., ups: 0.40, time: 02m 06s 421ms, time_since_start: 03h 04m 48s 079ms, eta: 04h 04m 660ms\n",
            "\u001b[32m2021-05-01T06:38:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4450/10000, train/hateful_memes/cross_entropy: 0.0494, train/hateful_memes/cross_entropy/avg: 0.3439, train/total_loss: 0.0494, train/total_loss/avg: 0.3439, max mem: 8122.0, experiment: run, epoch: 17, num_updates: 4450, iterations: 4450, max_updates: 10000, lr: 0., ups: 0.39, time: 02m 08s 205ms, time_since_start: 03h 06m 56s 285ms, eta: 04h 05m 14s 708ms\n",
            "\u001b[32m2021-05-01T06:41:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4500/10000, train/hateful_memes/cross_entropy: 0.0494, train/hateful_memes/cross_entropy/avg: 0.3404, train/total_loss: 0.0494, train/total_loss/avg: 0.3404, max mem: 8122.0, experiment: run, epoch: 17, num_updates: 4500, iterations: 4500, max_updates: 10000, lr: 0., ups: 0.38, time: 02m 11s 161ms, time_since_start: 03h 09m 07s 447ms, eta: 04h 08m 38s 343ms\n",
            "\u001b[32m2021-05-01T06:41:00 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2021-05-01T06:41:00 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[32m2021-05-01T06:42:00 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-05-01T06:42:06 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-05-01T06:42:18 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-05-01T06:42:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4500/10000, val/hateful_memes/cross_entropy: 1.5671, val/total_loss: 1.5671, val/hateful_memes/accuracy: 0.6759, val/hateful_memes/binary_f1: 0.4373, val/hateful_memes/roc_auc: 0.6846, num_updates: 4500, epoch: 17, iterations: 4500, max_updates: 10000, val_time: 01m 18s 484ms, best_update: 3500, best_iteration: 3500, best_val/hateful_memes/roc_auc: 0.697721\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-01T06:42:57 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-01T06:42:57 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2021-05-01T06:44:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4550/10000, train/hateful_memes/cross_entropy: 0.0494, train/hateful_memes/cross_entropy/avg: 0.3377, train/total_loss: 0.0494, train/total_loss/avg: 0.3377, max mem: 8122.0, experiment: run, epoch: 18, num_updates: 4550, iterations: 4550, max_updates: 10000, lr: 0., ups: 0.50, time: 01m 41s 774ms, time_since_start: 03h 12m 07s 759ms, eta: 03h 11m 10s 569ms\n",
            "\u001b[32m2021-05-01T06:45:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4600/10000, train/hateful_memes/cross_entropy: 0.0468, train/hateful_memes/cross_entropy/avg: 0.3345, train/total_loss: 0.0468, train/total_loss/avg: 0.3345, max mem: 8122.0, experiment: run, epoch: 18, num_updates: 4600, iterations: 4600, max_updates: 10000, lr: 0., ups: 0.44, time: 01m 54s 422ms, time_since_start: 03h 14m 02s 181ms, eta: 03h 32m 57s 788ms\n",
            "\u001b[32m2021-05-01T06:47:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4650/10000, train/hateful_memes/cross_entropy: 0.0458, train/hateful_memes/cross_entropy/avg: 0.3311, train/total_loss: 0.0458, train/total_loss/avg: 0.3311, max mem: 8122.0, experiment: run, epoch: 18, num_updates: 4650, iterations: 4650, max_updates: 10000, lr: 0., ups: 0.42, time: 01m 59s 426ms, time_since_start: 03h 16m 01s 608ms, eta: 03h 40m 13s 145ms\n",
            "\u001b[32m2021-05-01T06:50:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4700/10000, train/hateful_memes/cross_entropy: 0.0458, train/hateful_memes/cross_entropy/avg: 0.3286, train/total_loss: 0.0458, train/total_loss/avg: 0.3286, max mem: 8122.0, experiment: run, epoch: 18, num_updates: 4700, iterations: 4700, max_updates: 10000, lr: 0., ups: 0.38, time: 02m 10s 761ms, time_since_start: 03h 18m 12s 369ms, eta: 03h 58m 51s 940ms\n",
            "\u001b[32m2021-05-01T06:52:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4750/10000, train/hateful_memes/cross_entropy: 0.0468, train/hateful_memes/cross_entropy/avg: 0.3261, train/total_loss: 0.0468, train/total_loss/avg: 0.3261, max mem: 8122.0, experiment: run, epoch: 18, num_updates: 4750, iterations: 4750, max_updates: 10000, lr: 0., ups: 0.40, time: 02m 05s 418ms, time_since_start: 03h 20m 17s 788ms, eta: 03h 46m 56s 686ms\n",
            "\u001b[32m2021-05-01T06:54:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4800/10000, train/hateful_memes/cross_entropy: 0.0468, train/hateful_memes/cross_entropy/avg: 0.3251, train/total_loss: 0.0468, train/total_loss/avg: 0.3251, max mem: 8122.0, experiment: run, epoch: 19, num_updates: 4800, iterations: 4800, max_updates: 10000, lr: 0., ups: 0.43, time: 01m 55s 987ms, time_since_start: 03h 22m 13s 776ms, eta: 03h 27m 52s 859ms\n",
            "\u001b[32m2021-05-01T06:55:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4850/10000, train/hateful_memes/cross_entropy: 0.0458, train/hateful_memes/cross_entropy/avg: 0.3220, train/total_loss: 0.0458, train/total_loss/avg: 0.3220, max mem: 8122.0, experiment: run, epoch: 19, num_updates: 4850, iterations: 4850, max_updates: 10000, lr: 0., ups: 0.57, time: 01m 27s 895ms, time_since_start: 03h 23m 41s 672ms, eta: 02h 36m 01s 088ms\n",
            "\u001b[32m2021-05-01T06:57:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4900/10000, train/hateful_memes/cross_entropy: 0.0458, train/hateful_memes/cross_entropy/avg: 0.3195, train/total_loss: 0.0458, train/total_loss/avg: 0.3195, max mem: 8122.0, experiment: run, epoch: 19, num_updates: 4900, iterations: 4900, max_updates: 10000, lr: 0., ups: 0.46, time: 01m 49s 084ms, time_since_start: 03h 25m 30s 756ms, eta: 03h 11m 44s 895ms\n",
            "\u001b[32m2021-05-01T06:59:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4950/10000, train/hateful_memes/cross_entropy: 0.0458, train/hateful_memes/cross_entropy/avg: 0.3171, train/total_loss: 0.0458, train/total_loss/avg: 0.3171, max mem: 8122.0, experiment: run, epoch: 19, num_updates: 4950, iterations: 4950, max_updates: 10000, lr: 0., ups: 0.38, time: 02m 13s 414ms, time_since_start: 03h 27m 44s 171ms, eta: 03h 52m 13s 053ms\n",
            "\u001b[32m2021-05-01T07:01:45 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[32m2021-05-01T07:01:45 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-05-01T07:01:53 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-05-01T07:02:07 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-05-01T07:02:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5000/10000, train/hateful_memes/cross_entropy: 0.0458, train/hateful_memes/cross_entropy/avg: 0.3144, train/total_loss: 0.0458, train/total_loss/avg: 0.3144, max mem: 8122.0, experiment: run, epoch: 19, num_updates: 5000, iterations: 5000, max_updates: 10000, lr: 0., ups: 0.33, time: 02m 30s 849ms, time_since_start: 03h 30m 15s 020ms, eta: 04h 19m 57s 809ms\n",
            "\u001b[32m2021-05-01T07:02:07 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2021-05-01T07:02:07 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-01T07:02:07 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-01T07:02:07 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2021-05-01T07:02:54 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-05-01T07:03:00 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
            "\u001b[32m2021-05-01T07:03:12 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-05-01T07:03:30 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-05-01T07:03:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5000/10000, val/hateful_memes/cross_entropy: 1.4935, val/total_loss: 1.4935, val/hateful_memes/accuracy: 0.6648, val/hateful_memes/binary_f1: 0.4723, val/hateful_memes/roc_auc: 0.7010, num_updates: 5000, epoch: 19, iterations: 5000, max_updates: 10000, val_time: 01m 22s 415ms, best_update: 5000, best_iteration: 5000, best_val/hateful_memes/roc_auc: 0.700974\n",
            "\u001b[32m2021-05-01T07:05:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5050/10000, train/hateful_memes/cross_entropy: 0.0468, train/hateful_memes/cross_entropy/avg: 0.3128, train/total_loss: 0.0468, train/total_loss/avg: 0.3128, max mem: 8122.0, experiment: run, epoch: 19, num_updates: 5050, iterations: 5050, max_updates: 10000, lr: 0., ups: 0.42, time: 01m 59s 462ms, time_since_start: 03h 33m 36s 900ms, eta: 03h 23m 48s 926ms\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-01T07:05:32 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-01T07:05:32 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2021-05-01T07:07:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5100/10000, train/hateful_memes/cross_entropy: 0.0458, train/hateful_memes/cross_entropy/avg: 0.3098, train/total_loss: 0.0458, train/total_loss/avg: 0.3098, max mem: 8122.0, experiment: run, epoch: 20, num_updates: 5100, iterations: 5100, max_updates: 10000, lr: 0., ups: 0.52, time: 01m 37s 269ms, time_since_start: 03h 35m 14s 170ms, eta: 02h 44m 16s 471ms\n",
            "\u001b[32m2021-05-01T07:08:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5150/10000, train/hateful_memes/cross_entropy: 0.0447, train/hateful_memes/cross_entropy/avg: 0.3068, train/total_loss: 0.0447, train/total_loss/avg: 0.3068, max mem: 8122.0, experiment: run, epoch: 20, num_updates: 5150, iterations: 5150, max_updates: 10000, lr: 0., ups: 0.45, time: 01m 52s 599ms, time_since_start: 03h 37m 06s 769ms, eta: 03h 08m 13s 485ms\n",
            "\u001b[32m2021-05-01T07:11:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5200/10000, train/hateful_memes/cross_entropy: 0.0444, train/hateful_memes/cross_entropy/avg: 0.3042, train/total_loss: 0.0444, train/total_loss/avg: 0.3042, max mem: 8122.0, experiment: run, epoch: 20, num_updates: 5200, iterations: 5200, max_updates: 10000, lr: 0., ups: 0.39, time: 02m 08s 565ms, time_since_start: 03h 39m 15s 334ms, eta: 03h 32m 41s 891ms\n",
            "\u001b[32m2021-05-01T07:13:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5250/10000, train/hateful_memes/cross_entropy: 0.0417, train/hateful_memes/cross_entropy/avg: 0.3015, train/total_loss: 0.0417, train/total_loss/avg: 0.3015, max mem: 8122.0, experiment: run, epoch: 20, num_updates: 5250, iterations: 5250, max_updates: 10000, lr: 0., ups: 0.40, time: 02m 06s 631ms, time_since_start: 03h 41m 21s 965ms, eta: 03h 27m 18s 993ms\n",
            "\u001b[32m2021-05-01T07:15:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5300/10000, train/hateful_memes/cross_entropy: 0.0417, train/hateful_memes/cross_entropy/avg: 0.3007, train/total_loss: 0.0417, train/total_loss/avg: 0.3007, max mem: 8122.0, experiment: run, epoch: 20, num_updates: 5300, iterations: 5300, max_updates: 10000, lr: 0., ups: 0.37, time: 02m 14s 224ms, time_since_start: 03h 43m 36s 190ms, eta: 03h 37m 26s 057ms\n",
            "\u001b[32m2021-05-01T07:17:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5350/10000, train/hateful_memes/cross_entropy: 0.0359, train/hateful_memes/cross_entropy/avg: 0.2979, train/total_loss: 0.0359, train/total_loss/avg: 0.2979, max mem: 8122.0, experiment: run, epoch: 21, num_updates: 5350, iterations: 5350, max_updates: 10000, lr: 0., ups: 0.51, time: 01m 39s 280ms, time_since_start: 03h 45m 15s 470ms, eta: 02h 39m 07s 016ms\n",
            "\u001b[32m2021-05-01T07:18:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5400/10000, train/hateful_memes/cross_entropy: 0.0359, train/hateful_memes/cross_entropy/avg: 0.2954, train/total_loss: 0.0359, train/total_loss/avg: 0.2954, max mem: 8122.0, experiment: run, epoch: 21, num_updates: 5400, iterations: 5400, max_updates: 10000, lr: 0., ups: 0.55, time: 01m 31s 489ms, time_since_start: 03h 46m 46s 959ms, eta: 02h 25m 03s 184ms\n",
            "\u001b[32m2021-05-01T07:20:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5450/10000, train/hateful_memes/cross_entropy: 0.0359, train/hateful_memes/cross_entropy/avg: 0.2928, train/total_loss: 0.0359, train/total_loss/avg: 0.2928, max mem: 8122.0, experiment: run, epoch: 21, num_updates: 5450, iterations: 5450, max_updates: 10000, lr: 0., ups: 0.40, time: 02m 04s 406ms, time_since_start: 03h 48m 51s 366ms, eta: 03h 15m 05s 879ms\n",
            "\u001b[32m2021-05-01T07:22:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5500/10000, train/hateful_memes/cross_entropy: 0.0359, train/hateful_memes/cross_entropy/avg: 0.2902, train/total_loss: 0.0359, train/total_loss/avg: 0.2902, max mem: 8122.0, experiment: run, epoch: 21, num_updates: 5500, iterations: 5500, max_updates: 10000, lr: 0., ups: 0.39, time: 02m 07s 751ms, time_since_start: 03h 50m 59s 118ms, eta: 03h 18m 08s 602ms\n",
            "\u001b[32m2021-05-01T07:22:51 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2021-05-01T07:22:52 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[32m2021-05-01T07:23:52 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-05-01T07:23:58 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-05-01T07:24:11 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-05-01T07:24:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5500/10000, val/hateful_memes/cross_entropy: 1.7874, val/total_loss: 1.7874, val/hateful_memes/accuracy: 0.6926, val/hateful_memes/binary_f1: 0.4780, val/hateful_memes/roc_auc: 0.6792, num_updates: 5500, epoch: 21, iterations: 5500, max_updates: 10000, val_time: 01m 19s 661ms, best_update: 5000, best_iteration: 5000, best_val/hateful_memes/roc_auc: 0.700974\n",
            "\u001b[32m2021-05-01T07:26:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5550/10000, train/hateful_memes/cross_entropy: 0.0359, train/hateful_memes/cross_entropy/avg: 0.2879, train/total_loss: 0.0359, train/total_loss/avg: 0.2879, max mem: 8122.0, experiment: run, epoch: 21, num_updates: 5550, iterations: 5550, max_updates: 10000, lr: 0., ups: 0.42, time: 01m 58s 246ms, time_since_start: 03h 54m 17s 027ms, eta: 03h 01m 21s 777ms\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-01T07:27:39 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-01T07:27:39 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2021-05-01T07:28:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5600/10000, train/hateful_memes/cross_entropy: 0.0243, train/hateful_memes/cross_entropy/avg: 0.2856, train/total_loss: 0.0243, train/total_loss/avg: 0.2856, max mem: 8122.0, experiment: run, epoch: 22, num_updates: 5600, iterations: 5600, max_updates: 10000, lr: 0., ups: 0.42, time: 01m 58s 365ms, time_since_start: 03h 56m 15s 393ms, eta: 02h 59m 30s 318ms\n",
            "\u001b[32m2021-05-01T07:29:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5650/10000, train/hateful_memes/cross_entropy: 0.0243, train/hateful_memes/cross_entropy/avg: 0.2830, train/total_loss: 0.0243, train/total_loss/avg: 0.2830, max mem: 8122.0, experiment: run, epoch: 22, num_updates: 5650, iterations: 5650, max_updates: 10000, lr: 0., ups: 0.54, time: 01m 32s 689ms, time_since_start: 03h 57m 48s 082ms, eta: 02h 18m 58s 159ms\n",
            "\u001b[32m2021-05-01T07:31:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5700/10000, train/hateful_memes/cross_entropy: 0.0235, train/hateful_memes/cross_entropy/avg: 0.2806, train/total_loss: 0.0235, train/total_loss/avg: 0.2806, max mem: 8122.0, experiment: run, epoch: 22, num_updates: 5700, iterations: 5700, max_updates: 10000, lr: 0., ups: 0.42, time: 02m 744ms, time_since_start: 03h 59m 48s 827ms, eta: 02h 58m 57s 054ms\n",
            "\u001b[32m2021-05-01T07:33:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5750/10000, train/hateful_memes/cross_entropy: 0.0232, train/hateful_memes/cross_entropy/avg: 0.2782, train/total_loss: 0.0232, train/total_loss/avg: 0.2782, max mem: 8122.0, experiment: run, epoch: 22, num_updates: 5750, iterations: 5750, max_updates: 10000, lr: 0., ups: 0.40, time: 02m 05s 204ms, time_since_start: 04h 01m 54s 031ms, eta: 03h 03m 24s 188ms\n",
            "\u001b[32m2021-05-01T07:36:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5800/10000, train/hateful_memes/cross_entropy: 0.0128, train/hateful_memes/cross_entropy/avg: 0.2758, train/total_loss: 0.0128, train/total_loss/avg: 0.2758, max mem: 8122.0, experiment: run, epoch: 22, num_updates: 5800, iterations: 5800, max_updates: 10000, lr: 0., ups: 0.37, time: 02m 16s 315ms, time_since_start: 04h 04m 10s 346ms, eta: 03h 17m 19s 795ms\n",
            "\u001b[32m2021-05-01T07:38:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5850/10000, train/hateful_memes/cross_entropy: 0.0119, train/hateful_memes/cross_entropy/avg: 0.2735, train/total_loss: 0.0119, train/total_loss/avg: 0.2735, max mem: 8122.0, experiment: run, epoch: 22, num_updates: 5850, iterations: 5850, max_updates: 10000, lr: 0., ups: 0.40, time: 02m 04s 937ms, time_since_start: 04h 06m 15s 283ms, eta: 02h 58m 42s 375ms\n",
            "\u001b[32m2021-05-01T07:39:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5900/10000, train/hateful_memes/cross_entropy: 0.0112, train/hateful_memes/cross_entropy/avg: 0.2713, train/total_loss: 0.0112, train/total_loss/avg: 0.2713, max mem: 8122.0, experiment: run, epoch: 23, num_updates: 5900, iterations: 5900, max_updates: 10000, lr: 0., ups: 0.59, time: 01m 25s 079ms, time_since_start: 04h 07m 40s 363ms, eta: 02h 13s 693ms\n",
            "\u001b[32m2021-05-01T07:41:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5950/10000, train/hateful_memes/cross_entropy: 0.0112, train/hateful_memes/cross_entropy/avg: 0.2698, train/total_loss: 0.0112, train/total_loss/avg: 0.2698, max mem: 8122.0, experiment: run, epoch: 23, num_updates: 5950, iterations: 5950, max_updates: 10000, lr: 0., ups: 0.47, time: 01m 47s 600ms, time_since_start: 04h 09m 27s 963ms, eta: 02h 30m 11s 946ms\n",
            "\u001b[32m2021-05-01T07:43:23 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[32m2021-05-01T07:43:23 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-05-01T07:43:31 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-05-01T07:43:45 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-05-01T07:43:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6000/10000, train/hateful_memes/cross_entropy: 0.0112, train/hateful_memes/cross_entropy/avg: 0.2679, train/total_loss: 0.0112, train/total_loss/avg: 0.2679, max mem: 8122.0, experiment: run, epoch: 23, num_updates: 6000, iterations: 6000, max_updates: 10000, lr: 0., ups: 0.35, time: 02m 24s 392ms, time_since_start: 04h 11m 52s 355ms, eta: 03h 19m 04s 113ms\n",
            "\u001b[32m2021-05-01T07:43:45 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2021-05-01T07:43:45 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-01T07:43:45 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-01T07:43:45 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2021-05-01T07:44:36 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-05-01T07:44:42 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-05-01T07:44:56 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-05-01T07:44:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6000/10000, val/hateful_memes/cross_entropy: 1.8585, val/total_loss: 1.8585, val/hateful_memes/accuracy: 0.6815, val/hateful_memes/binary_f1: 0.4452, val/hateful_memes/roc_auc: 0.6731, num_updates: 6000, epoch: 23, iterations: 6000, max_updates: 10000, val_time: 01m 11s 017ms, best_update: 5000, best_iteration: 5000, best_val/hateful_memes/roc_auc: 0.700974\n",
            "\u001b[32m2021-05-01T07:46:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6050/10000, train/hateful_memes/cross_entropy: 0.0112, train/hateful_memes/cross_entropy/avg: 0.2659, train/total_loss: 0.0112, train/total_loss/avg: 0.2659, max mem: 8122.0, experiment: run, epoch: 23, num_updates: 6050, iterations: 6050, max_updates: 10000, lr: 0., ups: 0.43, time: 01m 57s 254ms, time_since_start: 04h 15m 629ms, eta: 02h 39m 38s 026ms\n",
            "\u001b[32m2021-05-01T07:49:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6100/10000, train/hateful_memes/cross_entropy: 0.0076, train/hateful_memes/cross_entropy/avg: 0.2637, train/total_loss: 0.0076, train/total_loss/avg: 0.2637, max mem: 8122.0, experiment: run, epoch: 23, num_updates: 6100, iterations: 6100, max_updates: 10000, lr: 0., ups: 0.38, time: 02m 10s 401ms, time_since_start: 04h 17m 11s 031ms, eta: 02h 55m 17s 133ms\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-01T07:49:48 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-01T07:49:48 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2021-05-01T07:50:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6150/10000, train/hateful_memes/cross_entropy: 0.0101, train/hateful_memes/cross_entropy/avg: 0.2617, train/total_loss: 0.0101, train/total_loss/avg: 0.2617, max mem: 8122.0, experiment: run, epoch: 24, num_updates: 6150, iterations: 6150, max_updates: 10000, lr: 0., ups: 0.54, time: 01m 32s 882ms, time_since_start: 04h 18m 43s 913ms, eta: 02h 03m 15s 149ms\n",
            "\u001b[32m2021-05-01T07:52:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6200/10000, train/hateful_memes/cross_entropy: 0.0077, train/hateful_memes/cross_entropy/avg: 0.2596, train/total_loss: 0.0077, train/total_loss/avg: 0.2596, max mem: 8122.0, experiment: run, epoch: 24, num_updates: 6200, iterations: 6200, max_updates: 10000, lr: 0., ups: 0.52, time: 01m 37s 269ms, time_since_start: 04h 20m 21s 184ms, eta: 02h 07m 23s 866ms\n",
            "\u001b[32m2021-05-01T07:54:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6250/10000, train/hateful_memes/cross_entropy: 0.0077, train/hateful_memes/cross_entropy/avg: 0.2576, train/total_loss: 0.0077, train/total_loss/avg: 0.2576, max mem: 8122.0, experiment: run, epoch: 24, num_updates: 6250, iterations: 6250, max_updates: 10000, lr: 0., ups: 0.43, time: 01m 55s 990ms, time_since_start: 04h 22m 17s 174ms, eta: 02h 29m 55s 047ms\n",
            "\u001b[32m2021-05-01T07:56:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6300/10000, train/hateful_memes/cross_entropy: 0.0076, train/hateful_memes/cross_entropy/avg: 0.2556, train/total_loss: 0.0076, train/total_loss/avg: 0.2556, max mem: 8122.0, experiment: run, epoch: 24, num_updates: 6300, iterations: 6300, max_updates: 10000, lr: 0., ups: 0.37, time: 02m 16s 785ms, time_since_start: 04h 24m 33s 959ms, eta: 02h 54m 26s 268ms\n",
            "\u001b[32m2021-05-01T07:58:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6350/10000, train/hateful_memes/cross_entropy: 0.0077, train/hateful_memes/cross_entropy/avg: 0.2541, train/total_loss: 0.0077, train/total_loss/avg: 0.2541, max mem: 8122.0, experiment: run, epoch: 24, num_updates: 6350, iterations: 6350, max_updates: 10000, lr: 0., ups: 0.40, time: 02m 05s 318ms, time_since_start: 04h 26m 39s 277ms, eta: 02h 37m 39s 255ms\n",
            "\u001b[32m2021-05-01T08:00:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6400/10000, train/hateful_memes/cross_entropy: 0.0076, train/hateful_memes/cross_entropy/avg: 0.2521, train/total_loss: 0.0076, train/total_loss/avg: 0.2521, max mem: 8122.0, experiment: run, epoch: 25, num_updates: 6400, iterations: 6400, max_updates: 10000, lr: 0., ups: 0.45, time: 01m 51s 924ms, time_since_start: 04h 28m 31s 202ms, eta: 02h 18m 52s 537ms\n",
            "\u001b[32m2021-05-01T08:01:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6450/10000, train/hateful_memes/cross_entropy: 0.0071, train/hateful_memes/cross_entropy/avg: 0.2502, train/total_loss: 0.0071, train/total_loss/avg: 0.2502, max mem: 8122.0, experiment: run, epoch: 25, num_updates: 6450, iterations: 6450, max_updates: 10000, lr: 0., ups: 0.59, time: 01m 25s 834ms, time_since_start: 04h 29m 57s 036ms, eta: 01h 45m 01s 424ms\n",
            "\u001b[32m2021-05-01T08:03:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6500/10000, train/hateful_memes/cross_entropy: 0.0067, train/hateful_memes/cross_entropy/avg: 0.2483, train/total_loss: 0.0067, train/total_loss/avg: 0.2483, max mem: 8122.0, experiment: run, epoch: 25, num_updates: 6500, iterations: 6500, max_updates: 10000, lr: 0., ups: 0.44, time: 01m 54s 258ms, time_since_start: 04h 31m 51s 294ms, eta: 02h 17m 50s 009ms\n",
            "\u001b[32m2021-05-01T08:03:44 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2021-05-01T08:03:44 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[32m2021-05-01T08:04:43 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-05-01T08:04:48 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-05-01T08:05:02 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-05-01T08:05:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6500/10000, val/hateful_memes/cross_entropy: 1.8801, val/total_loss: 1.8801, val/hateful_memes/accuracy: 0.6704, val/hateful_memes/binary_f1: 0.4540, val/hateful_memes/roc_auc: 0.6787, num_updates: 6500, epoch: 25, iterations: 6500, max_updates: 10000, val_time: 01m 17s 903ms, best_update: 5000, best_iteration: 5000, best_val/hateful_memes/roc_auc: 0.700974\n",
            "\u001b[32m2021-05-01T08:06:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6550/10000, train/hateful_memes/cross_entropy: 0.0067, train/hateful_memes/cross_entropy/avg: 0.2472, train/total_loss: 0.0067, train/total_loss/avg: 0.2472, max mem: 8122.0, experiment: run, epoch: 25, num_updates: 6550, iterations: 6550, max_updates: 10000, lr: 0., ups: 0.44, time: 01m 54s 955ms, time_since_start: 04h 35m 04s 155ms, eta: 02h 16m 41s 617ms\n",
            "\u001b[32m2021-05-01T08:09:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6600/10000, train/hateful_memes/cross_entropy: 0.0067, train/hateful_memes/cross_entropy/avg: 0.2455, train/total_loss: 0.0067, train/total_loss/avg: 0.2455, max mem: 8122.0, experiment: run, epoch: 25, num_updates: 6600, iterations: 6600, max_updates: 10000, lr: 0., ups: 0.38, time: 02m 12s 206ms, time_since_start: 04h 37m 16s 361ms, eta: 02h 34m 55s 710ms\n",
            "\u001b[32m2021-05-01T08:11:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6650/10000, train/hateful_memes/cross_entropy: 0.0067, train/hateful_memes/cross_entropy/avg: 0.2443, train/total_loss: 0.0067, train/total_loss/avg: 0.2443, max mem: 8122.0, experiment: run, epoch: 25, num_updates: 6650, iterations: 6650, max_updates: 10000, lr: 0., ups: 0.39, time: 02m 09s 519ms, time_since_start: 04h 39m 25s 881ms, eta: 02h 29m 32s 864ms\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-01T08:11:19 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-01T08:11:19 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2021-05-01T08:12:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6700/10000, train/hateful_memes/cross_entropy: 0.0067, train/hateful_memes/cross_entropy/avg: 0.2425, train/total_loss: 0.0067, train/total_loss/avg: 0.2425, max mem: 8122.0, experiment: run, epoch: 26, num_updates: 6700, iterations: 6700, max_updates: 10000, lr: 0., ups: 0.60, time: 01m 24s 959ms, time_since_start: 04h 40m 50s 840ms, eta: 01h 36m 37s 981ms\n",
            "\u001b[32m2021-05-01T08:14:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6750/10000, train/hateful_memes/cross_entropy: 0.0071, train/hateful_memes/cross_entropy/avg: 0.2416, train/total_loss: 0.0071, train/total_loss/avg: 0.2416, max mem: 8122.0, experiment: run, epoch: 26, num_updates: 6750, iterations: 6750, max_updates: 10000, lr: 0., ups: 0.47, time: 01m 47s 101ms, time_since_start: 04h 42m 37s 942ms, eta: 01h 59m 58s 295ms\n",
            "\u001b[32m2021-05-01T08:16:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6800/10000, train/hateful_memes/cross_entropy: 0.0071, train/hateful_memes/cross_entropy/avg: 0.2398, train/total_loss: 0.0071, train/total_loss/avg: 0.2398, max mem: 8122.0, experiment: run, epoch: 26, num_updates: 6800, iterations: 6800, max_updates: 10000, lr: 0., ups: 0.39, time: 02m 07s 587ms, time_since_start: 04h 44m 45s 530ms, eta: 02h 20m 43s 258ms\n",
            "\u001b[32m2021-05-01T08:18:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6850/10000, train/hateful_memes/cross_entropy: 0.0063, train/hateful_memes/cross_entropy/avg: 0.2381, train/total_loss: 0.0063, train/total_loss/avg: 0.2381, max mem: 8122.0, experiment: run, epoch: 26, num_updates: 6850, iterations: 6850, max_updates: 10000, lr: 0., ups: 0.39, time: 02m 09s 404ms, time_since_start: 04h 46m 54s 934ms, eta: 02h 20m 29s 653ms\n",
            "\u001b[32m2021-05-01T08:21:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6900/10000, train/hateful_memes/cross_entropy: 0.0063, train/hateful_memes/cross_entropy/avg: 0.2364, train/total_loss: 0.0063, train/total_loss/avg: 0.2364, max mem: 8122.0, experiment: run, epoch: 26, num_updates: 6900, iterations: 6900, max_updates: 10000, lr: 0., ups: 0.37, time: 02m 14s 407ms, time_since_start: 04h 49m 09s 342ms, eta: 02h 23m 36s 621ms\n",
            "\u001b[32m2021-05-01T08:22:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6950/10000, train/hateful_memes/cross_entropy: 0.0063, train/hateful_memes/cross_entropy/avg: 0.2348, train/total_loss: 0.0063, train/total_loss/avg: 0.2348, max mem: 8122.0, experiment: run, epoch: 27, num_updates: 6950, iterations: 6950, max_updates: 10000, lr: 0., ups: 0.58, time: 01m 26s 458ms, time_since_start: 04h 50m 35s 800ms, eta: 01h 30m 53s 263ms\n",
            "\u001b[32m2021-05-01T08:23:59 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[32m2021-05-01T08:23:59 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-05-01T08:24:07 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-05-01T08:24:20 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-05-01T08:24:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7000/10000, train/hateful_memes/cross_entropy: 0.0063, train/hateful_memes/cross_entropy/avg: 0.2334, train/total_loss: 0.0063, train/total_loss/avg: 0.2334, max mem: 8122.0, experiment: run, epoch: 27, num_updates: 7000, iterations: 7000, max_updates: 10000, lr: 0., ups: 0.45, time: 01m 52s 251ms, time_since_start: 04h 52m 28s 052ms, eta: 01h 56m 04s 078ms\n",
            "\u001b[32m2021-05-01T08:24:20 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2021-05-01T08:24:20 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-01T08:24:20 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-01T08:24:20 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2021-05-01T08:25:14 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-05-01T08:25:20 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-05-01T08:25:33 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-05-01T08:25:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7000/10000, val/hateful_memes/cross_entropy: 1.8991, val/total_loss: 1.8991, val/hateful_memes/accuracy: 0.6796, val/hateful_memes/binary_f1: 0.4742, val/hateful_memes/roc_auc: 0.6784, num_updates: 7000, epoch: 27, iterations: 7000, max_updates: 10000, val_time: 01m 12s 400ms, best_update: 5000, best_iteration: 5000, best_val/hateful_memes/roc_auc: 0.700974\n",
            "\u001b[32m2021-05-01T08:27:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7050/10000, train/hateful_memes/cross_entropy: 0.0053, train/hateful_memes/cross_entropy/avg: 0.2317, train/total_loss: 0.0053, train/total_loss/avg: 0.2317, max mem: 8122.0, experiment: run, epoch: 27, num_updates: 7050, iterations: 7050, max_updates: 10000, lr: 0., ups: 0.42, time: 01m 59s 159ms, time_since_start: 04h 55m 39s 614ms, eta: 02h 01m 09s 422ms\n",
            "\u001b[32m2021-05-01T08:29:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7100/10000, train/hateful_memes/cross_entropy: 0.0050, train/hateful_memes/cross_entropy/avg: 0.2301, train/total_loss: 0.0050, train/total_loss/avg: 0.2301, max mem: 8122.0, experiment: run, epoch: 27, num_updates: 7100, iterations: 7100, max_updates: 10000, lr: 0., ups: 0.39, time: 02m 09s 611ms, time_since_start: 04h 57m 49s 225ms, eta: 02h 09m 33s 038ms\n",
            "\u001b[32m2021-05-01T08:31:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7150/10000, train/hateful_memes/cross_entropy: 0.0043, train/hateful_memes/cross_entropy/avg: 0.2285, train/total_loss: 0.0043, train/total_loss/avg: 0.2285, max mem: 8122.0, experiment: run, epoch: 27, num_updates: 7150, iterations: 7150, max_updates: 10000, lr: 0., ups: 0.38, time: 02m 12s 837ms, time_since_start: 05h 02s 062ms, eta: 02h 10m 29s 166ms\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-01T08:33:14 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-01T08:33:14 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2021-05-01T08:33:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7200/10000, train/hateful_memes/cross_entropy: 0.0043, train/hateful_memes/cross_entropy/avg: 0.2270, train/total_loss: 0.0043, train/total_loss/avg: 0.2270, max mem: 8122.0, experiment: run, epoch: 28, num_updates: 7200, iterations: 7200, max_updates: 10000, lr: 0., ups: 0.46, time: 01m 49s 759ms, time_since_start: 05h 01m 51s 822ms, eta: 01h 45m 55s 529ms\n",
            "\u001b[32m2021-05-01T08:35:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7250/10000, train/hateful_memes/cross_entropy: 0.0043, train/hateful_memes/cross_entropy/avg: 0.2262, train/total_loss: 0.0043, train/total_loss/avg: 0.2262, max mem: 8122.0, experiment: run, epoch: 28, num_updates: 7250, iterations: 7250, max_updates: 10000, lr: 0., ups: 0.57, time: 01m 27s 014ms, time_since_start: 05h 03m 18s 836ms, eta: 01h 22m 28s 493ms\n",
            "\u001b[32m2021-05-01T08:37:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7300/10000, train/hateful_memes/cross_entropy: 0.0050, train/hateful_memes/cross_entropy/avg: 0.2253, train/total_loss: 0.0050, train/total_loss/avg: 0.2253, max mem: 8122.0, experiment: run, epoch: 28, num_updates: 7300, iterations: 7300, max_updates: 10000, lr: 0., ups: 0.43, time: 01m 57s 352ms, time_since_start: 05h 05m 16s 188ms, eta: 01h 49m 12s 471ms\n",
            "\u001b[32m2021-05-01T08:39:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7350/10000, train/hateful_memes/cross_entropy: 0.0043, train/hateful_memes/cross_entropy/avg: 0.2238, train/total_loss: 0.0043, train/total_loss/avg: 0.2238, max mem: 8122.0, experiment: run, epoch: 28, num_updates: 7350, iterations: 7350, max_updates: 10000, lr: 0., ups: 0.40, time: 02m 06s 711ms, time_since_start: 05h 07m 22s 900ms, eta: 01h 55m 44s 055ms\n",
            "\u001b[32m2021-05-01T08:41:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7400/10000, train/hateful_memes/cross_entropy: 0.0050, train/hateful_memes/cross_entropy/avg: 0.2224, train/total_loss: 0.0050, train/total_loss/avg: 0.2224, max mem: 8122.0, experiment: run, epoch: 28, num_updates: 7400, iterations: 7400, max_updates: 10000, lr: 0., ups: 0.38, time: 02m 12s 924ms, time_since_start: 05h 09m 35s 825ms, eta: 01h 59m 07s 079ms\n",
            "\u001b[32m2021-05-01T08:43:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7450/10000, train/hateful_memes/cross_entropy: 0.0053, train/hateful_memes/cross_entropy/avg: 0.2224, train/total_loss: 0.0053, train/total_loss/avg: 0.2224, max mem: 8122.0, experiment: run, epoch: 29, num_updates: 7450, iterations: 7450, max_updates: 10000, lr: 0., ups: 0.40, time: 02m 05s 111ms, time_since_start: 05h 11m 40s 937ms, eta: 01h 49m 57s 656ms\n",
            "\u001b[32m2021-05-01T08:44:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7500/10000, train/hateful_memes/cross_entropy: 0.0053, train/hateful_memes/cross_entropy/avg: 0.2209, train/total_loss: 0.0053, train/total_loss/avg: 0.2209, max mem: 8122.0, experiment: run, epoch: 29, num_updates: 7500, iterations: 7500, max_updates: 10000, lr: 0., ups: 0.68, time: 01m 14s 181ms, time_since_start: 05h 12m 55s 118ms, eta: 01h 03m 55s 194ms\n",
            "\u001b[32m2021-05-01T08:44:47 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2021-05-01T08:44:47 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[32m2021-05-01T08:45:43 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-05-01T08:45:48 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-05-01T08:46:01 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-05-01T08:46:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7500/10000, val/hateful_memes/cross_entropy: 1.9752, val/total_loss: 1.9752, val/hateful_memes/accuracy: 0.6889, val/hateful_memes/binary_f1: 0.4581, val/hateful_memes/roc_auc: 0.6747, num_updates: 7500, epoch: 29, iterations: 7500, max_updates: 10000, val_time: 01m 13s 512ms, best_update: 5000, best_iteration: 5000, best_val/hateful_memes/roc_auc: 0.700974\n",
            "\u001b[32m2021-05-01T08:47:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7550/10000, train/hateful_memes/cross_entropy: 0.0050, train/hateful_memes/cross_entropy/avg: 0.2195, train/total_loss: 0.0050, train/total_loss/avg: 0.2195, max mem: 8122.0, experiment: run, epoch: 29, num_updates: 7550, iterations: 7550, max_updates: 10000, lr: 0., ups: 0.43, time: 01m 56s 144ms, time_since_start: 05h 16m 04s 808ms, eta: 01h 38m 04s 569ms\n",
            "\u001b[32m2021-05-01T08:50:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7600/10000, train/hateful_memes/cross_entropy: 0.0043, train/hateful_memes/cross_entropy/avg: 0.2180, train/total_loss: 0.0043, train/total_loss/avg: 0.2180, max mem: 8122.0, experiment: run, epoch: 29, num_updates: 7600, iterations: 7600, max_updates: 10000, lr: 0., ups: 0.41, time: 02m 03s 487ms, time_since_start: 05h 18m 08s 296ms, eta: 01h 42m 08s 946ms\n",
            "\u001b[32m2021-05-01T08:52:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7650/10000, train/hateful_memes/cross_entropy: 0.0043, train/hateful_memes/cross_entropy/avg: 0.2166, train/total_loss: 0.0043, train/total_loss/avg: 0.2166, max mem: 8122.0, experiment: run, epoch: 29, num_updates: 7650, iterations: 7650, max_updates: 10000, lr: 0., ups: 0.38, time: 02m 11s 503ms, time_since_start: 05h 20m 19s 799ms, eta: 01h 46m 30s 787ms\n",
            "\u001b[32m2021-05-01T08:54:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7700/10000, train/hateful_memes/cross_entropy: 0.0043, train/hateful_memes/cross_entropy/avg: 0.2152, train/total_loss: 0.0043, train/total_loss/avg: 0.2152, max mem: 8122.0, experiment: run, epoch: 29, num_updates: 7700, iterations: 7700, max_updates: 10000, lr: 0., ups: 0.39, time: 02m 09s 409ms, time_since_start: 05h 22m 29s 208ms, eta: 01h 42m 35s 219ms\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-01T08:54:54 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-01T08:54:54 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2021-05-01T08:55:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7750/10000, train/hateful_memes/cross_entropy: 0.0040, train/hateful_memes/cross_entropy/avg: 0.2139, train/total_loss: 0.0040, train/total_loss/avg: 0.2139, max mem: 8122.0, experiment: run, epoch: 30, num_updates: 7750, iterations: 7750, max_updates: 10000, lr: 0., ups: 0.54, time: 01m 33s 252ms, time_since_start: 05h 24m 02s 461ms, eta: 01h 12m 19s 031ms\n",
            "\u001b[32m2021-05-01T08:57:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7800/10000, train/hateful_memes/cross_entropy: 0.0040, train/hateful_memes/cross_entropy/avg: 0.2125, train/total_loss: 0.0040, train/total_loss/avg: 0.2125, max mem: 8122.0, experiment: run, epoch: 30, num_updates: 7800, iterations: 7800, max_updates: 10000, lr: 0., ups: 0.51, time: 01m 38s 644ms, time_since_start: 05h 25m 41s 105ms, eta: 01h 14m 47s 926ms\n",
            "\u001b[32m2021-05-01T08:59:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7850/10000, train/hateful_memes/cross_entropy: 0.0028, train/hateful_memes/cross_entropy/avg: 0.2112, train/total_loss: 0.0028, train/total_loss/avg: 0.2112, max mem: 8122.0, experiment: run, epoch: 30, num_updates: 7850, iterations: 7850, max_updates: 10000, lr: 0., ups: 0.42, time: 01m 59s 683ms, time_since_start: 05h 27m 40s 788ms, eta: 01h 28m 41s 349ms\n",
            "\u001b[32m2021-05-01T09:01:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7900/10000, train/hateful_memes/cross_entropy: 0.0028, train/hateful_memes/cross_entropy/avg: 0.2099, train/total_loss: 0.0028, train/total_loss/avg: 0.2099, max mem: 8122.0, experiment: run, epoch: 30, num_updates: 7900, iterations: 7900, max_updates: 10000, lr: 0., ups: 0.37, time: 02m 16s 424ms, time_since_start: 05h 29m 57s 212ms, eta: 01h 38m 44s 622ms\n",
            "\u001b[32m2021-05-01T09:03:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7950/10000, train/hateful_memes/cross_entropy: 0.0028, train/hateful_memes/cross_entropy/avg: 0.2087, train/total_loss: 0.0028, train/total_loss/avg: 0.2087, max mem: 8122.0, experiment: run, epoch: 30, num_updates: 7950, iterations: 7950, max_updates: 10000, lr: 0., ups: 0.39, time: 02m 07s 179ms, time_since_start: 05h 32m 04s 392ms, eta: 01h 29m 51s 638ms\n",
            "\u001b[32m2021-05-01T09:05:41 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[32m2021-05-01T09:05:41 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-05-01T09:05:50 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-05-01T09:06:04 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-05-01T09:06:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8000/10000, train/hateful_memes/cross_entropy: 0.0027, train/hateful_memes/cross_entropy/avg: 0.2074, train/total_loss: 0.0027, train/total_loss/avg: 0.2074, max mem: 8122.0, experiment: run, epoch: 31, num_updates: 8000, iterations: 8000, max_updates: 10000, lr: 0., ups: 0.40, time: 02m 06s 847ms, time_since_start: 05h 34m 11s 239ms, eta: 01h 27m 26s 422ms\n",
            "\u001b[32m2021-05-01T09:06:04 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2021-05-01T09:06:04 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-01T09:06:04 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-01T09:06:04 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2021-05-01T09:06:53 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-05-01T09:06:59 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-05-01T09:07:14 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-05-01T09:07:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8000/10000, val/hateful_memes/cross_entropy: 2.0450, val/total_loss: 2.0450, val/hateful_memes/accuracy: 0.6759, val/hateful_memes/binary_f1: 0.4514, val/hateful_memes/roc_auc: 0.6719, num_updates: 8000, epoch: 31, iterations: 8000, max_updates: 10000, val_time: 01m 10s 020ms, best_update: 5000, best_iteration: 5000, best_val/hateful_memes/roc_auc: 0.700974\n",
            "\u001b[32m2021-05-01T09:09:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8050/10000, train/hateful_memes/cross_entropy: 0.0026, train/hateful_memes/cross_entropy/avg: 0.2061, train/total_loss: 0.0026, train/total_loss/avg: 0.2061, max mem: 8122.0, experiment: run, epoch: 31, num_updates: 8050, iterations: 8050, max_updates: 10000, lr: 0., ups: 0.45, time: 01m 52s 123ms, time_since_start: 05h 37m 13s 386ms, eta: 01h 15m 21s 498ms\n",
            "\u001b[32m2021-05-01T09:11:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8100/10000, train/hateful_memes/cross_entropy: 0.0028, train/hateful_memes/cross_entropy/avg: 0.2048, train/total_loss: 0.0028, train/total_loss/avg: 0.2048, max mem: 8122.0, experiment: run, epoch: 31, num_updates: 8100, iterations: 8100, max_updates: 10000, lr: 0., ups: 0.41, time: 02m 02s 941ms, time_since_start: 05h 39m 16s 327ms, eta: 01h 20m 30s 601ms\n",
            "\u001b[32m2021-05-01T09:13:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8150/10000, train/hateful_memes/cross_entropy: 0.0037, train/hateful_memes/cross_entropy/avg: 0.2036, train/total_loss: 0.0037, train/total_loss/avg: 0.2036, max mem: 8122.0, experiment: run, epoch: 31, num_updates: 8150, iterations: 8150, max_updates: 10000, lr: 0., ups: 0.39, time: 02m 08s 953ms, time_since_start: 05h 41m 25s 281ms, eta: 01h 22m 13s 521ms\n",
            "\u001b[32m2021-05-01T09:15:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8200/10000, train/hateful_memes/cross_entropy: 0.0037, train/hateful_memes/cross_entropy/avg: 0.2024, train/total_loss: 0.0037, train/total_loss/avg: 0.2024, max mem: 8122.0, experiment: run, epoch: 31, num_updates: 8200, iterations: 8200, max_updates: 10000, lr: 0., ups: 0.41, time: 02m 03s 925ms, time_since_start: 05h 43m 29s 207ms, eta: 01h 16m 53s 007ms\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-01T09:17:13 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-01T09:17:13 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2021-05-01T09:17:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8250/10000, train/hateful_memes/cross_entropy: 0.0028, train/hateful_memes/cross_entropy/avg: 0.2012, train/total_loss: 0.0028, train/total_loss/avg: 0.2012, max mem: 8122.0, experiment: run, epoch: 32, num_updates: 8250, iterations: 8250, max_updates: 10000, lr: 0., ups: 0.42, time: 01m 59s 440ms, time_since_start: 05h 45m 28s 647ms, eta: 01h 12m 02s 539ms\n",
            "\u001b[32m2021-05-01T09:18:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8300/10000, train/hateful_memes/cross_entropy: 0.0028, train/hateful_memes/cross_entropy/avg: 0.2000, train/total_loss: 0.0028, train/total_loss/avg: 0.2000, max mem: 8122.0, experiment: run, epoch: 32, num_updates: 8300, iterations: 8300, max_updates: 10000, lr: 0., ups: 0.67, time: 01m 15s 766ms, time_since_start: 05h 46m 44s 414ms, eta: 44m 23s 656ms\n",
            "\u001b[32m2021-05-01T09:20:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8350/10000, train/hateful_memes/cross_entropy: 0.0030, train/hateful_memes/cross_entropy/avg: 0.1988, train/total_loss: 0.0030, train/total_loss/avg: 0.1988, max mem: 8122.0, experiment: run, epoch: 32, num_updates: 8350, iterations: 8350, max_updates: 10000, lr: 0., ups: 0.51, time: 01m 38s 198ms, time_since_start: 05h 48m 22s 613ms, eta: 55m 50s 744ms\n",
            "\u001b[32m2021-05-01T09:22:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8400/10000, train/hateful_memes/cross_entropy: 0.0028, train/hateful_memes/cross_entropy/avg: 0.1977, train/total_loss: 0.0028, train/total_loss/avg: 0.1977, max mem: 8122.0, experiment: run, epoch: 32, num_updates: 8400, iterations: 8400, max_updates: 10000, lr: 0., ups: 0.38, time: 02m 10s 410ms, time_since_start: 05h 50m 33s 023ms, eta: 01h 11m 55s 015ms\n",
            "\u001b[32m2021-05-01T09:24:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8450/10000, train/hateful_memes/cross_entropy: 0.0028, train/hateful_memes/cross_entropy/avg: 0.1965, train/total_loss: 0.0028, train/total_loss/avg: 0.1965, max mem: 8122.0, experiment: run, epoch: 32, num_updates: 8450, iterations: 8450, max_updates: 10000, lr: 0., ups: 0.39, time: 02m 08s 717ms, time_since_start: 05h 52m 41s 740ms, eta: 01h 08m 45s 899ms\n",
            "\u001b[32m2021-05-01T09:26:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8500/10000, train/hateful_memes/cross_entropy: 0.0030, train/hateful_memes/cross_entropy/avg: 0.1954, train/total_loss: 0.0030, train/total_loss/avg: 0.1954, max mem: 8122.0, experiment: run, epoch: 32, num_updates: 8500, iterations: 8500, max_updates: 10000, lr: 0., ups: 0.37, time: 02m 16s 871ms, time_since_start: 05h 54m 58s 612ms, eta: 01h 10m 45s 755ms\n",
            "\u001b[32m2021-05-01T09:26:51 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2021-05-01T09:26:51 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[32m2021-05-01T09:27:47 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-05-01T09:27:53 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-05-01T09:28:05 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-05-01T09:28:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8500/10000, val/hateful_memes/cross_entropy: 2.0812, val/total_loss: 2.0812, val/hateful_memes/accuracy: 0.6889, val/hateful_memes/binary_f1: 0.4581, val/hateful_memes/roc_auc: 0.6703, num_updates: 8500, epoch: 32, iterations: 8500, max_updates: 10000, val_time: 01m 14s 277ms, best_update: 5000, best_iteration: 5000, best_val/hateful_memes/roc_auc: 0.700974\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-01T09:28:18 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-01T09:28:18 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2021-05-01T09:29:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8550/10000, train/hateful_memes/cross_entropy: 0.0037, train/hateful_memes/cross_entropy/avg: 0.1943, train/total_loss: 0.0037, train/total_loss/avg: 0.1943, max mem: 8122.0, experiment: run, epoch: 33, num_updates: 8550, iterations: 8550, max_updates: 10000, lr: 0., ups: 0.49, time: 01m 42s 593ms, time_since_start: 05h 57m 55s 510ms, eta: 51m 16s 373ms\n",
            "\u001b[32m2021-05-01T09:31:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8600/10000, train/hateful_memes/cross_entropy: 0.0037, train/hateful_memes/cross_entropy/avg: 0.1931, train/total_loss: 0.0037, train/total_loss/avg: 0.1931, max mem: 8122.0, experiment: run, epoch: 33, num_updates: 8600, iterations: 8600, max_updates: 10000, lr: 0., ups: 0.43, time: 01m 56s 788ms, time_since_start: 05h 59m 52s 298ms, eta: 56m 21s 261ms\n",
            "\u001b[32m2021-05-01T09:33:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8650/10000, train/hateful_memes/cross_entropy: 0.0038, train/hateful_memes/cross_entropy/avg: 0.1921, train/total_loss: 0.0038, train/total_loss/avg: 0.1921, max mem: 8122.0, experiment: run, epoch: 33, num_updates: 8650, iterations: 8650, max_updates: 10000, lr: 0., ups: 0.39, time: 02m 07s 812ms, time_since_start: 06h 02m 111ms, eta: 59m 28s 259ms\n",
            "\u001b[32m2021-05-01T09:35:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8700/10000, train/hateful_memes/cross_entropy: 0.0038, train/hateful_memes/cross_entropy/avg: 0.1911, train/total_loss: 0.0038, train/total_loss/avg: 0.1911, max mem: 8122.0, experiment: run, epoch: 33, num_updates: 8700, iterations: 8700, max_updates: 10000, lr: 0., ups: 0.41, time: 02m 03s 821ms, time_since_start: 06h 04m 03s 932ms, eta: 55m 28s 815ms\n",
            "\u001b[32m2021-05-01T09:38:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8750/10000, train/hateful_memes/cross_entropy: 0.0037, train/hateful_memes/cross_entropy/avg: 0.1900, train/total_loss: 0.0037, train/total_loss/avg: 0.1900, max mem: 8122.0, experiment: run, epoch: 33, num_updates: 8750, iterations: 8750, max_updates: 10000, lr: 0., ups: 0.38, time: 02m 13s 710ms, time_since_start: 06h 06m 17s 643ms, eta: 57m 36s 423ms\n",
            "\u001b[32m2021-05-01T09:39:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8800/10000, train/hateful_memes/cross_entropy: 0.0037, train/hateful_memes/cross_entropy/avg: 0.1889, train/total_loss: 0.0037, train/total_loss/avg: 0.1889, max mem: 8122.0, experiment: run, epoch: 34, num_updates: 8800, iterations: 8800, max_updates: 10000, lr: 0., ups: 0.51, time: 01m 38s 142ms, time_since_start: 06h 07m 55s 786ms, eta: 40m 35s 517ms\n",
            "\u001b[32m2021-05-01T09:41:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8850/10000, train/hateful_memes/cross_entropy: 0.0037, train/hateful_memes/cross_entropy/avg: 0.1879, train/total_loss: 0.0037, train/total_loss/avg: 0.1879, max mem: 8122.0, experiment: run, epoch: 34, num_updates: 8850, iterations: 8850, max_updates: 10000, lr: 0., ups: 0.57, time: 01m 28s 674ms, time_since_start: 06h 09m 24s 460ms, eta: 35m 08s 853ms\n",
            "\u001b[32m2021-05-01T09:43:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8900/10000, train/hateful_memes/cross_entropy: 0.0037, train/hateful_memes/cross_entropy/avg: 0.1868, train/total_loss: 0.0037, train/total_loss/avg: 0.1868, max mem: 8122.0, experiment: run, epoch: 34, num_updates: 8900, iterations: 8900, max_updates: 10000, lr: 0., ups: 0.41, time: 02m 01s 798ms, time_since_start: 06h 11m 26s 259ms, eta: 46m 10s 678ms\n",
            "\u001b[32m2021-05-01T09:45:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8950/10000, train/hateful_memes/cross_entropy: 0.0030, train/hateful_memes/cross_entropy/avg: 0.1858, train/total_loss: 0.0030, train/total_loss/avg: 0.1858, max mem: 8122.0, experiment: run, epoch: 34, num_updates: 8950, iterations: 8950, max_updates: 10000, lr: 0., ups: 0.39, time: 02m 07s 192ms, time_since_start: 06h 13m 33s 452ms, eta: 46m 01s 862ms\n",
            "\u001b[32m2021-05-01T09:47:41 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[32m2021-05-01T09:47:41 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-05-01T09:47:50 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-05-01T09:48:03 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-05-01T09:48:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9000/10000, train/hateful_memes/cross_entropy: 0.0030, train/hateful_memes/cross_entropy/avg: 0.1848, train/total_loss: 0.0030, train/total_loss/avg: 0.1848, max mem: 8122.0, experiment: run, epoch: 34, num_updates: 9000, iterations: 9000, max_updates: 10000, lr: 0., ups: 0.32, time: 02m 37s 323ms, time_since_start: 06h 16m 10s 775ms, eta: 54m 13s 445ms\n",
            "\u001b[32m2021-05-01T09:48:03 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2021-05-01T09:48:03 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-01T09:48:03 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-01T09:48:03 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2021-05-01T09:48:51 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-05-01T09:48:56 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-05-01T09:49:09 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-05-01T09:49:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9000/10000, val/hateful_memes/cross_entropy: 2.1209, val/total_loss: 2.1209, val/hateful_memes/accuracy: 0.6833, val/hateful_memes/binary_f1: 0.4466, val/hateful_memes/roc_auc: 0.6698, num_updates: 9000, epoch: 34, iterations: 9000, max_updates: 10000, val_time: 01m 07s 313ms, best_update: 5000, best_iteration: 5000, best_val/hateful_memes/roc_auc: 0.700974\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-01T09:50:48 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-01T09:50:48 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2021-05-01T09:51:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9050/10000, train/hateful_memes/cross_entropy: 0.0030, train/hateful_memes/cross_entropy/avg: 0.1838, train/total_loss: 0.0030, train/total_loss/avg: 0.1838, max mem: 8122.0, experiment: run, epoch: 35, num_updates: 9050, iterations: 9050, max_updates: 10000, lr: 0., ups: 0.43, time: 01m 57s 003ms, time_since_start: 06h 19m 15s 097ms, eta: 38m 18s 641ms\n",
            "\u001b[32m2021-05-01T09:52:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9100/10000, train/hateful_memes/cross_entropy: 0.0030, train/hateful_memes/cross_entropy/avg: 0.1828, train/total_loss: 0.0030, train/total_loss/avg: 0.1828, max mem: 8122.0, experiment: run, epoch: 35, num_updates: 9100, iterations: 9100, max_updates: 10000, lr: 0., ups: 0.48, time: 01m 44s 611ms, time_since_start: 06h 20m 59s 709ms, eta: 32m 27s 038ms\n",
            "\u001b[32m2021-05-01T09:54:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9150/10000, train/hateful_memes/cross_entropy: 0.0024, train/hateful_memes/cross_entropy/avg: 0.1818, train/total_loss: 0.0024, train/total_loss/avg: 0.1818, max mem: 8122.0, experiment: run, epoch: 35, num_updates: 9150, iterations: 9150, max_updates: 10000, lr: 0., ups: 0.41, time: 02m 02s 349ms, time_since_start: 06h 23m 02s 059ms, eta: 35m 50s 664ms\n",
            "\u001b[32m2021-05-01T09:56:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9200/10000, train/hateful_memes/cross_entropy: 0.0020, train/hateful_memes/cross_entropy/avg: 0.1808, train/total_loss: 0.0020, train/total_loss/avg: 0.1808, max mem: 8122.0, experiment: run, epoch: 35, num_updates: 9200, iterations: 9200, max_updates: 10000, lr: 0., ups: 0.41, time: 02m 03s 680ms, time_since_start: 06h 25m 05s 740ms, eta: 34m 06s 169ms\n",
            "\u001b[32m2021-05-01T09:59:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9250/10000, train/hateful_memes/cross_entropy: 0.0020, train/hateful_memes/cross_entropy/avg: 0.1800, train/total_loss: 0.0020, train/total_loss/avg: 0.1800, max mem: 8122.0, experiment: run, epoch: 35, num_updates: 9250, iterations: 9250, max_updates: 10000, lr: 0., ups: 0.38, time: 02m 11s 926ms, time_since_start: 06h 27m 17s 667ms, eta: 34m 06s 186ms\n",
            "\u001b[32m2021-05-01T10:01:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9300/10000, train/hateful_memes/cross_entropy: 0.0020, train/hateful_memes/cross_entropy/avg: 0.1795, train/total_loss: 0.0020, train/total_loss/avg: 0.1795, max mem: 8122.0, experiment: run, epoch: 35, num_updates: 9300, iterations: 9300, max_updates: 10000, lr: 0., ups: 0.39, time: 02m 07s 082ms, time_since_start: 06h 29m 24s 749ms, eta: 30m 39s 649ms\n",
            "\u001b[32m2021-05-01T10:02:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9350/10000, train/hateful_memes/cross_entropy: 0.0019, train/hateful_memes/cross_entropy/avg: 0.1786, train/total_loss: 0.0019, train/total_loss/avg: 0.1786, max mem: 8122.0, experiment: run, epoch: 36, num_updates: 9350, iterations: 9350, max_updates: 10000, lr: 0., ups: 0.57, time: 01m 27s 916ms, time_since_start: 06h 30m 52s 666ms, eta: 19m 41s 771ms\n",
            "\u001b[32m2021-05-01T10:04:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9400/10000, train/hateful_memes/cross_entropy: 0.0020, train/hateful_memes/cross_entropy/avg: 0.1777, train/total_loss: 0.0020, train/total_loss/avg: 0.1777, max mem: 8122.0, experiment: run, epoch: 36, num_updates: 9400, iterations: 9400, max_updates: 10000, lr: 0., ups: 0.51, time: 01m 39s 544ms, time_since_start: 06h 32m 32s 210ms, eta: 20m 35s 151ms\n",
            "\u001b[32m2021-05-01T10:06:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9450/10000, train/hateful_memes/cross_entropy: 0.0019, train/hateful_memes/cross_entropy/avg: 0.1768, train/total_loss: 0.0019, train/total_loss/avg: 0.1768, max mem: 8122.0, experiment: run, epoch: 36, num_updates: 9450, iterations: 9450, max_updates: 10000, lr: 0., ups: 0.39, time: 02m 09s 059ms, time_since_start: 06h 34m 41s 270ms, eta: 24m 27s 919ms\n",
            "\u001b[32m2021-05-01T10:08:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9500/10000, train/hateful_memes/cross_entropy: 0.0019, train/hateful_memes/cross_entropy/avg: 0.1762, train/total_loss: 0.0019, train/total_loss/avg: 0.1762, max mem: 8122.0, experiment: run, epoch: 36, num_updates: 9500, iterations: 9500, max_updates: 10000, lr: 0., ups: 0.39, time: 02m 08s 902ms, time_since_start: 06h 36m 50s 172ms, eta: 22m 12s 847ms\n",
            "\u001b[32m2021-05-01T10:08:43 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2021-05-01T10:08:43 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[32m2021-05-01T10:09:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9500/10000, val/hateful_memes/cross_entropy: 2.1002, val/total_loss: 2.1002, val/hateful_memes/accuracy: 0.6907, val/hateful_memes/binary_f1: 0.4595, val/hateful_memes/roc_auc: 0.6736, num_updates: 9500, epoch: 36, iterations: 9500, max_updates: 10000, val_time: 01m 01s 917ms, best_update: 5000, best_iteration: 5000, best_val/hateful_memes/roc_auc: 0.700974\n",
            "\u001b[32m2021-05-01T10:11:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9550/10000, train/hateful_memes/cross_entropy: 0.0019, train/hateful_memes/cross_entropy/avg: 0.1753, train/total_loss: 0.0019, train/total_loss/avg: 0.1753, max mem: 8122.0, experiment: run, epoch: 36, num_updates: 9550, iterations: 9550, max_updates: 10000, lr: 0., ups: 0.43, time: 01m 57s 422ms, time_since_start: 06h 39m 49s 513ms, eta: 18m 12s 736ms\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-01T10:12:45 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-01T10:12:45 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2021-05-01T10:13:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9600/10000, train/hateful_memes/cross_entropy: 0.0019, train/hateful_memes/cross_entropy/avg: 0.1746, train/total_loss: 0.0019, train/total_loss/avg: 0.1746, max mem: 8122.0, experiment: run, epoch: 37, num_updates: 9600, iterations: 9600, max_updates: 10000, lr: 0., ups: 0.48, time: 01m 44s 995ms, time_since_start: 06h 41m 34s 509ms, eta: 14m 28s 521ms\n",
            "\u001b[32m2021-05-01T10:14:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9650/10000, train/hateful_memes/cross_entropy: 0.0019, train/hateful_memes/cross_entropy/avg: 0.1738, train/total_loss: 0.0019, train/total_loss/avg: 0.1738, max mem: 8122.0, experiment: run, epoch: 37, num_updates: 9650, iterations: 9650, max_updates: 10000, lr: 0., ups: 0.56, time: 01m 30s 946ms, time_since_start: 06h 43m 05s 456ms, eta: 10m 58s 274ms\n",
            "\u001b[32m2021-05-01T10:17:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9700/10000, train/hateful_memes/cross_entropy: 0.0017, train/hateful_memes/cross_entropy/avg: 0.1729, train/total_loss: 0.0017, train/total_loss/avg: 0.1729, max mem: 8122.0, experiment: run, epoch: 37, num_updates: 9700, iterations: 9700, max_updates: 10000, lr: 0., ups: 0.41, time: 02m 02s 875ms, time_since_start: 06h 45m 08s 332ms, eta: 12m 42s 322ms\n",
            "\u001b[32m2021-05-01T10:19:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9750/10000, train/hateful_memes/cross_entropy: 0.0017, train/hateful_memes/cross_entropy/avg: 0.1720, train/total_loss: 0.0017, train/total_loss/avg: 0.1720, max mem: 8122.0, experiment: run, epoch: 37, num_updates: 9750, iterations: 9750, max_updates: 10000, lr: 0., ups: 0.40, time: 02m 06s 620ms, time_since_start: 06h 47m 14s 952ms, eta: 10m 54s 627ms\n",
            "\u001b[32m2021-05-01T10:21:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9800/10000, train/hateful_memes/cross_entropy: 0.0019, train/hateful_memes/cross_entropy/avg: 0.1711, train/total_loss: 0.0019, train/total_loss/avg: 0.1711, max mem: 8122.0, experiment: run, epoch: 37, num_updates: 9800, iterations: 9800, max_updates: 10000, lr: 0., ups: 0.37, time: 02m 15s 585ms, time_since_start: 06h 49m 30s 538ms, eta: 09m 20s 783ms\n",
            "\u001b[32m2021-05-01T10:23:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9850/10000, train/hateful_memes/cross_entropy: 0.0029, train/hateful_memes/cross_entropy/avg: 0.1703, train/total_loss: 0.0029, train/total_loss/avg: 0.1703, max mem: 8122.0, experiment: run, epoch: 38, num_updates: 9850, iterations: 9850, max_updates: 10000, lr: 0., ups: 0.43, time: 01m 55s 531ms, time_since_start: 06h 51m 26s 070ms, eta: 05m 58s 380ms\n",
            "\u001b[32m2021-05-01T10:24:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9900/10000, train/hateful_memes/cross_entropy: 0.0029, train/hateful_memes/cross_entropy/avg: 0.1695, train/total_loss: 0.0029, train/total_loss/avg: 0.1695, max mem: 8122.0, experiment: run, epoch: 38, num_updates: 9900, iterations: 9900, max_updates: 10000, lr: 0., ups: 0.59, time: 01m 25s 579ms, time_since_start: 06h 52m 51s 650ms, eta: 02m 56s 978ms\n",
            "\u001b[32m2021-05-01T10:26:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9950/10000, train/hateful_memes/cross_entropy: 0.0030, train/hateful_memes/cross_entropy/avg: 0.1686, train/total_loss: 0.0030, train/total_loss/avg: 0.1686, max mem: 8122.0, experiment: run, epoch: 38, num_updates: 9950, iterations: 9950, max_updates: 10000, lr: 0., ups: 0.46, time: 01m 49s 222ms, time_since_start: 06h 54m 40s 872ms, eta: 01m 52s 936ms\n",
            "\u001b[32m2021-05-01T10:28:47 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[32m2021-05-01T10:28:48 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-05-01T10:28:55 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-05-01T10:29:08 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-05-01T10:29:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10000/10000, train/hateful_memes/cross_entropy: 0.0030, train/hateful_memes/cross_entropy/avg: 0.1678, train/total_loss: 0.0030, train/total_loss/avg: 0.1678, max mem: 8122.0, experiment: run, epoch: 38, num_updates: 10000, iterations: 10000, max_updates: 10000, lr: 0., ups: 0.32, time: 02m 34s 705ms, time_since_start: 06h 57m 15s 578ms, eta: 0ms\n",
            "\u001b[32m2021-05-01T10:29:08 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2021-05-01T10:29:08 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-01T10:29:08 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-01T10:29:08 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2021-05-01T10:29:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10000/10000, val/hateful_memes/cross_entropy: 2.0656, val/total_loss: 2.0656, val/hateful_memes/accuracy: 0.6944, val/hateful_memes/binary_f1: 0.4828, val/hateful_memes/roc_auc: 0.6771, num_updates: 10000, epoch: 38, iterations: 10000, max_updates: 10000, val_time: 45s 663ms, best_update: 5000, best_iteration: 5000, best_val/hateful_memes/roc_auc: 0.700974\n",
            "\u001b[32m2021-05-01T10:29:54 | mmf.trainers.core.training_loop: \u001b[0mStepping into final validation check\n",
            "\u001b[32m2021-05-01T10:29:54 | mmf.utils.checkpoint: \u001b[0mRestoring checkpoint\n",
            "\u001b[32m2021-05-01T10:29:54 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
            "\u001b[32m2021-05-01T10:30:13 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
            "\u001b[32m2021-05-01T10:30:13 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 5000\n",
            "\u001b[32m2021-05-01T10:30:13 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 5000\n",
            "\u001b[32m2021-05-01T10:30:13 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 19\n",
            "\u001b[32m2021-05-01T10:30:14 | mmf.trainers.core.evaluation_loop: \u001b[0mStarting test inference predictions\n",
            "\u001b[32m2021-05-01T10:30:14 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-01T10:30:14 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-01T10:30:14 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "100% 63/63 [02:46<00:00,  2.65s/it]\n",
            "\u001b[32m2021-05-01T10:33:01 | mmf.common.test_reporter: \u001b[0mWrote predictions for hateful_memes to /content/save/hateful_memes_visual_bert_33189342/reports/hateful_memes_run_test_2021-05-01T10:33:01.csv\n",
            "\u001b[32m2021-05-01T10:33:01 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished predicting. Loaded 63\n",
            "\u001b[32m2021-05-01T10:33:01 | mmf.trainers.core.evaluation_loop: \u001b[0m -- skipped 0 batches.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSPhPT3Z575o"
      },
      "source": [
        "Evaluating Pretrained model on Validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JYM-YI053j9",
        "outputId": "402b4faf-b4bd-44be-d1d3-818c082f57e7"
      },
      "source": [
        "#! Evaluating Pretrained model on Validation set\n",
        "!cd /content/mmf/\n",
        "!mmf_run config=$REPLACE_WITH_BASELINE_CONFIG model=$REPLACE_WITH_MODEL_KEY dataset=hateful_memes run_type=val checkpoint.resume_zoo=$REPLACE_WITH_PRETRAINED_ZOO_KEY checkpoint.resume_pretrained=False\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-21 20:12:27.771581: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "\u001b[32m2021-04-21T20:12:30 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/hateful_memes/configs/unimodal/bert.yaml\n",
            "\u001b[32m2021-04-21T20:12:30 | mmf.utils.configuration: \u001b[0mOverriding option model to unimodal_text\n",
            "\u001b[32m2021-04-21T20:12:30 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n",
            "\u001b[32m2021-04-21T20:12:30 | mmf.utils.configuration: \u001b[0mOverriding option run_type to val\n",
            "\u001b[32m2021-04-21T20:12:30 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_zoo to unimodal_text.hateful_memes.bert\n",
            "\u001b[32m2021-04-21T20:12:30 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_pretrained to False\n",
            "\u001b[32m2021-04-21T20:12:30 | mmf: \u001b[0mLogging to: ./save/train.log\n",
            "\u001b[32m2021-04-21T20:12:30 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/hateful_memes/configs/unimodal/bert.yaml', 'model=unimodal_text', 'dataset=hateful_memes', 'run_type=val', 'checkpoint.resume_zoo=unimodal_text.hateful_memes.bert', 'checkpoint.resume_pretrained=False'])\n",
            "\u001b[32m2021-04-21T20:12:30 | mmf_cli.run: \u001b[0mTorch version: 1.8.1+cu102\n",
            "\u001b[32m2021-04-21T20:12:30 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla P100-PCIE-16GB\n",
            "\u001b[32m2021-04-21T20:12:30 | mmf_cli.run: \u001b[0mUsing seed 30677088\n",
            "\u001b[32m2021-04-21T20:12:30 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-21T20:12:32 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-21T20:12:32 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2021-04-21T20:12:32 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
            "\u001b[32m2021-04-21T20:12:32 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
            "\u001b[32m2021-04-21T20:12:32 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
            "\u001b[32m2021-04-21T20:12:32 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
            "\u001b[32m2021-04-21T20:12:40 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
            "\u001b[32m2021-04-21T20:12:40 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-21T20:12:40 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/basecontainer.py:232: UserWarning: cfg.pretty() is deprecated and will be removed in a future version.\n",
            "Use OmegaConf.to_yaml(cfg)\n",
            "\n",
            "  category=UserWarning,\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-21T20:12:40 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/basecontainer.py:232: UserWarning: cfg.pretty() is deprecated and will be removed in a future version.\n",
            "Use OmegaConf.to_yaml(cfg)\n",
            "\n",
            "  category=UserWarning,\n",
            "\n",
            "\u001b[32m2021-04-21T20:12:40 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-21T20:12:42 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-21T20:12:42 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-21T20:12:42 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-21T20:12:42 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-21T20:12:42 | mmf.utils.checkpoint: \u001b[0mMissing keys ['base.encoder.embeddings.position_ids'] in the checkpoint.\n",
            "If this is not your checkpoint, please open up an issue on MMF GitHub. \n",
            "Unexpected keys if any: []\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-21T20:12:42 | py.warnings: \u001b[0m/content/mmf/mmf/utils/checkpoint.py:304: UserWarning: 'optimizer' key is not present in the checkpoint asked to be loaded. Skipping.\n",
            "  \"'optimizer' key is not present in the \"\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-21T20:12:42 | py.warnings: \u001b[0m/content/mmf/mmf/utils/checkpoint.py:304: UserWarning: 'optimizer' key is not present in the checkpoint asked to be loaded. Skipping.\n",
            "  \"'optimizer' key is not present in the \"\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-21T20:12:42 | py.warnings: \u001b[0m/content/mmf/mmf/utils/checkpoint.py:347: UserWarning: 'lr_scheduler' key is not present in the checkpoint asked to be loaded. Setting lr_scheduler's last_epoch to current_iteration.\n",
            "  \"'lr_scheduler' key is not present in the \"\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-21T20:12:42 | py.warnings: \u001b[0m/content/mmf/mmf/utils/checkpoint.py:347: UserWarning: 'lr_scheduler' key is not present in the checkpoint asked to be loaded. Setting lr_scheduler's last_epoch to current_iteration.\n",
            "  \"'lr_scheduler' key is not present in the \"\n",
            "\n",
            "\u001b[32m2021-04-21T20:12:42 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
            "\u001b[32m2021-04-21T20:12:42 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 0\n",
            "\u001b[32m2021-04-21T20:12:42 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 0\n",
            "\u001b[32m2021-04-21T20:12:42 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 0\n",
            "\u001b[32m2021-04-21T20:12:42 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n",
            "\u001b[32m2021-04-21T20:12:42 | mmf.trainers.mmf_trainer: \u001b[0mUnimodalText(\n",
            "  (base): UnimodalBase(\n",
            "    (encoder): BertModelJit(\n",
            "      (embeddings): BertEmbeddingsJit(\n",
            "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "        (position_embeddings): Embedding(512, 768)\n",
            "        (token_type_embeddings): Embedding(2, 768)\n",
            "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (encoder): BertEncoderJit(\n",
            "        (layer): ModuleList(\n",
            "          (0): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (2): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (3): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (4): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (5): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (6): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (7): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (8): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (9): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (10): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (11): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pooler): BertPooler(\n",
            "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (activation): Tanh()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (classifier): MLPClassifer(\n",
            "    (layers): ModuleList(\n",
            "      (0): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (1): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU()\n",
            "      (3): Dropout(p=0.5, inplace=False)\n",
            "      (4): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (5): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (6): ReLU()\n",
            "      (7): Dropout(p=0.5, inplace=False)\n",
            "      (8): Linear(in_features=768, out_features=2, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (losses): Losses(\n",
            "    (losses): ModuleList(\n",
            "      (0): MMFLoss(\n",
            "        (loss_criterion): CrossEntropyLoss(\n",
            "          (loss_fn): CrossEntropyLoss()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "\u001b[32m2021-04-21T20:12:42 | mmf.utils.general: \u001b[0mTotal Parameters: 110668034. Trained Parameters: 110668034\n",
            "\u001b[32m2021-04-21T20:12:42 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on val set\n",
            "\u001b[32m2021-04-21T20:12:42 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "100% 5/5 [00:08<00:00,  1.75s/it]\n",
            "\u001b[32m2021-04-21T20:12:51 | mmf.trainers.callbacks.logistics: \u001b[0mval/hateful_memes/cross_entropy: 0.7017, val/total_loss: 0.7017, val/hateful_memes/accuracy: 0.6167, val/hateful_memes/binary_f1: 0.4069, val/hateful_memes/roc_auc: 0.6119\n",
            "\u001b[32m2021-04-21T20:12:51 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 10s 578ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecjISm5J54PD"
      },
      "source": [
        "Evaluating Pretrained model on Test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Ev1jCIe55Ev",
        "outputId": "24fb9b85-02c4-4b57-922a-6fe7b3ec5550"
      },
      "source": [
        "\n",
        "#! Evaluating Pretrained model on Test set\n",
        "!cd /content/mmf/\n",
        "!mmf_predict config=$REPLACE_WITH_BASELINE_CONFIG model=$REPLACE_WITH_MODEL_KEY dataset=hateful_memes run_type=test checkpoint.resume_zoo=$REPLACE_WITH_PRETRAINED_ZOO_KEY checkpoint.resume_pretrained=False\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-21 20:05:55.082562: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "\u001b[32m2021-04-21T20:05:58 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/hateful_memes/configs/unimodal/bert.yaml\n",
            "\u001b[32m2021-04-21T20:05:58 | mmf.utils.configuration: \u001b[0mOverriding option model to unimodal_text\n",
            "\u001b[32m2021-04-21T20:05:58 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n",
            "\u001b[32m2021-04-21T20:05:58 | mmf.utils.configuration: \u001b[0mOverriding option run_type to test\n",
            "\u001b[32m2021-04-21T20:05:58 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_zoo to unimodal_text.hateful_memes.bert\n",
            "\u001b[32m2021-04-21T20:05:58 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_pretrained to False\n",
            "\u001b[32m2021-04-21T20:05:58 | mmf.utils.configuration: \u001b[0mOverriding option evaluation.predict to true\n",
            "\u001b[32m2021-04-21T20:05:58 | mmf: \u001b[0mLogging to: ./save/train.log\n",
            "\u001b[32m2021-04-21T20:05:58 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/hateful_memes/configs/unimodal/bert.yaml', 'model=unimodal_text', 'dataset=hateful_memes', 'run_type=test', 'checkpoint.resume_zoo=unimodal_text.hateful_memes.bert', 'checkpoint.resume_pretrained=False', 'evaluation.predict=true'])\n",
            "\u001b[32m2021-04-21T20:05:58 | mmf_cli.run: \u001b[0mTorch version: 1.8.1+cu102\n",
            "\u001b[32m2021-04-21T20:05:58 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla P100-PCIE-16GB\n",
            "\u001b[32m2021-04-21T20:05:58 | mmf_cli.run: \u001b[0mUsing seed 58177360\n",
            "\u001b[32m2021-04-21T20:05:58 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-21T20:06:00 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-21T20:06:00 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2021-04-21T20:06:00 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
            "\u001b[32m2021-04-21T20:06:00 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
            "\u001b[32m2021-04-21T20:06:00 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
            "\u001b[32m2021-04-21T20:06:00 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
            "\u001b[32m2021-04-21T20:06:17 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
            "\u001b[32m2021-04-21T20:06:17 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-21T20:06:17 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/basecontainer.py:232: UserWarning: cfg.pretty() is deprecated and will be removed in a future version.\n",
            "Use OmegaConf.to_yaml(cfg)\n",
            "\n",
            "  category=UserWarning,\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-21T20:06:17 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/basecontainer.py:232: UserWarning: cfg.pretty() is deprecated and will be removed in a future version.\n",
            "Use OmegaConf.to_yaml(cfg)\n",
            "\n",
            "  category=UserWarning,\n",
            "\n",
            "\u001b[32m2021-04-21T20:06:17 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
            "[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/models/unimodal_text/unimodal_text.finetuned.hateful_memes_bert.tar.gz to /root/.cache/torch/mmf/data/models/unimodal_text.hateful_memes.bert/unimodal_text.finetuned.hateful_memes_bert.tar.gz ]\n",
            "Downloading unimodal_text.finetuned.hateful_memes_bert.tar.gz: 100% 410M/410M [00:06<00:00, 61.3MB/s]\n",
            "[ Starting checksum for unimodal_text.finetuned.hateful_memes_bert.tar.gz]\n",
            "[ Checksum successful for unimodal_text.finetuned.hateful_memes_bert.tar.gz]\n",
            "Unpacking unimodal_text.finetuned.hateful_memes_bert.tar.gz\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-21T20:06:30 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-21T20:06:30 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-21T20:06:30 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-21T20:06:30 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-21T20:06:31 | mmf.utils.checkpoint: \u001b[0mMissing keys ['base.encoder.embeddings.position_ids'] in the checkpoint.\n",
            "If this is not your checkpoint, please open up an issue on MMF GitHub. \n",
            "Unexpected keys if any: []\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-21T20:06:31 | py.warnings: \u001b[0m/content/mmf/mmf/utils/checkpoint.py:304: UserWarning: 'optimizer' key is not present in the checkpoint asked to be loaded. Skipping.\n",
            "  \"'optimizer' key is not present in the \"\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-21T20:06:31 | py.warnings: \u001b[0m/content/mmf/mmf/utils/checkpoint.py:304: UserWarning: 'optimizer' key is not present in the checkpoint asked to be loaded. Skipping.\n",
            "  \"'optimizer' key is not present in the \"\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-21T20:06:31 | py.warnings: \u001b[0m/content/mmf/mmf/utils/checkpoint.py:347: UserWarning: 'lr_scheduler' key is not present in the checkpoint asked to be loaded. Setting lr_scheduler's last_epoch to current_iteration.\n",
            "  \"'lr_scheduler' key is not present in the \"\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-21T20:06:31 | py.warnings: \u001b[0m/content/mmf/mmf/utils/checkpoint.py:347: UserWarning: 'lr_scheduler' key is not present in the checkpoint asked to be loaded. Setting lr_scheduler's last_epoch to current_iteration.\n",
            "  \"'lr_scheduler' key is not present in the \"\n",
            "\n",
            "\u001b[32m2021-04-21T20:06:31 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
            "\u001b[32m2021-04-21T20:06:31 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 0\n",
            "\u001b[32m2021-04-21T20:06:31 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 0\n",
            "\u001b[32m2021-04-21T20:06:31 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 0\n",
            "\u001b[32m2021-04-21T20:06:31 | mmf.trainers.core.evaluation_loop: \u001b[0mStarting test inference predictions\n",
            "\u001b[32m2021-04-21T20:06:31 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "100% 16/16 [00:23<00:00,  1.48s/it]\n",
            "\u001b[32m2021-04-21T20:06:54 | mmf.common.test_reporter: \u001b[0mWrote predictions for hateful_memes to /content/save/hateful_memes_unimodal_text_58177360/reports/hateful_memes_run_test_2021-04-21T20:06:54.csv\n",
            "\u001b[32m2021-04-21T20:06:54 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished predicting\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhKvYHtWHlyr"
      },
      "source": [
        "## Submit a prediction\n",
        "\n",
        "Now, we will use a pretrained model from MMF to submit a prediction to DrivenData. Run the command in the next block and at the end it will output the path to the csv file generated. Download and upload that file to [DrivenData's submission page](https://www.drivendata.org/competitions/64/hateful-memes/submissions/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlPz8R7QWXIp"
      },
      "source": [
        "!mmf_predict config=projects/hateful_memes/configs/mmbt/defaults.yaml \\\n",
        "  model=mmbt \\ \n",
        "  dataset=hateful_memes \\\n",
        "  run_type=test \\ \n",
        "  checkpoint.resume_zoo=mmbt.hateful_memes.images \\\n",
        "  training.batch_size=16"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDDnTlQZV46f"
      },
      "source": [
        "## Train an existing model\n",
        "\n",
        "We will use MMF to train an existing baseline from MMF's model zoo on the Hateful Memes dataset. Run the next code cell to start training MMBT-Grid model on the dataset. You can adjust the batch size, maximum number of updates, log and evaluation interval among other things by using command line overrides. Read more about MMF's configuration system at https://mmf.readthedocs.io/en/latest/notes/configuration.html."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1nwebqtdWOfZ",
        "outputId": "eb450095-ddde-4e4c-d5f5-c49464e08e1a"
      },
      "source": [
        "!mmf_run config=projects/hateful_memes/configs/mmbt/defaults.yaml \\\n",
        "  model=mmbt \\\n",
        "  dataset=hateful_memes \\\n",
        "  training.log_interval=50 \\\n",
        "  training.max_updates=10000 \\\n",
        "  training.batch_size=32 \\\n",
        "  training.evaluation_interval=500"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-21 23:55:35.715105: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "\u001b[32m2021-04-21T23:55:38 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/hateful_memes/configs/mmbt/defaults.yaml\n",
            "\u001b[32m2021-04-21T23:55:38 | mmf.utils.configuration: \u001b[0mOverriding option model to mmbt\n",
            "\u001b[32m2021-04-21T23:55:38 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n",
            "\u001b[32m2021-04-21T23:55:38 | mmf.utils.configuration: \u001b[0mOverriding option training.log_interval to 50\n",
            "\u001b[32m2021-04-21T23:55:38 | mmf.utils.configuration: \u001b[0mOverriding option training.max_updates to 10000\n",
            "\u001b[32m2021-04-21T23:55:38 | mmf.utils.configuration: \u001b[0mOverriding option training.batch_size to 32\n",
            "\u001b[32m2021-04-21T23:55:38 | mmf.utils.configuration: \u001b[0mOverriding option training.evaluation_interval to 500\n",
            "\u001b[32m2021-04-21T23:55:38 | mmf: \u001b[0mLogging to: ./save/train.log\n",
            "\u001b[32m2021-04-21T23:55:38 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/hateful_memes/configs/mmbt/defaults.yaml', 'model=mmbt', 'dataset=hateful_memes', 'training.log_interval=50', 'training.max_updates=10000', 'training.batch_size=32', 'training.evaluation_interval=500'])\n",
            "\u001b[32m2021-04-21T23:55:38 | mmf_cli.run: \u001b[0mTorch version: 1.8.1+cu102\n",
            "\u001b[32m2021-04-21T23:55:38 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla P100-PCIE-16GB\n",
            "\u001b[32m2021-04-21T23:55:38 | mmf_cli.run: \u001b[0mUsing seed 38507753\n",
            "\u001b[32m2021-04-21T23:55:38 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-21T23:55:40 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-21T23:55:40 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2021-04-21T23:55:40 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
            "\u001b[32m2021-04-21T23:55:40 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
            "\u001b[32m2021-04-21T23:55:40 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
            "\u001b[32m2021-04-21T23:55:40 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
            "\u001b[32m2021-04-21T23:55:50 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
            "\u001b[32m2021-04-21T23:55:50 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-21T23:55:50 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/basecontainer.py:232: UserWarning: cfg.pretty() is deprecated and will be removed in a future version.\n",
            "Use OmegaConf.to_yaml(cfg)\n",
            "\n",
            "  category=UserWarning,\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-21T23:55:50 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/basecontainer.py:232: UserWarning: cfg.pretty() is deprecated and will be removed in a future version.\n",
            "Use OmegaConf.to_yaml(cfg)\n",
            "\n",
            "  category=UserWarning,\n",
            "\n",
            "\u001b[32m2021-04-21T23:55:50 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n",
            "\u001b[32m2021-04-21T23:55:50 | mmf.trainers.mmf_trainer: \u001b[0mMMBT(\n",
            "  (model): MMBTForClassification(\n",
            "    (bert): MMBTBase(\n",
            "      (mmbt): MMBTModel(\n",
            "        (transformer): BertModelJit(\n",
            "          (embeddings): BertEmbeddingsJit(\n",
            "            (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "            (position_embeddings): Embedding(512, 768)\n",
            "            (token_type_embeddings): Embedding(2, 768)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (encoder): BertEncoderJit(\n",
            "            (layer): ModuleList(\n",
            "              (0): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (1): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (2): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (3): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (4): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (5): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (6): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (7): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (8): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (9): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (10): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (11): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (pooler): BertPooler(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (activation): Tanh()\n",
            "          )\n",
            "        )\n",
            "        (modal_encoder): ModalEmbeddings(\n",
            "          (encoder): ResNet152ImageEncoder(\n",
            "            (model): Sequential(\n",
            "              (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): ReLU(inplace=True)\n",
            "              (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "              (4): Sequential(\n",
            "                (0): Bottleneck(\n",
            "                  (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                  (downsample): Sequential(\n",
            "                    (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  )\n",
            "                )\n",
            "                (1): Bottleneck(\n",
            "                  (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (2): Bottleneck(\n",
            "                  (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "              )\n",
            "              (5): Sequential(\n",
            "                (0): Bottleneck(\n",
            "                  (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                  (downsample): Sequential(\n",
            "                    (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "                    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  )\n",
            "                )\n",
            "                (1): Bottleneck(\n",
            "                  (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (2): Bottleneck(\n",
            "                  (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (3): Bottleneck(\n",
            "                  (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (4): Bottleneck(\n",
            "                  (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (5): Bottleneck(\n",
            "                  (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (6): Bottleneck(\n",
            "                  (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (7): Bottleneck(\n",
            "                  (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "              )\n",
            "              (6): Sequential(\n",
            "                (0): Bottleneck(\n",
            "                  (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                  (downsample): Sequential(\n",
            "                    (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "                    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  )\n",
            "                )\n",
            "                (1): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (2): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (3): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (4): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (5): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (6): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (7): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (8): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (9): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (10): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (11): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (12): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (13): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (14): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (15): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (16): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (17): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (18): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (19): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (20): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (21): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (22): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (23): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (24): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (25): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (26): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (27): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (28): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (29): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (30): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (31): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (32): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (33): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (34): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (35): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "              )\n",
            "              (7): Sequential(\n",
            "                (0): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                  (downsample): Sequential(\n",
            "                    (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "                    (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  )\n",
            "                )\n",
            "                (1): Bottleneck(\n",
            "                  (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (2): Bottleneck(\n",
            "                  (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "          )\n",
            "          (proj_embeddings): Linear(in_features=2048, out_features=768, bias=True)\n",
            "          (position_embeddings): Embedding(512, 768)\n",
            "          (token_type_embeddings): Embedding(2, 768)\n",
            "          (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "    (classifier): Sequential(\n",
            "      (0): BertPredictionHeadTransform(\n",
            "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      )\n",
            "      (1): Linear(in_features=768, out_features=2, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (losses): Losses(\n",
            "    (losses): ModuleList(\n",
            "      (0): MMFLoss(\n",
            "        (loss_criterion): CrossEntropyLoss(\n",
            "          (loss_fn): CrossEntropyLoss()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "\u001b[32m2021-04-21T23:55:50 | mmf.utils.general: \u001b[0mTotal Parameters: 169793346. Trained Parameters: 169793346\n",
            "\u001b[32m2021-04-21T23:55:50 | mmf.trainers.core.training_loop: \u001b[0mStarting training...\n",
            "\u001b[32m2021-04-21T23:56:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 50/10000, train/hateful_memes/cross_entropy: 0.6460, train/hateful_memes/cross_entropy/avg: 0.6460, train/total_loss: 0.6460, train/total_loss/avg: 0.6460, max mem: 11656.0, experiment: run, epoch: 1, num_updates: 50, iterations: 50, max_updates: 10000, lr: 0., ups: 1.06, time: 47s 188ms, time_since_start: 47s 265ms, eta: 02h 41m 49s 764ms\n",
            "\u001b[32m2021-04-21T23:57:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/10000, train/hateful_memes/cross_entropy: 0.6460, train/hateful_memes/cross_entropy/avg: 0.6886, train/total_loss: 0.6460, train/total_loss/avg: 0.6886, max mem: 11656.0, experiment: run, epoch: 1, num_updates: 100, iterations: 100, max_updates: 10000, lr: 0., ups: 1.14, time: 44s 624ms, time_since_start: 01m 31s 890ms, eta: 02h 32m 16s 113ms\n",
            "\u001b[32m2021-04-21T23:58:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 150/10000, train/hateful_memes/cross_entropy: 0.6807, train/hateful_memes/cross_entropy/avg: 0.6859, train/total_loss: 0.6807, train/total_loss/avg: 0.6859, max mem: 11656.0, experiment: run, epoch: 1, num_updates: 150, iterations: 150, max_updates: 10000, lr: 0., ups: 1.14, time: 44s 617ms, time_since_start: 02m 16s 507ms, eta: 02h 31m 28s 403ms\n",
            "\u001b[32m2021-04-21T23:58:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/10000, train/hateful_memes/cross_entropy: 0.6460, train/hateful_memes/cross_entropy/avg: 0.6644, train/total_loss: 0.6460, train/total_loss/avg: 0.6644, max mem: 11656.0, experiment: run, epoch: 1, num_updates: 200, iterations: 200, max_updates: 10000, lr: 0., ups: 1.14, time: 44s 633ms, time_since_start: 03m 01s 140ms, eta: 02h 30m 45s 546ms\n",
            "\u001b[32m2021-04-21T23:59:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 250/10000, train/hateful_memes/cross_entropy: 0.6807, train/hateful_memes/cross_entropy/avg: 0.6679, train/total_loss: 0.6807, train/total_loss/avg: 0.6679, max mem: 11656.0, experiment: run, epoch: 1, num_updates: 250, iterations: 250, max_updates: 10000, lr: 0., ups: 1.14, time: 44s 630ms, time_since_start: 03m 45s 771ms, eta: 02h 29m 58s 831ms\n",
            "\u001b[32m2021-04-22T00:00:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/10000, train/hateful_memes/cross_entropy: 0.6460, train/hateful_memes/cross_entropy/avg: 0.6460, train/total_loss: 0.6460, train/total_loss/avg: 0.6460, max mem: 11656.0, experiment: run, epoch: 2, num_updates: 300, iterations: 300, max_updates: 10000, lr: 0., ups: 1.09, time: 46s 807ms, time_since_start: 04m 32s 578ms, eta: 02h 36m 29s 410ms\n",
            "\u001b[32m2021-04-22T00:01:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 350/10000, train/hateful_memes/cross_entropy: 0.6460, train/hateful_memes/cross_entropy/avg: 0.6409, train/total_loss: 0.6460, train/total_loss/avg: 0.6409, max mem: 11656.0, experiment: run, epoch: 2, num_updates: 350, iterations: 350, max_updates: 10000, lr: 0., ups: 1.14, time: 44s 621ms, time_since_start: 05m 17s 200ms, eta: 02h 28m 24s 790ms\n",
            "\u001b[32m2021-04-22T00:01:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/10000, train/hateful_memes/cross_entropy: 0.6105, train/hateful_memes/cross_entropy/avg: 0.6359, train/total_loss: 0.6105, train/total_loss/avg: 0.6359, max mem: 11656.0, experiment: run, epoch: 2, num_updates: 400, iterations: 400, max_updates: 10000, lr: 0., ups: 1.14, time: 44s 583ms, time_since_start: 06m 01s 784ms, eta: 02h 27m 31s 137ms\n",
            "\u001b[32m2021-04-22T00:02:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 450/10000, train/hateful_memes/cross_entropy: 0.6105, train/hateful_memes/cross_entropy/avg: 0.6325, train/total_loss: 0.6105, train/total_loss/avg: 0.6325, max mem: 11656.0, experiment: run, epoch: 2, num_updates: 450, iterations: 450, max_updates: 10000, lr: 0., ups: 1.14, time: 44s 636ms, time_since_start: 06m 46s 421ms, eta: 02h 26m 55s 505ms\n",
            "\u001b[32m2021-04-22T00:03:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/10000, train/hateful_memes/cross_entropy: 0.6105, train/hateful_memes/cross_entropy/avg: 0.6335, train/total_loss: 0.6105, train/total_loss/avg: 0.6335, max mem: 11656.0, experiment: run, epoch: 2, num_updates: 500, iterations: 500, max_updates: 10000, lr: 0., ups: 1.14, time: 44s 639ms, time_since_start: 07m 31s 061ms, eta: 02h 26m 09s 976ms\n",
            "\u001b[32m2021-04-22T00:03:21 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2021-04-22T00:03:21 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[32m2021-04-22T00:03:30 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-04-22T00:03:41 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
            "\u001b[32m2021-04-22T00:04:07 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-04-22T00:04:30 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-04-22T00:04:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/10000, val/hateful_memes/cross_entropy: 0.6998, val/total_loss: 0.6998, val/hateful_memes/accuracy: 0.6204, val/hateful_memes/binary_f1: 0.3003, val/hateful_memes/roc_auc: 0.5743, num_updates: 500, epoch: 2, iterations: 500, max_updates: 10000, val_time: 01m 09s 316ms, best_update: 500, best_iteration: 500, best_val/hateful_memes/roc_auc: 0.574265\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-22T00:05:00 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-22T00:05:00 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2021-04-22T00:05:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 550/10000, train/hateful_memes/cross_entropy: 0.6105, train/hateful_memes/cross_entropy/avg: 0.6308, train/total_loss: 0.6105, train/total_loss/avg: 0.6308, max mem: 11667.0, experiment: run, epoch: 3, num_updates: 550, iterations: 550, max_updates: 10000, lr: 0., ups: 1.06, time: 47s 545ms, time_since_start: 09m 27s 925ms, eta: 02h 34m 51s 703ms\n",
            "\u001b[32m2021-04-22T00:06:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/10000, train/hateful_memes/cross_entropy: 0.6053, train/hateful_memes/cross_entropy/avg: 0.6281, train/total_loss: 0.6053, train/total_loss/avg: 0.6281, max mem: 11667.0, experiment: run, epoch: 3, num_updates: 600, iterations: 600, max_updates: 10000, lr: 0., ups: 1.14, time: 44s 623ms, time_since_start: 10m 12s 548ms, eta: 02h 24m 34s 413ms\n",
            "\u001b[32m2021-04-22T00:06:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 650/10000, train/hateful_memes/cross_entropy: 0.6105, train/hateful_memes/cross_entropy/avg: 0.6321, train/total_loss: 0.6105, train/total_loss/avg: 0.6321, max mem: 11667.0, experiment: run, epoch: 3, num_updates: 650, iterations: 650, max_updates: 10000, lr: 0., ups: 1.14, time: 44s 641ms, time_since_start: 10m 57s 190ms, eta: 02h 23m 51s 766ms\n",
            "\u001b[32m2021-04-22T00:07:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/10000, train/hateful_memes/cross_entropy: 0.6053, train/hateful_memes/cross_entropy/avg: 0.6155, train/total_loss: 0.6053, train/total_loss/avg: 0.6155, max mem: 11667.0, experiment: run, epoch: 3, num_updates: 700, iterations: 700, max_updates: 10000, lr: 0., ups: 1.14, time: 44s 639ms, time_since_start: 11m 41s 829ms, eta: 02h 23m 05s 212ms\n",
            "\u001b[32m2021-04-22T00:08:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 750/10000, train/hateful_memes/cross_entropy: 0.6105, train/hateful_memes/cross_entropy/avg: 0.6163, train/total_loss: 0.6105, train/total_loss/avg: 0.6163, max mem: 11667.0, experiment: run, epoch: 3, num_updates: 750, iterations: 750, max_updates: 10000, lr: 0., ups: 1.14, time: 44s 653ms, time_since_start: 12m 26s 483ms, eta: 02h 22m 21s 806ms\n",
            "\u001b[32m2021-04-22T00:09:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/10000, train/hateful_memes/cross_entropy: 0.6053, train/hateful_memes/cross_entropy/avg: 0.6131, train/total_loss: 0.6053, train/total_loss/avg: 0.6131, max mem: 11667.0, experiment: run, epoch: 4, num_updates: 800, iterations: 800, max_updates: 10000, lr: 0., ups: 1.09, time: 46s 485ms, time_since_start: 13m 12s 968ms, eta: 02h 27m 24s 184ms\n",
            "\u001b[32m2021-04-22T00:09:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 850/10000, train/hateful_memes/cross_entropy: 0.6053, train/hateful_memes/cross_entropy/avg: 0.6030, train/total_loss: 0.6053, train/total_loss/avg: 0.6030, max mem: 11667.0, experiment: run, epoch: 4, num_updates: 850, iterations: 850, max_updates: 10000, lr: 0., ups: 1.14, time: 44s 786ms, time_since_start: 13m 57s 755ms, eta: 02h 21m 14s 675ms\n",
            "\u001b[32m2021-04-22T00:10:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/10000, train/hateful_memes/cross_entropy: 0.6036, train/hateful_memes/cross_entropy/avg: 0.6023, train/total_loss: 0.6036, train/total_loss/avg: 0.6023, max mem: 11667.0, experiment: run, epoch: 4, num_updates: 900, iterations: 900, max_updates: 10000, lr: 0., ups: 1.14, time: 44s 659ms, time_since_start: 14m 42s 415ms, eta: 02h 20m 04s 363ms\n",
            "\u001b[32m2021-04-22T00:11:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 950/10000, train/hateful_memes/cross_entropy: 0.6036, train/hateful_memes/cross_entropy/avg: 0.5946, train/total_loss: 0.6036, train/total_loss/avg: 0.5946, max mem: 11667.0, experiment: run, epoch: 4, num_updates: 950, iterations: 950, max_updates: 10000, lr: 0., ups: 1.14, time: 44s 629ms, time_since_start: 15m 27s 044ms, eta: 02h 19m 12s 642ms\n",
            "\u001b[32m2021-04-22T00:12:02 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[32m2021-04-22T00:12:02 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-04-22T00:12:12 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-04-22T00:12:37 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-04-22T00:12:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/10000, train/hateful_memes/cross_entropy: 0.6010, train/hateful_memes/cross_entropy/avg: 0.5875, train/total_loss: 0.6010, train/total_loss/avg: 0.5875, max mem: 11667.0, experiment: run, epoch: 4, num_updates: 1000, iterations: 1000, max_updates: 10000, lr: 0.00001, ups: 0.63, time: 01m 19s 933ms, time_since_start: 16m 46s 978ms, eta: 04h 07m 57s 155ms\n",
            "\u001b[32m2021-04-22T00:12:37 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2021-04-22T00:12:39 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-22T00:12:39 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-22T00:12:39 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2021-04-22T00:12:47 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-04-22T00:13:02 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
            "\u001b[32m2021-04-22T00:13:27 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-04-22T00:13:52 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-04-22T00:13:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/10000, val/hateful_memes/cross_entropy: 0.7036, val/total_loss: 0.7036, val/hateful_memes/accuracy: 0.6463, val/hateful_memes/binary_f1: 0.3436, val/hateful_memes/roc_auc: 0.6195, num_updates: 1000, epoch: 4, iterations: 1000, max_updates: 10000, val_time: 01m 15s 255ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.619485\n",
            "\u001b[32m2021-04-22T00:14:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1050/10000, train/hateful_memes/cross_entropy: 0.5997, train/hateful_memes/cross_entropy/avg: 0.5841, train/total_loss: 0.5997, train/total_loss/avg: 0.5841, max mem: 11667.0, experiment: run, epoch: 4, num_updates: 1050, iterations: 1050, max_updates: 10000, lr: 0.00001, ups: 1.11, time: 45s 452ms, time_since_start: 18m 47s 687ms, eta: 02h 20m 12s 645ms\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-22T00:14:50 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-22T00:14:50 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2021-04-22T00:15:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1100/10000, train/hateful_memes/cross_entropy: 0.5986, train/hateful_memes/cross_entropy/avg: 0.5782, train/total_loss: 0.5986, train/total_loss/avg: 0.5782, max mem: 11667.0, experiment: run, epoch: 5, num_updates: 1100, iterations: 1100, max_updates: 10000, lr: 0.00001, ups: 1.09, time: 46s 824ms, time_since_start: 19m 34s 512ms, eta: 02h 23m 38s 238ms\n",
            "\u001b[32m2021-04-22T00:16:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1150/10000, train/hateful_memes/cross_entropy: 0.5894, train/hateful_memes/cross_entropy/avg: 0.5650, train/total_loss: 0.5894, train/total_loss/avg: 0.5650, max mem: 11667.0, experiment: run, epoch: 5, num_updates: 1150, iterations: 1150, max_updates: 10000, lr: 0.00001, ups: 1.14, time: 44s 681ms, time_since_start: 20m 19s 194ms, eta: 02h 16m 17s 560ms\n",
            "\u001b[32m2021-04-22T00:16:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/10000, train/hateful_memes/cross_entropy: 0.5656, train/hateful_memes/cross_entropy/avg: 0.5632, train/total_loss: 0.5656, train/total_loss/avg: 0.5632, max mem: 11667.0, experiment: run, epoch: 5, num_updates: 1200, iterations: 1200, max_updates: 10000, lr: 0.00001, ups: 1.14, time: 44s 645ms, time_since_start: 21m 03s 840ms, eta: 02h 15m 24s 773ms\n",
            "\u001b[32m2021-04-22T00:17:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1250/10000, train/hateful_memes/cross_entropy: 0.5363, train/hateful_memes/cross_entropy/avg: 0.5593, train/total_loss: 0.5363, train/total_loss/avg: 0.5593, max mem: 11667.0, experiment: run, epoch: 5, num_updates: 1250, iterations: 1250, max_updates: 10000, lr: 0.00001, ups: 1.14, time: 44s 634ms, time_since_start: 21m 48s 474ms, eta: 02h 14m 36s 631ms\n",
            "\u001b[32m2021-04-22T00:18:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1300/10000, train/hateful_memes/cross_entropy: 0.5656, train/hateful_memes/cross_entropy/avg: 0.5618, train/total_loss: 0.5656, train/total_loss/avg: 0.5618, max mem: 11667.0, experiment: run, epoch: 5, num_updates: 1300, iterations: 1300, max_updates: 10000, lr: 0.00001, ups: 1.14, time: 44s 644ms, time_since_start: 22m 33s 118ms, eta: 02h 13m 52s 206ms\n",
            "\u001b[32m2021-04-22T00:19:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1350/10000, train/hateful_memes/cross_entropy: 0.5209, train/hateful_memes/cross_entropy/avg: 0.5562, train/total_loss: 0.5209, train/total_loss/avg: 0.5562, max mem: 11667.0, experiment: run, epoch: 6, num_updates: 1350, iterations: 1350, max_updates: 10000, lr: 0.00001, ups: 1.09, time: 46s 677ms, time_since_start: 23m 19s 796ms, eta: 02h 19m 09s 691ms\n",
            "\u001b[32m2021-04-22T00:19:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/10000, train/hateful_memes/cross_entropy: 0.5209, train/hateful_memes/cross_entropy/avg: 0.5553, train/total_loss: 0.5209, train/total_loss/avg: 0.5553, max mem: 11667.0, experiment: run, epoch: 6, num_updates: 1400, iterations: 1400, max_updates: 10000, lr: 0.00001, ups: 1.14, time: 44s 643ms, time_since_start: 24m 04s 439ms, eta: 02h 12m 19s 768ms\n",
            "\u001b[32m2021-04-22T00:20:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1450/10000, train/hateful_memes/cross_entropy: 0.5170, train/hateful_memes/cross_entropy/avg: 0.5467, train/total_loss: 0.5170, train/total_loss/avg: 0.5467, max mem: 11667.0, experiment: run, epoch: 6, num_updates: 1450, iterations: 1450, max_updates: 10000, lr: 0.00001, ups: 1.14, time: 44s 610ms, time_since_start: 24m 49s 049ms, eta: 02h 11m 27s 713ms\n",
            "\u001b[32m2021-04-22T00:21:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1500/10000, train/hateful_memes/cross_entropy: 0.4800, train/hateful_memes/cross_entropy/avg: 0.5445, train/total_loss: 0.4800, train/total_loss/avg: 0.5445, max mem: 11667.0, experiment: run, epoch: 6, num_updates: 1500, iterations: 1500, max_updates: 10000, lr: 0.00001, ups: 1.14, time: 44s 641ms, time_since_start: 25m 33s 691ms, eta: 02h 10m 47s 066ms\n",
            "\u001b[32m2021-04-22T00:21:24 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2021-04-22T00:21:24 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[32m2021-04-22T00:21:32 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-04-22T00:21:42 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-04-22T00:22:07 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-04-22T00:22:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1500/10000, val/hateful_memes/cross_entropy: 0.8329, val/total_loss: 0.8329, val/hateful_memes/accuracy: 0.6259, val/hateful_memes/binary_f1: 0.2681, val/hateful_memes/roc_auc: 0.6187, num_updates: 1500, epoch: 6, iterations: 1500, max_updates: 10000, val_time: 45s 819ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.619485\n",
            "\u001b[32m2021-04-22T00:22:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1550/10000, train/hateful_memes/cross_entropy: 0.4648, train/hateful_memes/cross_entropy/avg: 0.5374, train/total_loss: 0.4648, train/total_loss/avg: 0.5374, max mem: 11667.0, experiment: run, epoch: 6, num_updates: 1550, iterations: 1550, max_updates: 10000, lr: 0.00001, ups: 1.11, time: 45s 252ms, time_since_start: 27m 04s 764ms, eta: 02h 11m 47s 642ms\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-22T00:23:36 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-22T00:23:36 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2021-04-22T00:23:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1600/10000, train/hateful_memes/cross_entropy: 0.4567, train/hateful_memes/cross_entropy/avg: 0.5283, train/total_loss: 0.4567, train/total_loss/avg: 0.5283, max mem: 11667.0, experiment: run, epoch: 7, num_updates: 1600, iterations: 1600, max_updates: 10000, lr: 0.00001, ups: 1.09, time: 46s 755ms, time_since_start: 27m 51s 519ms, eta: 02h 15m 21s 926ms\n",
            "\u001b[32m2021-04-22T00:24:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1650/10000, train/hateful_memes/cross_entropy: 0.4543, train/hateful_memes/cross_entropy/avg: 0.5207, train/total_loss: 0.4543, train/total_loss/avg: 0.5207, max mem: 11667.0, experiment: run, epoch: 7, num_updates: 1650, iterations: 1650, max_updates: 10000, lr: 0.00001, ups: 1.14, time: 44s 637ms, time_since_start: 28m 36s 157ms, eta: 02h 08m 27s 986ms\n",
            "\u001b[32m2021-04-22T00:25:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1700/10000, train/hateful_memes/cross_entropy: 0.4543, train/hateful_memes/cross_entropy/avg: 0.5115, train/total_loss: 0.4543, train/total_loss/avg: 0.5115, max mem: 11667.0, experiment: run, epoch: 7, num_updates: 1700, iterations: 1700, max_updates: 10000, lr: 0.00001, ups: 1.14, time: 44s 680ms, time_since_start: 29m 20s 838ms, eta: 02h 07m 49s 192ms\n",
            "\u001b[32m2021-04-22T00:25:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1750/10000, train/hateful_memes/cross_entropy: 0.4515, train/hateful_memes/cross_entropy/avg: 0.5024, train/total_loss: 0.4515, train/total_loss/avg: 0.5024, max mem: 11667.0, experiment: run, epoch: 7, num_updates: 1750, iterations: 1750, max_updates: 10000, lr: 0.00001, ups: 1.14, time: 44s 638ms, time_since_start: 30m 05s 476ms, eta: 02h 06m 55s 751ms\n",
            "\u001b[32m2021-04-22T00:26:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1800/10000, train/hateful_memes/cross_entropy: 0.4419, train/hateful_memes/cross_entropy/avg: 0.4970, train/total_loss: 0.4419, train/total_loss/avg: 0.4970, max mem: 11667.0, experiment: run, epoch: 7, num_updates: 1800, iterations: 1800, max_updates: 10000, lr: 0.00001, ups: 1.14, time: 44s 608ms, time_since_start: 30m 50s 085ms, eta: 02h 06m 04s 588ms\n",
            "\u001b[32m2021-04-22T00:27:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1850/10000, train/hateful_memes/cross_entropy: 0.4130, train/hateful_memes/cross_entropy/avg: 0.4880, train/total_loss: 0.4130, train/total_loss/avg: 0.4880, max mem: 11667.0, experiment: run, epoch: 7, num_updates: 1850, iterations: 1850, max_updates: 10000, lr: 0.00001, ups: 1.14, time: 44s 603ms, time_since_start: 31m 34s 689ms, eta: 02h 05m 17s 592ms\n",
            "\u001b[32m2021-04-22T00:28:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1900/10000, train/hateful_memes/cross_entropy: 0.3230, train/hateful_memes/cross_entropy/avg: 0.4803, train/total_loss: 0.3230, train/total_loss/avg: 0.4803, max mem: 11667.0, experiment: run, epoch: 8, num_updates: 1900, iterations: 1900, max_updates: 10000, lr: 0.00001, ups: 1.09, time: 46s 624ms, time_since_start: 32m 21s 313ms, eta: 02h 10m 09s 901ms\n",
            "\u001b[32m2021-04-22T00:28:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1950/10000, train/hateful_memes/cross_entropy: 0.3083, train/hateful_memes/cross_entropy/avg: 0.4695, train/total_loss: 0.3083, train/total_loss/avg: 0.4695, max mem: 11667.0, experiment: run, epoch: 8, num_updates: 1950, iterations: 1950, max_updates: 10000, lr: 0.00001, ups: 1.14, time: 44s 617ms, time_since_start: 33m 05s 931ms, eta: 02h 03m 47s 705ms\n",
            "\u001b[32m2021-04-22T00:29:41 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[32m2021-04-22T00:29:41 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-04-22T00:29:51 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-04-22T00:30:16 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-04-22T00:30:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/10000, train/hateful_memes/cross_entropy: 0.3073, train/hateful_memes/cross_entropy/avg: 0.4592, train/total_loss: 0.3073, train/total_loss/avg: 0.4592, max mem: 11667.0, experiment: run, epoch: 8, num_updates: 2000, iterations: 2000, max_updates: 10000, lr: 0.00001, ups: 0.63, time: 01m 19s 794ms, time_since_start: 34m 25s 725ms, eta: 03h 40m 01s 222ms\n",
            "\u001b[32m2021-04-22T00:30:16 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2021-04-22T00:30:16 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-22T00:30:16 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-22T00:30:16 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2021-04-22T00:30:26 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-04-22T00:30:41 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-04-22T00:31:06 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-04-22T00:31:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/10000, val/hateful_memes/cross_entropy: 1.2757, val/total_loss: 1.2757, val/hateful_memes/accuracy: 0.6296, val/hateful_memes/binary_f1: 0.3464, val/hateful_memes/roc_auc: 0.5838, num_updates: 2000, epoch: 8, iterations: 2000, max_updates: 10000, val_time: 50s 234ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.619485\n",
            "\u001b[32m2021-04-22T00:31:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2050/10000, train/hateful_memes/cross_entropy: 0.2785, train/hateful_memes/cross_entropy/avg: 0.4545, train/total_loss: 0.2785, train/total_loss/avg: 0.4545, max mem: 11667.0, experiment: run, epoch: 8, num_updates: 2050, iterations: 2050, max_updates: 10000, lr: 0.00001, ups: 1.11, time: 45s 372ms, time_since_start: 36m 01s 335ms, eta: 02h 04m 19s 527ms\n",
            "\u001b[32m2021-04-22T00:32:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2100/10000, train/hateful_memes/cross_entropy: 0.2754, train/hateful_memes/cross_entropy/avg: 0.4479, train/total_loss: 0.2754, train/total_loss/avg: 0.4479, max mem: 11667.0, experiment: run, epoch: 8, num_updates: 2100, iterations: 2100, max_updates: 10000, lr: 0.00001, ups: 1.14, time: 44s 648ms, time_since_start: 36m 45s 983ms, eta: 02h 01m 34s 248ms\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-22T00:33:01 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-22T00:33:01 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2021-04-22T00:33:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2150/10000, train/hateful_memes/cross_entropy: 0.2671, train/hateful_memes/cross_entropy/avg: 0.4394, train/total_loss: 0.2671, train/total_loss/avg: 0.4394, max mem: 11667.0, experiment: run, epoch: 9, num_updates: 2150, iterations: 2150, max_updates: 10000, lr: 0.00001, ups: 1.09, time: 46s 629ms, time_since_start: 37m 32s 613ms, eta: 02h 06m 09s 779ms\n",
            "\u001b[32m2021-04-22T00:34:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2200/10000, train/hateful_memes/cross_entropy: 0.2472, train/hateful_memes/cross_entropy/avg: 0.4346, train/total_loss: 0.2472, train/total_loss/avg: 0.4346, max mem: 11667.0, experiment: run, epoch: 9, num_updates: 2200, iterations: 2200, max_updates: 10000, lr: 0.00001, ups: 1.14, time: 44s 612ms, time_since_start: 38m 17s 226ms, eta: 01h 59m 56s 209ms\n",
            "\u001b[32m2021-04-22T00:34:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2250/10000, train/hateful_memes/cross_entropy: 0.2248, train/hateful_memes/cross_entropy/avg: 0.4263, train/total_loss: 0.2248, train/total_loss/avg: 0.4263, max mem: 11667.0, experiment: run, epoch: 9, num_updates: 2250, iterations: 2250, max_updates: 10000, lr: 0.00001, ups: 1.14, time: 44s 622ms, time_since_start: 39m 01s 848ms, eta: 01h 59m 11s 657ms\n",
            "\u001b[32m2021-04-22T00:35:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2300/10000, train/hateful_memes/cross_entropy: 0.2069, train/hateful_memes/cross_entropy/avg: 0.4177, train/total_loss: 0.2069, train/total_loss/avg: 0.4177, max mem: 11667.0, experiment: run, epoch: 9, num_updates: 2300, iterations: 2300, max_updates: 10000, lr: 0.00001, ups: 1.14, time: 44s 624ms, time_since_start: 39m 46s 473ms, eta: 01h 58m 25s 908ms\n",
            "\u001b[32m2021-04-22T00:36:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2350/10000, train/hateful_memes/cross_entropy: 0.1933, train/hateful_memes/cross_entropy/avg: 0.4099, train/total_loss: 0.1933, train/total_loss/avg: 0.4099, max mem: 11667.0, experiment: run, epoch: 9, num_updates: 2350, iterations: 2350, max_updates: 10000, lr: 0.00001, ups: 1.14, time: 44s 607ms, time_since_start: 40m 31s 081ms, eta: 01h 57m 37s 026ms\n",
            "\u001b[32m2021-04-22T00:37:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2400/10000, train/hateful_memes/cross_entropy: 0.1927, train/hateful_memes/cross_entropy/avg: 0.4016, train/total_loss: 0.1927, train/total_loss/avg: 0.4016, max mem: 11667.0, experiment: run, epoch: 10, num_updates: 2400, iterations: 2400, max_updates: 10000, lr: 0.00001, ups: 1.09, time: 46s 567ms, time_since_start: 41m 17s 649ms, eta: 02h 01m 58s 997ms\n",
            "\u001b[32m2021-04-22T00:37:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2450/10000, train/hateful_memes/cross_entropy: 0.1764, train/hateful_memes/cross_entropy/avg: 0.3965, train/total_loss: 0.1764, train/total_loss/avg: 0.3965, max mem: 11667.0, experiment: run, epoch: 10, num_updates: 2450, iterations: 2450, max_updates: 10000, lr: 0.00001, ups: 1.14, time: 44s 646ms, time_since_start: 42m 02s 295ms, eta: 01h 56m 10s 764ms\n",
            "\u001b[32m2021-04-22T00:38:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2500/10000, train/hateful_memes/cross_entropy: 0.1660, train/hateful_memes/cross_entropy/avg: 0.3886, train/total_loss: 0.1660, train/total_loss/avg: 0.3886, max mem: 11667.0, experiment: run, epoch: 10, num_updates: 2500, iterations: 2500, max_updates: 10000, lr: 0.00001, ups: 1.14, time: 44s 583ms, time_since_start: 42m 46s 879ms, eta: 01h 55m 14s 939ms\n",
            "\u001b[32m2021-04-22T00:38:37 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2021-04-22T00:38:37 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[32m2021-04-22T00:38:45 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-04-22T00:38:55 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-04-22T00:39:20 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-04-22T00:39:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2500/10000, val/hateful_memes/cross_entropy: 1.6936, val/total_loss: 1.6936, val/hateful_memes/accuracy: 0.5926, val/hateful_memes/binary_f1: 0.3529, val/hateful_memes/roc_auc: 0.5895, num_updates: 2500, epoch: 10, iterations: 2500, max_updates: 10000, val_time: 43s 439ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.619485\n",
            "\u001b[32m2021-04-22T00:40:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2550/10000, train/hateful_memes/cross_entropy: 0.1515, train/hateful_memes/cross_entropy/avg: 0.3817, train/total_loss: 0.1515, train/total_loss/avg: 0.3817, max mem: 11667.0, experiment: run, epoch: 10, num_updates: 2550, iterations: 2550, max_updates: 10000, lr: 0.00001, ups: 1.11, time: 45s 343ms, time_since_start: 44m 15s 663ms, eta: 01h 56m 25s 861ms\n",
            "\u001b[32m2021-04-22T00:40:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2600/10000, train/hateful_memes/cross_entropy: 0.0883, train/hateful_memes/cross_entropy/avg: 0.3760, train/total_loss: 0.0883, train/total_loss/avg: 0.3760, max mem: 11667.0, experiment: run, epoch: 10, num_updates: 2600, iterations: 2600, max_updates: 10000, lr: 0.00001, ups: 1.14, time: 44s 631ms, time_since_start: 45m 294ms, eta: 01h 53m 49s 998ms\n",
            "\u001b[32m2021-04-22T00:41:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2650/10000, train/hateful_memes/cross_entropy: 0.0883, train/hateful_memes/cross_entropy/avg: 0.3717, train/total_loss: 0.0883, train/total_loss/avg: 0.3717, max mem: 11667.0, experiment: run, epoch: 10, num_updates: 2650, iterations: 2650, max_updates: 10000, lr: 0.00001, ups: 1.14, time: 44s 665ms, time_since_start: 45m 44s 960ms, eta: 01h 53m 09s 080ms\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-22T00:41:44 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-22T00:41:44 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2021-04-22T00:42:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2700/10000, train/hateful_memes/cross_entropy: 0.0883, train/hateful_memes/cross_entropy/avg: 0.3692, train/total_loss: 0.0883, train/total_loss/avg: 0.3692, max mem: 11667.0, experiment: run, epoch: 11, num_updates: 2700, iterations: 2700, max_updates: 10000, lr: 0.00001, ups: 1.09, time: 46s 657ms, time_since_start: 46m 31s 617ms, eta: 01h 57m 23s 538ms\n",
            "\u001b[32m2021-04-22T00:43:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2750/10000, train/hateful_memes/cross_entropy: 0.0883, train/hateful_memes/cross_entropy/avg: 0.3642, train/total_loss: 0.0883, train/total_loss/avg: 0.3642, max mem: 11667.0, experiment: run, epoch: 11, num_updates: 2750, iterations: 2750, max_updates: 10000, lr: 0.00001, ups: 1.14, time: 44s 630ms, time_since_start: 47m 16s 247ms, eta: 01h 51m 31s 380ms\n",
            "\u001b[32m2021-04-22T00:43:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2800/10000, train/hateful_memes/cross_entropy: 0.0883, train/hateful_memes/cross_entropy/avg: 0.3597, train/total_loss: 0.0883, train/total_loss/avg: 0.3597, max mem: 11667.0, experiment: run, epoch: 11, num_updates: 2800, iterations: 2800, max_updates: 10000, lr: 0.00001, ups: 1.14, time: 44s 650ms, time_since_start: 48m 897ms, eta: 01h 50m 48s 239ms\n",
            "\u001b[32m2021-04-22T00:44:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2850/10000, train/hateful_memes/cross_entropy: 0.0857, train/hateful_memes/cross_entropy/avg: 0.3546, train/total_loss: 0.0857, train/total_loss/avg: 0.3546, max mem: 11667.0, experiment: run, epoch: 11, num_updates: 2850, iterations: 2850, max_updates: 10000, lr: 0.00001, ups: 1.14, time: 44s 611ms, time_since_start: 48m 45s 508ms, eta: 01h 49m 56s 305ms\n",
            "\u001b[32m2021-04-22T00:45:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2900/10000, train/hateful_memes/cross_entropy: 0.0682, train/hateful_memes/cross_entropy/avg: 0.3493, train/total_loss: 0.0682, train/total_loss/avg: 0.3493, max mem: 11667.0, experiment: run, epoch: 11, num_updates: 2900, iterations: 2900, max_updates: 10000, lr: 0.00001, ups: 1.14, time: 44s 599ms, time_since_start: 49m 30s 108ms, eta: 01h 49m 08s 527ms\n",
            "\u001b[32m2021-04-22T00:46:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2950/10000, train/hateful_memes/cross_entropy: 0.0682, train/hateful_memes/cross_entropy/avg: 0.3435, train/total_loss: 0.0682, train/total_loss/avg: 0.3435, max mem: 11667.0, experiment: run, epoch: 12, num_updates: 2950, iterations: 2950, max_updates: 10000, lr: 0.00001, ups: 1.09, time: 46s 895ms, time_since_start: 50m 17s 004ms, eta: 01h 53m 57s 103ms\n",
            "\u001b[32m2021-04-22T00:46:52 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[32m2021-04-22T00:46:52 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-04-22T00:47:01 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-04-22T00:47:27 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-04-22T00:47:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/10000, train/hateful_memes/cross_entropy: 0.0857, train/hateful_memes/cross_entropy/avg: 0.3407, train/total_loss: 0.0857, train/total_loss/avg: 0.3407, max mem: 11667.0, experiment: run, epoch: 12, num_updates: 3000, iterations: 3000, max_updates: 10000, lr: 0.00001, ups: 0.63, time: 01m 19s 614ms, time_since_start: 51m 36s 618ms, eta: 03h 12m 04s 959ms\n",
            "\u001b[32m2021-04-22T00:47:27 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2021-04-22T00:47:27 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-22T00:47:27 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-22T00:47:27 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2021-04-22T00:47:37 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-04-22T00:47:52 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-04-22T00:48:17 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-04-22T00:48:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/10000, val/hateful_memes/cross_entropy: 1.8806, val/total_loss: 1.8806, val/hateful_memes/accuracy: 0.6093, val/hateful_memes/binary_f1: 0.3664, val/hateful_memes/roc_auc: 0.5937, num_updates: 3000, epoch: 12, iterations: 3000, max_updates: 10000, val_time: 50s 264ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.619485\n",
            "\u001b[32m2021-04-22T00:49:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3050/10000, train/hateful_memes/cross_entropy: 0.0682, train/hateful_memes/cross_entropy/avg: 0.3353, train/total_loss: 0.0682, train/total_loss/avg: 0.3353, max mem: 11667.0, experiment: run, epoch: 12, num_updates: 3050, iterations: 3050, max_updates: 10000, lr: 0.00001, ups: 1.11, time: 45s 407ms, time_since_start: 53m 12s 292ms, eta: 01h 48m 46s 207ms\n",
            "\u001b[32m2021-04-22T00:49:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3100/10000, train/hateful_memes/cross_entropy: 0.0682, train/hateful_memes/cross_entropy/avg: 0.3317, train/total_loss: 0.0682, train/total_loss/avg: 0.3317, max mem: 11667.0, experiment: run, epoch: 12, num_updates: 3100, iterations: 3100, max_updates: 10000, lr: 0.00001, ups: 1.14, time: 44s 673ms, time_since_start: 53m 56s 966ms, eta: 01h 46m 14s 607ms\n",
            "\u001b[32m2021-04-22T00:50:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3150/10000, train/hateful_memes/cross_entropy: 0.0622, train/hateful_memes/cross_entropy/avg: 0.3270, train/total_loss: 0.0622, train/total_loss/avg: 0.3270, max mem: 11667.0, experiment: run, epoch: 12, num_updates: 3150, iterations: 3150, max_updates: 10000, lr: 0.00001, ups: 1.14, time: 44s 637ms, time_since_start: 54m 41s 603ms, eta: 01h 45m 23s 282ms\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-22T00:51:09 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-22T00:51:09 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2021-04-22T00:51:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3200/10000, train/hateful_memes/cross_entropy: 0.0527, train/hateful_memes/cross_entropy/avg: 0.3220, train/total_loss: 0.0527, train/total_loss/avg: 0.3220, max mem: 11667.0, experiment: run, epoch: 13, num_updates: 3200, iterations: 3200, max_updates: 10000, lr: 0.00001, ups: 1.09, time: 46s 610ms, time_since_start: 55m 28s 214ms, eta: 01h 49m 14s 579ms\n",
            "\u001b[32m2021-04-22T00:52:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3250/10000, train/hateful_memes/cross_entropy: 0.0527, train/hateful_memes/cross_entropy/avg: 0.3179, train/total_loss: 0.0527, train/total_loss/avg: 0.3179, max mem: 11667.0, experiment: run, epoch: 13, num_updates: 3250, iterations: 3250, max_updates: 10000, lr: 0.00001, ups: 1.14, time: 44s 683ms, time_since_start: 56m 12s 897ms, eta: 01h 43m 57s 316ms\n",
            "\u001b[32m2021-04-22T00:52:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3300/10000, train/hateful_memes/cross_entropy: 0.0527, train/hateful_memes/cross_entropy/avg: 0.3131, train/total_loss: 0.0527, train/total_loss/avg: 0.3131, max mem: 11667.0, experiment: run, epoch: 13, num_updates: 3300, iterations: 3300, max_updates: 10000, lr: 0.00001, ups: 1.14, time: 44s 623ms, time_since_start: 56m 57s 520ms, eta: 01h 43m 02s 833ms\n",
            "\u001b[32m2021-04-22T00:53:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3350/10000, train/hateful_memes/cross_entropy: 0.0495, train/hateful_memes/cross_entropy/avg: 0.3088, train/total_loss: 0.0495, train/total_loss/avg: 0.3088, max mem: 11667.0, experiment: run, epoch: 13, num_updates: 3350, iterations: 3350, max_updates: 10000, lr: 0.00001, ups: 1.14, time: 44s 644ms, time_since_start: 57m 42s 165ms, eta: 01h 42m 19s 653ms\n",
            "\u001b[32m2021-04-22T00:54:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3400/10000, train/hateful_memes/cross_entropy: 0.0527, train/hateful_memes/cross_entropy/avg: 0.3063, train/total_loss: 0.0527, train/total_loss/avg: 0.3063, max mem: 11667.0, experiment: run, epoch: 13, num_updates: 3400, iterations: 3400, max_updates: 10000, lr: 0.00001, ups: 1.14, time: 44s 605ms, time_since_start: 58m 26s 771ms, eta: 01h 41m 28s 091ms\n",
            "\u001b[32m2021-04-22T00:55:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3450/10000, train/hateful_memes/cross_entropy: 0.0527, train/hateful_memes/cross_entropy/avg: 0.3041, train/total_loss: 0.0527, train/total_loss/avg: 0.3041, max mem: 11667.0, experiment: run, epoch: 13, num_updates: 3450, iterations: 3450, max_updates: 10000, lr: 0.00001, ups: 1.14, time: 44s 666ms, time_since_start: 59m 11s 437ms, eta: 01h 40m 50s 235ms\n",
            "\u001b[32m2021-04-22T00:55:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3500/10000, train/hateful_memes/cross_entropy: 0.0527, train/hateful_memes/cross_entropy/avg: 0.2997, train/total_loss: 0.0527, train/total_loss/avg: 0.2997, max mem: 11667.0, experiment: run, epoch: 14, num_updates: 3500, iterations: 3500, max_updates: 10000, lr: 0.00001, ups: 1.09, time: 46s 591ms, time_since_start: 59m 58s 028ms, eta: 01h 44m 22s 817ms\n",
            "\u001b[32m2021-04-22T00:55:48 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2021-04-22T00:55:48 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[32m2021-04-22T00:55:56 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-04-22T00:56:06 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-04-22T00:56:31 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-04-22T00:56:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3500/10000, val/hateful_memes/cross_entropy: 1.9831, val/total_loss: 1.9831, val/hateful_memes/accuracy: 0.6204, val/hateful_memes/binary_f1: 0.3844, val/hateful_memes/roc_auc: 0.5969, num_updates: 3500, epoch: 14, iterations: 3500, max_updates: 10000, val_time: 43s 427ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.619485\n",
            "\u001b[32m2021-04-22T00:57:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3550/10000, train/hateful_memes/cross_entropy: 0.0527, train/hateful_memes/cross_entropy/avg: 0.2962, train/total_loss: 0.0527, train/total_loss/avg: 0.2962, max mem: 11667.0, experiment: run, epoch: 14, num_updates: 3550, iterations: 3550, max_updates: 10000, lr: 0.00001, ups: 1.11, time: 45s 376ms, time_since_start: 01h 01m 26s 834ms, eta: 01h 40m 52s 548ms\n",
            "\u001b[32m2021-04-22T00:58:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3600/10000, train/hateful_memes/cross_entropy: 0.0495, train/hateful_memes/cross_entropy/avg: 0.2921, train/total_loss: 0.0495, train/total_loss/avg: 0.2921, max mem: 11667.0, experiment: run, epoch: 14, num_updates: 3600, iterations: 3600, max_updates: 10000, lr: 0.00001, ups: 1.14, time: 44s 671ms, time_since_start: 01h 02m 11s 506ms, eta: 01h 38m 32s 372ms\n",
            "\u001b[32m2021-04-22T00:58:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3650/10000, train/hateful_memes/cross_entropy: 0.0457, train/hateful_memes/cross_entropy/avg: 0.2882, train/total_loss: 0.0457, train/total_loss/avg: 0.2882, max mem: 11667.0, experiment: run, epoch: 14, num_updates: 3650, iterations: 3650, max_updates: 10000, lr: 0.00001, ups: 1.14, time: 44s 640ms, time_since_start: 01h 02m 56s 146ms, eta: 01h 37m 42s 094ms\n",
            "\u001b[32m2021-04-22T00:59:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3700/10000, train/hateful_memes/cross_entropy: 0.0379, train/hateful_memes/cross_entropy/avg: 0.2848, train/total_loss: 0.0379, train/total_loss/avg: 0.2848, max mem: 11667.0, experiment: run, epoch: 14, num_updates: 3700, iterations: 3700, max_updates: 10000, lr: 0.00001, ups: 1.14, time: 44s 655ms, time_since_start: 01h 03m 40s 801ms, eta: 01h 36m 57s 840ms\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-22T00:59:52 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-22T00:59:52 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2021-04-22T01:00:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3750/10000, train/hateful_memes/cross_entropy: 0.0351, train/hateful_memes/cross_entropy/avg: 0.2810, train/total_loss: 0.0351, train/total_loss/avg: 0.2810, max mem: 11667.0, experiment: run, epoch: 15, num_updates: 3750, iterations: 3750, max_updates: 10000, lr: 0.00001, ups: 1.09, time: 46s 733ms, time_since_start: 01h 04m 27s 535ms, eta: 01h 40m 40s 283ms\n",
            "\u001b[32m2021-04-22T01:01:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3800/10000, train/hateful_memes/cross_entropy: 0.0211, train/hateful_memes/cross_entropy/avg: 0.2774, train/total_loss: 0.0211, train/total_loss/avg: 0.2774, max mem: 11667.0, experiment: run, epoch: 15, num_updates: 3800, iterations: 3800, max_updates: 10000, lr: 0.00001, ups: 1.14, time: 44s 614ms, time_since_start: 01h 05m 12s 149ms, eta: 01h 35m 20s 299ms\n",
            "\u001b[32m2021-04-22T01:01:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3850/10000, train/hateful_memes/cross_entropy: 0.0103, train/hateful_memes/cross_entropy/avg: 0.2739, train/total_loss: 0.0103, train/total_loss/avg: 0.2739, max mem: 11667.0, experiment: run, epoch: 15, num_updates: 3850, iterations: 3850, max_updates: 10000, lr: 0.00001, ups: 1.14, time: 44s 640ms, time_since_start: 01h 05m 56s 790ms, eta: 01h 34m 37s 503ms\n",
            "\u001b[32m2021-04-22T01:02:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3900/10000, train/hateful_memes/cross_entropy: 0.0103, train/hateful_memes/cross_entropy/avg: 0.2707, train/total_loss: 0.0103, train/total_loss/avg: 0.2707, max mem: 11667.0, experiment: run, epoch: 15, num_updates: 3900, iterations: 3900, max_updates: 10000, lr: 0.00001, ups: 1.14, time: 44s 593ms, time_since_start: 01h 06m 41s 383ms, eta: 01h 33m 45s 348ms\n",
            "\u001b[32m2021-04-22T01:03:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3950/10000, train/hateful_memes/cross_entropy: 0.0103, train/hateful_memes/cross_entropy/avg: 0.2673, train/total_loss: 0.0103, train/total_loss/avg: 0.2673, max mem: 11667.0, experiment: run, epoch: 15, num_updates: 3950, iterations: 3950, max_updates: 10000, lr: 0.00001, ups: 1.14, time: 44s 617ms, time_since_start: 01h 07m 26s 001ms, eta: 01h 33m 02s 225ms\n",
            "\u001b[32m2021-04-22T01:04:03 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[32m2021-04-22T01:04:03 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-04-22T01:04:12 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-04-22T01:04:38 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-04-22T01:04:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4000/10000, train/hateful_memes/cross_entropy: 0.0103, train/hateful_memes/cross_entropy/avg: 0.2648, train/total_loss: 0.0103, train/total_loss/avg: 0.2648, max mem: 11667.0, experiment: run, epoch: 16, num_updates: 4000, iterations: 4000, max_updates: 10000, lr: 0.00001, ups: 0.62, time: 01m 21s 465ms, time_since_start: 01h 08m 47s 466ms, eta: 02h 48m 28s 214ms\n",
            "\u001b[32m2021-04-22T01:04:38 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2021-04-22T01:04:38 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-22T01:04:38 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-22T01:04:38 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2021-04-22T01:04:48 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-04-22T01:05:03 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-04-22T01:05:28 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-04-22T01:05:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4000/10000, val/hateful_memes/cross_entropy: 2.3245, val/total_loss: 2.3245, val/hateful_memes/accuracy: 0.6352, val/hateful_memes/binary_f1: 0.3039, val/hateful_memes/roc_auc: 0.5653, num_updates: 4000, epoch: 16, iterations: 4000, max_updates: 10000, val_time: 50s 141ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.619485\n",
            "\u001b[32m2021-04-22T01:06:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4050/10000, train/hateful_memes/cross_entropy: 0.0103, train/hateful_memes/cross_entropy/avg: 0.2617, train/total_loss: 0.0103, train/total_loss/avg: 0.2617, max mem: 11667.0, experiment: run, epoch: 16, num_updates: 4050, iterations: 4050, max_updates: 10000, lr: 0.00001, ups: 1.11, time: 45s 357ms, time_since_start: 01h 10m 22s 967ms, eta: 01h 33m 01s 048ms\n",
            "\u001b[32m2021-04-22T01:06:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4100/10000, train/hateful_memes/cross_entropy: 0.0103, train/hateful_memes/cross_entropy/avg: 0.2586, train/total_loss: 0.0103, train/total_loss/avg: 0.2586, max mem: 11667.0, experiment: run, epoch: 16, num_updates: 4100, iterations: 4100, max_updates: 10000, lr: 0.00001, ups: 1.14, time: 44s 707ms, time_since_start: 01h 11m 07s 674ms, eta: 01h 30m 54s 848ms\n",
            "\u001b[32m2021-04-22T01:07:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4150/10000, train/hateful_memes/cross_entropy: 0.0103, train/hateful_memes/cross_entropy/avg: 0.2557, train/total_loss: 0.0103, train/total_loss/avg: 0.2557, max mem: 11667.0, experiment: run, epoch: 16, num_updates: 4150, iterations: 4150, max_updates: 10000, lr: 0.00001, ups: 1.14, time: 44s 665ms, time_since_start: 01h 11m 52s 340ms, eta: 01h 30m 03s 550ms\n",
            "\u001b[32m2021-04-22T01:08:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4200/10000, train/hateful_memes/cross_entropy: 0.0103, train/hateful_memes/cross_entropy/avg: 0.2527, train/total_loss: 0.0103, train/total_loss/avg: 0.2527, max mem: 11667.0, experiment: run, epoch: 16, num_updates: 4200, iterations: 4200, max_updates: 10000, lr: 0.00001, ups: 1.14, time: 44s 639ms, time_since_start: 01h 12m 36s 979ms, eta: 01h 29m 14s 224ms\n",
            "\u001b[32m2021-04-22T01:09:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4250/10000, train/hateful_memes/cross_entropy: 0.0103, train/hateful_memes/cross_entropy/avg: 0.2503, train/total_loss: 0.0103, train/total_loss/avg: 0.2503, max mem: 11667.0, experiment: run, epoch: 16, num_updates: 4250, iterations: 4250, max_updates: 10000, lr: 0.00001, ups: 1.14, time: 44s 615ms, time_since_start: 01h 13m 21s 595ms, eta: 01h 28m 25s 229ms\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-22T01:09:17 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-22T01:09:17 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2021-04-22T01:09:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4300/10000, train/hateful_memes/cross_entropy: 0.0126, train/hateful_memes/cross_entropy/avg: 0.2476, train/total_loss: 0.0126, train/total_loss/avg: 0.2476, max mem: 11667.0, experiment: run, epoch: 17, num_updates: 4300, iterations: 4300, max_updates: 10000, lr: 0.00001, ups: 1.09, time: 46s 560ms, time_since_start: 01h 14m 08s 155ms, eta: 01h 31m 28s 342ms\n",
            "\u001b[32m2021-04-22T01:10:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4350/10000, train/hateful_memes/cross_entropy: 0.0126, train/hateful_memes/cross_entropy/avg: 0.2455, train/total_loss: 0.0126, train/total_loss/avg: 0.2455, max mem: 11667.0, experiment: run, epoch: 17, num_updates: 4350, iterations: 4350, max_updates: 10000, lr: 0.00001, ups: 1.14, time: 44s 595ms, time_since_start: 01h 14m 52s 750ms, eta: 01h 26m 50s 584ms\n",
            "\u001b[32m2021-04-22T01:11:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4400/10000, train/hateful_memes/cross_entropy: 0.0103, train/hateful_memes/cross_entropy/avg: 0.2427, train/total_loss: 0.0103, train/total_loss/avg: 0.2427, max mem: 11667.0, experiment: run, epoch: 17, num_updates: 4400, iterations: 4400, max_updates: 10000, lr: 0.00001, ups: 1.14, time: 44s 639ms, time_since_start: 01h 15m 37s 390ms, eta: 01h 26m 09s 660ms\n",
            "\u001b[32m2021-04-22T01:12:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4450/10000, train/hateful_memes/cross_entropy: 0.0093, train/hateful_memes/cross_entropy/avg: 0.2400, train/total_loss: 0.0093, train/total_loss/avg: 0.2400, max mem: 11667.0, experiment: run, epoch: 17, num_updates: 4450, iterations: 4450, max_updates: 10000, lr: 0.00001, ups: 1.14, time: 44s 627ms, time_since_start: 01h 16m 22s 018ms, eta: 01h 25m 22s 108ms\n",
            "\u001b[32m2021-04-22T01:12:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4500/10000, train/hateful_memes/cross_entropy: 0.0093, train/hateful_memes/cross_entropy/avg: 0.2374, train/total_loss: 0.0093, train/total_loss/avg: 0.2374, max mem: 11667.0, experiment: run, epoch: 17, num_updates: 4500, iterations: 4500, max_updates: 10000, lr: 0.00001, ups: 1.14, time: 44s 661ms, time_since_start: 01h 17m 06s 680ms, eta: 01h 24m 39s 845ms\n",
            "\u001b[32m2021-04-22T01:12:57 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2021-04-22T01:12:57 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[32m2021-04-22T01:13:05 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-04-22T01:13:14 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-04-22T01:13:41 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-04-22T01:13:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4500/10000, val/hateful_memes/cross_entropy: 2.2913, val/total_loss: 2.2913, val/hateful_memes/accuracy: 0.6056, val/hateful_memes/binary_f1: 0.3323, val/hateful_memes/roc_auc: 0.5888, num_updates: 4500, epoch: 17, iterations: 4500, max_updates: 10000, val_time: 44s 279ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.619485\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-22T01:14:01 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-22T01:14:01 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2021-04-22T01:14:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4550/10000, train/hateful_memes/cross_entropy: 0.0074, train/hateful_memes/cross_entropy/avg: 0.2348, train/total_loss: 0.0074, train/total_loss/avg: 0.2348, max mem: 11667.0, experiment: run, epoch: 18, num_updates: 4550, iterations: 4550, max_updates: 10000, lr: 0.00001, ups: 1.06, time: 47s 334ms, time_since_start: 01h 18m 38s 295ms, eta: 01h 28m 54s 882ms\n",
            "\u001b[32m2021-04-22T01:15:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4600/10000, train/hateful_memes/cross_entropy: 0.0074, train/hateful_memes/cross_entropy/avg: 0.2322, train/total_loss: 0.0074, train/total_loss/avg: 0.2322, max mem: 11667.0, experiment: run, epoch: 18, num_updates: 4600, iterations: 4600, max_updates: 10000, lr: 0.00001, ups: 1.14, time: 44s 643ms, time_since_start: 01h 19m 22s 939ms, eta: 01h 23m 05s 447ms\n",
            "\u001b[32m2021-04-22T01:15:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4650/10000, train/hateful_memes/cross_entropy: 0.0074, train/hateful_memes/cross_entropy/avg: 0.2297, train/total_loss: 0.0074, train/total_loss/avg: 0.2297, max mem: 11667.0, experiment: run, epoch: 18, num_updates: 4650, iterations: 4650, max_updates: 10000, lr: 0.00001, ups: 1.14, time: 44s 640ms, time_since_start: 01h 20m 07s 579ms, eta: 01h 22m 18s 900ms\n",
            "\u001b[32m2021-04-22T01:16:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4700/10000, train/hateful_memes/cross_entropy: 0.0069, train/hateful_memes/cross_entropy/avg: 0.2273, train/total_loss: 0.0069, train/total_loss/avg: 0.2273, max mem: 11667.0, experiment: run, epoch: 18, num_updates: 4700, iterations: 4700, max_updates: 10000, lr: 0.00001, ups: 1.14, time: 44s 653ms, time_since_start: 01h 20m 52s 233ms, eta: 01h 21m 34s 242ms\n",
            "\u001b[32m2021-04-22T01:17:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4750/10000, train/hateful_memes/cross_entropy: 0.0069, train/hateful_memes/cross_entropy/avg: 0.2249, train/total_loss: 0.0069, train/total_loss/avg: 0.2249, max mem: 11667.0, experiment: run, epoch: 18, num_updates: 4750, iterations: 4750, max_updates: 10000, lr: 0.00001, ups: 1.14, time: 44s 649ms, time_since_start: 01h 21m 36s 883ms, eta: 01h 20m 47s 601ms\n",
            "\u001b[32m2021-04-22T01:18:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4800/10000, train/hateful_memes/cross_entropy: 0.0037, train/hateful_memes/cross_entropy/avg: 0.2226, train/total_loss: 0.0037, train/total_loss/avg: 0.2226, max mem: 11667.0, experiment: run, epoch: 19, num_updates: 4800, iterations: 4800, max_updates: 10000, lr: 0.00001, ups: 1.09, time: 46s 598ms, time_since_start: 01h 22m 23s 482ms, eta: 01h 23m 31s 040ms\n",
            "\u001b[32m2021-04-22T01:18:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4850/10000, train/hateful_memes/cross_entropy: 0.0020, train/hateful_memes/cross_entropy/avg: 0.2203, train/total_loss: 0.0020, train/total_loss/avg: 0.2203, max mem: 11667.0, experiment: run, epoch: 19, num_updates: 4850, iterations: 4850, max_updates: 10000, lr: 0.00001, ups: 1.14, time: 44s 637ms, time_since_start: 01h 23m 08s 119ms, eta: 01h 19m 14s 020ms\n",
            "\u001b[32m2021-04-22T01:19:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4900/10000, train/hateful_memes/cross_entropy: 0.0019, train/hateful_memes/cross_entropy/avg: 0.2181, train/total_loss: 0.0019, train/total_loss/avg: 0.2181, max mem: 11667.0, experiment: run, epoch: 19, num_updates: 4900, iterations: 4900, max_updates: 10000, lr: 0.00001, ups: 1.14, time: 44s 656ms, time_since_start: 01h 23m 52s 776ms, eta: 01h 18m 29s 853ms\n",
            "\u001b[32m2021-04-22T01:20:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4950/10000, train/hateful_memes/cross_entropy: 0.0019, train/hateful_memes/cross_entropy/avg: 0.2159, train/total_loss: 0.0019, train/total_loss/avg: 0.2159, max mem: 11667.0, experiment: run, epoch: 19, num_updates: 4950, iterations: 4950, max_updates: 10000, lr: 0.00001, ups: 1.14, time: 44s 604ms, time_since_start: 01h 24m 37s 380ms, eta: 01h 17m 38s 184ms\n",
            "\u001b[32m2021-04-22T01:21:12 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[32m2021-04-22T01:21:12 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-04-22T01:21:22 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-04-22T01:21:47 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-04-22T01:21:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5000/10000, train/hateful_memes/cross_entropy: 0.0018, train/hateful_memes/cross_entropy/avg: 0.2138, train/total_loss: 0.0018, train/total_loss/avg: 0.2138, max mem: 11667.0, experiment: run, epoch: 19, num_updates: 5000, iterations: 5000, max_updates: 10000, lr: 0.00001, ups: 0.63, time: 01m 19s 965ms, time_since_start: 01h 25m 57s 346ms, eta: 02h 17m 48s 473ms\n",
            "\u001b[32m2021-04-22T01:21:47 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2021-04-22T01:21:50 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-22T01:21:50 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-22T01:21:50 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2021-04-22T01:21:58 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-04-22T01:22:12 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-04-22T01:22:37 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-04-22T01:22:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5000/10000, val/hateful_memes/cross_entropy: 2.3766, val/total_loss: 2.3766, val/hateful_memes/accuracy: 0.6167, val/hateful_memes/binary_f1: 0.3551, val/hateful_memes/roc_auc: 0.5990, num_updates: 5000, epoch: 19, iterations: 5000, max_updates: 10000, val_time: 50s 201ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.619485\n",
            "\u001b[32m2021-04-22T01:23:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5050/10000, train/hateful_memes/cross_entropy: 0.0018, train/hateful_memes/cross_entropy/avg: 0.2129, train/total_loss: 0.0018, train/total_loss/avg: 0.2129, max mem: 11667.0, experiment: run, epoch: 19, num_updates: 5050, iterations: 5050, max_updates: 10000, lr: 0.00001, ups: 1.11, time: 45s 328ms, time_since_start: 01h 27m 32s 878ms, eta: 01h 17m 20s 055ms\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-22T01:23:26 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-22T01:23:26 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2021-04-22T01:24:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5100/10000, train/hateful_memes/cross_entropy: 0.0016, train/hateful_memes/cross_entropy/avg: 0.2108, train/total_loss: 0.0016, train/total_loss/avg: 0.2108, max mem: 11667.0, experiment: run, epoch: 20, num_updates: 5100, iterations: 5100, max_updates: 10000, lr: 0.00001, ups: 1.09, time: 46s 610ms, time_since_start: 01h 28m 19s 488ms, eta: 01h 18m 43s 108ms\n",
            "\u001b[32m2021-04-22T01:24:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5150/10000, train/hateful_memes/cross_entropy: 0.0014, train/hateful_memes/cross_entropy/avg: 0.2088, train/total_loss: 0.0014, train/total_loss/avg: 0.2088, max mem: 11667.0, experiment: run, epoch: 20, num_updates: 5150, iterations: 5150, max_updates: 10000, lr: 0.00001, ups: 1.14, time: 44s 635ms, time_since_start: 01h 29m 04s 124ms, eta: 01h 14m 36s 868ms\n",
            "\u001b[32m2021-04-22T01:25:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5200/10000, train/hateful_memes/cross_entropy: 0.0014, train/hateful_memes/cross_entropy/avg: 0.2069, train/total_loss: 0.0014, train/total_loss/avg: 0.2069, max mem: 11667.0, experiment: run, epoch: 20, num_updates: 5200, iterations: 5200, max_updates: 10000, lr: 0.00001, ups: 1.14, time: 44s 623ms, time_since_start: 01h 29m 48s 747ms, eta: 01h 13m 49s 471ms\n",
            "\u001b[32m2021-04-22T01:26:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5250/10000, train/hateful_memes/cross_entropy: 0.0014, train/hateful_memes/cross_entropy/avg: 0.2049, train/total_loss: 0.0014, train/total_loss/avg: 0.2049, max mem: 11667.0, experiment: run, epoch: 20, num_updates: 5250, iterations: 5250, max_updates: 10000, lr: 0.00001, ups: 1.14, time: 44s 639ms, time_since_start: 01h 30m 33s 387ms, eta: 01h 13m 04s 968ms\n",
            "\u001b[32m2021-04-22T01:27:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5300/10000, train/hateful_memes/cross_entropy: 0.0014, train/hateful_memes/cross_entropy/avg: 0.2030, train/total_loss: 0.0014, train/total_loss/avg: 0.2030, max mem: 11667.0, experiment: run, epoch: 20, num_updates: 5300, iterations: 5300, max_updates: 10000, lr: 0.00001, ups: 1.14, time: 44s 626ms, time_since_start: 01h 31m 18s 013ms, eta: 01h 12m 17s 500ms\n",
            "\u001b[32m2021-04-22T01:27:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5350/10000, train/hateful_memes/cross_entropy: 0.0014, train/hateful_memes/cross_entropy/avg: 0.2011, train/total_loss: 0.0014, train/total_loss/avg: 0.2011, max mem: 11667.0, experiment: run, epoch: 21, num_updates: 5350, iterations: 5350, max_updates: 10000, lr: 0.00001, ups: 1.09, time: 46s 626ms, time_since_start: 01h 32m 04s 640ms, eta: 01h 14m 43s 715ms\n",
            "\u001b[32m2021-04-22T01:28:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5400/10000, train/hateful_memes/cross_entropy: 0.0014, train/hateful_memes/cross_entropy/avg: 0.1993, train/total_loss: 0.0014, train/total_loss/avg: 0.1993, max mem: 11667.0, experiment: run, epoch: 21, num_updates: 5400, iterations: 5400, max_updates: 10000, lr: 0.00001, ups: 1.14, time: 44s 600ms, time_since_start: 01h 32m 49s 240ms, eta: 01h 10m 42s 763ms\n",
            "\u001b[32m2021-04-22T01:29:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5450/10000, train/hateful_memes/cross_entropy: 0.0014, train/hateful_memes/cross_entropy/avg: 0.1975, train/total_loss: 0.0014, train/total_loss/avg: 0.1975, max mem: 11667.0, experiment: run, epoch: 21, num_updates: 5450, iterations: 5450, max_updates: 10000, lr: 0.00001, ups: 1.14, time: 44s 601ms, time_since_start: 01h 33m 33s 842ms, eta: 01h 09m 56s 772ms\n",
            "\u001b[32m2021-04-22T01:30:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5500/10000, train/hateful_memes/cross_entropy: 0.0014, train/hateful_memes/cross_entropy/avg: 0.1964, train/total_loss: 0.0014, train/total_loss/avg: 0.1964, max mem: 11667.0, experiment: run, epoch: 21, num_updates: 5500, iterations: 5500, max_updates: 10000, lr: 0.00001, ups: 1.14, time: 44s 629ms, time_since_start: 01h 34m 18s 472ms, eta: 01h 09m 13s 206ms\n",
            "\u001b[32m2021-04-22T01:30:09 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2021-04-22T01:30:09 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[32m2021-04-22T01:30:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5500/10000, val/hateful_memes/cross_entropy: 2.5704, val/total_loss: 2.5704, val/hateful_memes/accuracy: 0.6111, val/hateful_memes/binary_f1: 0.3558, val/hateful_memes/roc_auc: 0.6124, num_updates: 5500, epoch: 21, iterations: 5500, max_updates: 10000, val_time: 08s 205ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.619485\n",
            "\u001b[32m2021-04-22T01:31:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5550/10000, train/hateful_memes/cross_entropy: 0.0014, train/hateful_memes/cross_entropy/avg: 0.1947, train/total_loss: 0.0014, train/total_loss/avg: 0.1947, max mem: 11667.0, experiment: run, epoch: 21, num_updates: 5550, iterations: 5550, max_updates: 10000, lr: 0.00001, ups: 1.11, time: 45s 177ms, time_since_start: 01h 35m 11s 856ms, eta: 01h 09m 17s 518ms\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-22T01:31:34 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-22T01:31:34 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2021-04-22T01:31:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5600/10000, train/hateful_memes/cross_entropy: 0.0018, train/hateful_memes/cross_entropy/avg: 0.1930, train/total_loss: 0.0018, train/total_loss/avg: 0.1930, max mem: 11667.0, experiment: run, epoch: 22, num_updates: 5600, iterations: 5600, max_updates: 10000, lr: 0.00001, ups: 1.09, time: 46s 429ms, time_since_start: 01h 35m 58s 286ms, eta: 01h 10m 24s 746ms\n",
            "\u001b[32m2021-04-22T01:32:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5650/10000, train/hateful_memes/cross_entropy: 0.0019, train/hateful_memes/cross_entropy/avg: 0.1914, train/total_loss: 0.0019, train/total_loss/avg: 0.1914, max mem: 11667.0, experiment: run, epoch: 22, num_updates: 5650, iterations: 5650, max_updates: 10000, lr: 0.00001, ups: 1.14, time: 44s 620ms, time_since_start: 01h 36m 42s 906ms, eta: 01h 06m 53s 936ms\n",
            "\u001b[32m2021-04-22T01:33:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5700/10000, train/hateful_memes/cross_entropy: 0.0018, train/hateful_memes/cross_entropy/avg: 0.1897, train/total_loss: 0.0018, train/total_loss/avg: 0.1897, max mem: 11667.0, experiment: run, epoch: 22, num_updates: 5700, iterations: 5700, max_updates: 10000, lr: 0.00001, ups: 1.14, time: 44s 636ms, time_since_start: 01h 37m 27s 543ms, eta: 01h 06m 09s 292ms\n",
            "\u001b[32m2021-04-22T01:34:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5750/10000, train/hateful_memes/cross_entropy: 0.0014, train/hateful_memes/cross_entropy/avg: 0.1881, train/total_loss: 0.0014, train/total_loss/avg: 0.1881, max mem: 11667.0, experiment: run, epoch: 22, num_updates: 5750, iterations: 5750, max_updates: 10000, lr: 0.00001, ups: 1.14, time: 44s 616ms, time_since_start: 01h 38m 12s 159ms, eta: 01h 05m 21s 331ms\n",
            "\u001b[32m2021-04-22T01:34:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5800/10000, train/hateful_memes/cross_entropy: 0.0014, train/hateful_memes/cross_entropy/avg: 0.1865, train/total_loss: 0.0014, train/total_loss/avg: 0.1865, max mem: 11667.0, experiment: run, epoch: 22, num_updates: 5800, iterations: 5800, max_updates: 10000, lr: 0.00001, ups: 1.14, time: 44s 629ms, time_since_start: 01h 38m 56s 789ms, eta: 01h 04m 36s 361ms\n",
            "\u001b[32m2021-04-22T01:35:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5850/10000, train/hateful_memes/cross_entropy: 0.0015, train/hateful_memes/cross_entropy/avg: 0.1849, train/total_loss: 0.0015, train/total_loss/avg: 0.1849, max mem: 11667.0, experiment: run, epoch: 22, num_updates: 5850, iterations: 5850, max_updates: 10000, lr: 0.00001, ups: 1.14, time: 44s 602ms, time_since_start: 01h 39m 41s 391ms, eta: 01h 03m 47s 857ms\n",
            "\u001b[32m2021-04-22T01:36:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5900/10000, train/hateful_memes/cross_entropy: 0.0012, train/hateful_memes/cross_entropy/avg: 0.1834, train/total_loss: 0.0012, train/total_loss/avg: 0.1834, max mem: 11667.0, experiment: run, epoch: 23, num_updates: 5900, iterations: 5900, max_updates: 10000, lr: 0.00001, ups: 1.09, time: 46s 821ms, time_since_start: 01h 40m 28s 213ms, eta: 01h 06m 09s 897ms\n",
            "\u001b[32m2021-04-22T01:37:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5950/10000, train/hateful_memes/cross_entropy: 0.0014, train/hateful_memes/cross_entropy/avg: 0.1818, train/total_loss: 0.0014, train/total_loss/avg: 0.1818, max mem: 11667.0, experiment: run, epoch: 23, num_updates: 5950, iterations: 5950, max_updates: 10000, lr: 0.00001, ups: 1.14, time: 44s 593ms, time_since_start: 01h 41m 12s 807ms, eta: 01h 02m 14s 898ms\n",
            "\u001b[32m2021-04-22T01:37:47 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[32m2021-04-22T01:37:47 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-04-22T01:37:57 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-04-22T01:38:23 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-04-22T01:38:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6000/10000, train/hateful_memes/cross_entropy: 0.0014, train/hateful_memes/cross_entropy/avg: 0.1803, train/total_loss: 0.0014, train/total_loss/avg: 0.1803, max mem: 11667.0, experiment: run, epoch: 23, num_updates: 6000, iterations: 6000, max_updates: 10000, lr: 0.00001, ups: 0.63, time: 01m 19s 749ms, time_since_start: 01h 42m 32s 556ms, eta: 01h 49m 56s 855ms\n",
            "\u001b[32m2021-04-22T01:38:23 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2021-04-22T01:38:23 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-22T01:38:23 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-22T01:38:23 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2021-04-22T01:38:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6000/10000, val/hateful_memes/cross_entropy: 2.3944, val/total_loss: 2.3944, val/hateful_memes/accuracy: 0.6111, val/hateful_memes/binary_f1: 0.3636, val/hateful_memes/roc_auc: 0.6076, num_updates: 6000, epoch: 23, iterations: 6000, max_updates: 10000, val_time: 09s 122ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.619485\n",
            "\u001b[32m2021-04-22T01:39:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6050/10000, train/hateful_memes/cross_entropy: 0.0014, train/hateful_memes/cross_entropy/avg: 0.1789, train/total_loss: 0.0014, train/total_loss/avg: 0.1789, max mem: 11667.0, experiment: run, epoch: 23, num_updates: 6050, iterations: 6050, max_updates: 10000, lr: 0., ups: 1.11, time: 45s 192ms, time_since_start: 01h 43m 26s 873ms, eta: 01h 01m 31s 582ms\n",
            "\u001b[32m2021-04-22T01:40:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6100/10000, train/hateful_memes/cross_entropy: 0.0015, train/hateful_memes/cross_entropy/avg: 0.1774, train/total_loss: 0.0015, train/total_loss/avg: 0.1774, max mem: 11667.0, experiment: run, epoch: 23, num_updates: 6100, iterations: 6100, max_updates: 10000, lr: 0., ups: 1.14, time: 44s 616ms, time_since_start: 01h 44m 11s 490ms, eta: 59m 58s 442ms\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-22T01:40:17 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-22T01:40:17 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2021-04-22T01:40:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6150/10000, train/hateful_memes/cross_entropy: 0.0015, train/hateful_memes/cross_entropy/avg: 0.1760, train/total_loss: 0.0015, train/total_loss/avg: 0.1760, max mem: 11667.0, experiment: run, epoch: 24, num_updates: 6150, iterations: 6150, max_updates: 10000, lr: 0., ups: 1.09, time: 46s 719ms, time_since_start: 01h 44m 58s 210ms, eta: 01h 01m 59s 745ms\n",
            "\u001b[32m2021-04-22T01:41:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6200/10000, train/hateful_memes/cross_entropy: 0.0015, train/hateful_memes/cross_entropy/avg: 0.1746, train/total_loss: 0.0015, train/total_loss/avg: 0.1746, max mem: 11667.0, experiment: run, epoch: 24, num_updates: 6200, iterations: 6200, max_updates: 10000, lr: 0., ups: 1.14, time: 44s 616ms, time_since_start: 01h 45m 42s 826ms, eta: 58m 26s 119ms\n",
            "\u001b[32m2021-04-22T01:42:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6250/10000, train/hateful_memes/cross_entropy: 0.0014, train/hateful_memes/cross_entropy/avg: 0.1732, train/total_loss: 0.0014, train/total_loss/avg: 0.1732, max mem: 11667.0, experiment: run, epoch: 24, num_updates: 6250, iterations: 6250, max_updates: 10000, lr: 0., ups: 1.14, time: 44s 624ms, time_since_start: 01h 46m 27s 450ms, eta: 57m 40s 598ms\n",
            "\u001b[32m2021-04-22T01:43:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6300/10000, train/hateful_memes/cross_entropy: 0.0014, train/hateful_memes/cross_entropy/avg: 0.1718, train/total_loss: 0.0014, train/total_loss/avg: 0.1718, max mem: 11667.0, experiment: run, epoch: 24, num_updates: 6300, iterations: 6300, max_updates: 10000, lr: 0., ups: 1.14, time: 44s 614ms, time_since_start: 01h 47m 12s 065ms, eta: 56m 53s 724ms\n",
            "\u001b[32m2021-04-22T01:43:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6350/10000, train/hateful_memes/cross_entropy: 0.0014, train/hateful_memes/cross_entropy/avg: 0.1705, train/total_loss: 0.0014, train/total_loss/avg: 0.1705, max mem: 11667.0, experiment: run, epoch: 24, num_updates: 6350, iterations: 6350, max_updates: 10000, lr: 0., ups: 1.14, time: 44s 602ms, time_since_start: 01h 47m 56s 668ms, eta: 56m 06s 719ms\n",
            "\u001b[32m2021-04-22T01:44:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6400/10000, train/hateful_memes/cross_entropy: 0.0014, train/hateful_memes/cross_entropy/avg: 0.1692, train/total_loss: 0.0014, train/total_loss/avg: 0.1692, max mem: 11667.0, experiment: run, epoch: 25, num_updates: 6400, iterations: 6400, max_updates: 10000, lr: 0., ups: 1.09, time: 46s 741ms, time_since_start: 01h 48m 43s 409ms, eta: 57m 59s 808ms\n",
            "\u001b[32m2021-04-22T01:45:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6450/10000, train/hateful_memes/cross_entropy: 0.0014, train/hateful_memes/cross_entropy/avg: 0.1679, train/total_loss: 0.0014, train/total_loss/avg: 0.1679, max mem: 11667.0, experiment: run, epoch: 25, num_updates: 6450, iterations: 6450, max_updates: 10000, lr: 0., ups: 1.14, time: 44s 605ms, time_since_start: 01h 49m 28s 015ms, eta: 54m 34s 662ms\n",
            "\u001b[32m2021-04-22T01:46:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6500/10000, train/hateful_memes/cross_entropy: 0.0014, train/hateful_memes/cross_entropy/avg: 0.1666, train/total_loss: 0.0014, train/total_loss/avg: 0.1666, max mem: 11667.0, experiment: run, epoch: 25, num_updates: 6500, iterations: 6500, max_updates: 10000, lr: 0., ups: 1.14, time: 44s 629ms, time_since_start: 01h 50m 12s 644ms, eta: 53m 50s 272ms\n",
            "\u001b[32m2021-04-22T01:46:03 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2021-04-22T01:46:03 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[32m2021-04-22T01:46:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6500/10000, val/hateful_memes/cross_entropy: 2.6707, val/total_loss: 2.6707, val/hateful_memes/accuracy: 0.6167, val/hateful_memes/binary_f1: 0.3077, val/hateful_memes/roc_auc: 0.6053, num_updates: 6500, epoch: 25, iterations: 6500, max_updates: 10000, val_time: 08s 142ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.619485\n",
            "\u001b[32m2021-04-22T01:46:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6550/10000, train/hateful_memes/cross_entropy: 0.0014, train/hateful_memes/cross_entropy/avg: 0.1654, train/total_loss: 0.0014, train/total_loss/avg: 0.1654, max mem: 11667.0, experiment: run, epoch: 25, num_updates: 6550, iterations: 6550, max_updates: 10000, lr: 0., ups: 1.11, time: 45s 173ms, time_since_start: 01h 51m 05s 961ms, eta: 53m 42s 934ms\n",
            "\u001b[32m2021-04-22T01:47:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6600/10000, train/hateful_memes/cross_entropy: 0.0014, train/hateful_memes/cross_entropy/avg: 0.1641, train/total_loss: 0.0014, train/total_loss/avg: 0.1641, max mem: 11667.0, experiment: run, epoch: 25, num_updates: 6600, iterations: 6600, max_updates: 10000, lr: 0., ups: 1.14, time: 44s 613ms, time_since_start: 01h 51m 50s 575ms, eta: 52m 16s 863ms\n",
            "\u001b[32m2021-04-22T01:48:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6650/10000, train/hateful_memes/cross_entropy: 0.0014, train/hateful_memes/cross_entropy/avg: 0.1636, train/total_loss: 0.0014, train/total_loss/avg: 0.1636, max mem: 11667.0, experiment: run, epoch: 25, num_updates: 6650, iterations: 6650, max_updates: 10000, lr: 0., ups: 1.14, time: 44s 340ms, time_since_start: 01h 52m 34s 916ms, eta: 51m 11s 839ms\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-22T01:48:25 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-22T01:48:25 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2021-04-22T01:49:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6700/10000, train/hateful_memes/cross_entropy: 0.0015, train/hateful_memes/cross_entropy/avg: 0.1625, train/total_loss: 0.0015, train/total_loss/avg: 0.1625, max mem: 11667.0, experiment: run, epoch: 26, num_updates: 6700, iterations: 6700, max_updates: 10000, lr: 0., ups: 1.06, time: 47s 002ms, time_since_start: 01h 53m 21s 918ms, eta: 53m 27s 646ms\n",
            "\u001b[32m2021-04-22T01:49:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6750/10000, train/hateful_memes/cross_entropy: 0.0015, train/hateful_memes/cross_entropy/avg: 0.1613, train/total_loss: 0.0015, train/total_loss/avg: 0.1613, max mem: 11667.0, experiment: run, epoch: 26, num_updates: 6750, iterations: 6750, max_updates: 10000, lr: 0., ups: 1.14, time: 44s 633ms, time_since_start: 01h 54m 06s 551ms, eta: 49m 59s 790ms\n",
            "\u001b[32m2021-04-22T01:50:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6800/10000, train/hateful_memes/cross_entropy: 0.0015, train/hateful_memes/cross_entropy/avg: 0.1601, train/total_loss: 0.0015, train/total_loss/avg: 0.1601, max mem: 11667.0, experiment: run, epoch: 26, num_updates: 6800, iterations: 6800, max_updates: 10000, lr: 0., ups: 1.14, time: 44s 633ms, time_since_start: 01h 54m 51s 185ms, eta: 49m 13s 651ms\n",
            "\u001b[32m2021-04-22T01:51:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6850/10000, train/hateful_memes/cross_entropy: 0.0015, train/hateful_memes/cross_entropy/avg: 0.1590, train/total_loss: 0.0015, train/total_loss/avg: 0.1590, max mem: 11667.0, experiment: run, epoch: 26, num_updates: 6850, iterations: 6850, max_updates: 10000, lr: 0., ups: 1.14, time: 44s 625ms, time_since_start: 01h 55m 35s 810ms, eta: 48m 26s 977ms\n",
            "\u001b[32m2021-04-22T01:52:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6900/10000, train/hateful_memes/cross_entropy: 0.0015, train/hateful_memes/cross_entropy/avg: 0.1578, train/total_loss: 0.0015, train/total_loss/avg: 0.1578, max mem: 11667.0, experiment: run, epoch: 26, num_updates: 6900, iterations: 6900, max_updates: 10000, lr: 0., ups: 1.14, time: 44s 643ms, time_since_start: 01h 56m 20s 454ms, eta: 47m 42s 025ms\n",
            "\u001b[32m2021-04-22T01:52:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6950/10000, train/hateful_memes/cross_entropy: 0.0020, train/hateful_memes/cross_entropy/avg: 0.1567, train/total_loss: 0.0020, train/total_loss/avg: 0.1567, max mem: 11667.0, experiment: run, epoch: 27, num_updates: 6950, iterations: 6950, max_updates: 10000, lr: 0., ups: 1.09, time: 46s 617ms, time_since_start: 01h 57m 07s 071ms, eta: 49m 333ms\n",
            "\u001b[32m2021-04-22T01:53:42 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[32m2021-04-22T01:53:42 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-04-22T01:53:51 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-04-22T01:54:17 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-04-22T01:54:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7000/10000, train/hateful_memes/cross_entropy: 0.0021, train/hateful_memes/cross_entropy/avg: 0.1556, train/total_loss: 0.0021, train/total_loss/avg: 0.1556, max mem: 11667.0, experiment: run, epoch: 27, num_updates: 7000, iterations: 7000, max_updates: 10000, lr: 0., ups: 0.63, time: 01m 19s 763ms, time_since_start: 01h 58m 26s 834ms, eta: 01h 22m 28s 504ms\n",
            "\u001b[32m2021-04-22T01:54:17 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2021-04-22T01:54:17 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-22T01:54:17 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-22T01:54:17 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2021-04-22T01:54:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7000/10000, val/hateful_memes/cross_entropy: 3.0067, val/total_loss: 3.0067, val/hateful_memes/accuracy: 0.6352, val/hateful_memes/binary_f1: 0.2622, val/hateful_memes/roc_auc: 0.5888, num_updates: 7000, epoch: 27, iterations: 7000, max_updates: 10000, val_time: 08s 159ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.619485\n",
            "\u001b[32m2021-04-22T01:55:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7050/10000, train/hateful_memes/cross_entropy: 0.0021, train/hateful_memes/cross_entropy/avg: 0.1545, train/total_loss: 0.0021, train/total_loss/avg: 0.1545, max mem: 11667.0, experiment: run, epoch: 27, num_updates: 7050, iterations: 7050, max_updates: 10000, lr: 0., ups: 1.11, time: 45s 179ms, time_since_start: 01h 59m 20s 174ms, eta: 45m 56s 192ms\n",
            "\u001b[32m2021-04-22T01:55:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7100/10000, train/hateful_memes/cross_entropy: 0.0020, train/hateful_memes/cross_entropy/avg: 0.1534, train/total_loss: 0.0020, train/total_loss/avg: 0.1534, max mem: 11667.0, experiment: run, epoch: 27, num_updates: 7100, iterations: 7100, max_updates: 10000, lr: 0., ups: 1.14, time: 44s 591ms, time_since_start: 02h 04s 765ms, eta: 44m 34s 236ms\n",
            "\u001b[32m2021-04-22T01:56:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7150/10000, train/hateful_memes/cross_entropy: 0.0015, train/hateful_memes/cross_entropy/avg: 0.1524, train/total_loss: 0.0015, train/total_loss/avg: 0.1524, max mem: 11667.0, experiment: run, epoch: 27, num_updates: 7150, iterations: 7150, max_updates: 10000, lr: 0., ups: 1.14, time: 44s 606ms, time_since_start: 02h 49s 372ms, eta: 43m 49s 021ms\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-22T01:57:08 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-22T01:57:08 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2021-04-22T01:57:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7200/10000, train/hateful_memes/cross_entropy: 0.0020, train/hateful_memes/cross_entropy/avg: 0.1514, train/total_loss: 0.0020, train/total_loss/avg: 0.1514, max mem: 11667.0, experiment: run, epoch: 28, num_updates: 7200, iterations: 7200, max_updates: 10000, lr: 0., ups: 1.09, time: 46s 599ms, time_since_start: 02h 01m 35s 972ms, eta: 44m 58s 298ms\n",
            "\u001b[32m2021-04-22T01:58:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7250/10000, train/hateful_memes/cross_entropy: 0.0020, train/hateful_memes/cross_entropy/avg: 0.1503, train/total_loss: 0.0020, train/total_loss/avg: 0.1503, max mem: 11667.0, experiment: run, epoch: 28, num_updates: 7250, iterations: 7250, max_updates: 10000, lr: 0., ups: 1.14, time: 44s 612ms, time_since_start: 02h 02m 20s 585ms, eta: 42m 17s 140ms\n",
            "\u001b[32m2021-04-22T01:58:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7300/10000, train/hateful_memes/cross_entropy: 0.0020, train/hateful_memes/cross_entropy/avg: 0.1493, train/total_loss: 0.0020, train/total_loss/avg: 0.1493, max mem: 11667.0, experiment: run, epoch: 28, num_updates: 7300, iterations: 7300, max_updates: 10000, lr: 0., ups: 1.14, time: 44s 599ms, time_since_start: 02h 03m 05s 184ms, eta: 41m 30s 259ms\n",
            "\u001b[32m2021-04-22T01:59:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7350/10000, train/hateful_memes/cross_entropy: 0.0021, train/hateful_memes/cross_entropy/avg: 0.1485, train/total_loss: 0.0021, train/total_loss/avg: 0.1485, max mem: 11667.0, experiment: run, epoch: 28, num_updates: 7350, iterations: 7350, max_updates: 10000, lr: 0., ups: 1.14, time: 44s 593ms, time_since_start: 02h 03m 49s 778ms, eta: 40m 43s 808ms\n",
            "\u001b[32m2021-04-22T02:00:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7400/10000, train/hateful_memes/cross_entropy: 0.0020, train/hateful_memes/cross_entropy/avg: 0.1475, train/total_loss: 0.0020, train/total_loss/avg: 0.1475, max mem: 11667.0, experiment: run, epoch: 28, num_updates: 7400, iterations: 7400, max_updates: 10000, lr: 0., ups: 1.14, time: 44s 610ms, time_since_start: 02h 04m 34s 388ms, eta: 39m 58s 601ms\n",
            "\u001b[32m2021-04-22T02:01:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7450/10000, train/hateful_memes/cross_entropy: 0.0020, train/hateful_memes/cross_entropy/avg: 0.1465, train/total_loss: 0.0020, train/total_loss/avg: 0.1465, max mem: 11667.0, experiment: run, epoch: 29, num_updates: 7450, iterations: 7450, max_updates: 10000, lr: 0., ups: 1.09, time: 46s 481ms, time_since_start: 02h 05m 20s 869ms, eta: 40m 51s 147ms\n",
            "\u001b[32m2021-04-22T02:01:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7500/10000, train/hateful_memes/cross_entropy: 0.0020, train/hateful_memes/cross_entropy/avg: 0.1456, train/total_loss: 0.0020, train/total_loss/avg: 0.1456, max mem: 11667.0, experiment: run, epoch: 29, num_updates: 7500, iterations: 7500, max_updates: 10000, lr: 0., ups: 1.14, time: 44s 770ms, time_since_start: 02h 06m 05s 640ms, eta: 38m 34s 641ms\n",
            "\u001b[32m2021-04-22T02:01:56 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2021-04-22T02:01:56 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[32m2021-04-22T02:02:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7500/10000, val/hateful_memes/cross_entropy: 3.1026, val/total_loss: 3.1026, val/hateful_memes/accuracy: 0.6074, val/hateful_memes/binary_f1: 0.3837, val/hateful_memes/roc_auc: 0.5963, num_updates: 7500, epoch: 29, iterations: 7500, max_updates: 10000, val_time: 08s 147ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.619485\n",
            "\u001b[32m2021-04-22T02:02:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7550/10000, train/hateful_memes/cross_entropy: 0.0015, train/hateful_memes/cross_entropy/avg: 0.1446, train/total_loss: 0.0015, train/total_loss/avg: 0.1446, max mem: 11667.0, experiment: run, epoch: 29, num_updates: 7550, iterations: 7550, max_updates: 10000, lr: 0., ups: 1.11, time: 45s 187ms, time_since_start: 02h 06m 58s 976ms, eta: 38m 09s 453ms\n",
            "\u001b[32m2021-04-22T02:03:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7600/10000, train/hateful_memes/cross_entropy: 0.0009, train/hateful_memes/cross_entropy/avg: 0.1437, train/total_loss: 0.0009, train/total_loss/avg: 0.1437, max mem: 11667.0, experiment: run, epoch: 29, num_updates: 7600, iterations: 7600, max_updates: 10000, lr: 0., ups: 1.14, time: 44s 623ms, time_since_start: 02h 07m 43s 599ms, eta: 36m 54s 739ms\n",
            "\u001b[32m2021-04-22T02:04:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7650/10000, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.1427, train/total_loss: 0.0007, train/total_loss/avg: 0.1427, max mem: 11667.0, experiment: run, epoch: 29, num_updates: 7650, iterations: 7650, max_updates: 10000, lr: 0., ups: 1.14, time: 44s 678ms, time_since_start: 02h 08m 28s 278ms, eta: 36m 11s 285ms\n",
            "\u001b[32m2021-04-22T02:05:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7700/10000, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.1418, train/total_loss: 0.0006, train/total_loss/avg: 0.1418, max mem: 11667.0, experiment: run, epoch: 29, num_updates: 7700, iterations: 7700, max_updates: 10000, lr: 0., ups: 1.14, time: 44s 623ms, time_since_start: 02h 09m 12s 901ms, eta: 35m 22s 455ms\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-22T02:05:15 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-22T02:05:15 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2021-04-22T02:05:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7750/10000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.1409, train/total_loss: 0.0004, train/total_loss/avg: 0.1409, max mem: 11667.0, experiment: run, epoch: 30, num_updates: 7750, iterations: 7750, max_updates: 10000, lr: 0., ups: 1.09, time: 46s 757ms, time_since_start: 02h 09m 59s 659ms, eta: 36m 15s 645ms\n",
            "\u001b[32m2021-04-22T02:06:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7800/10000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.1400, train/total_loss: 0.0004, train/total_loss/avg: 0.1400, max mem: 11667.0, experiment: run, epoch: 30, num_updates: 7800, iterations: 7800, max_updates: 10000, lr: 0., ups: 1.14, time: 44s 619ms, time_since_start: 02h 10m 44s 278ms, eta: 33m 50s 012ms\n",
            "\u001b[32m2021-04-22T02:07:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7850/10000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.1391, train/total_loss: 0.0004, train/total_loss/avg: 0.1391, max mem: 11667.0, experiment: run, epoch: 30, num_updates: 7850, iterations: 7850, max_updates: 10000, lr: 0., ups: 1.14, time: 44s 613ms, time_since_start: 02h 11m 28s 891ms, eta: 33m 03s 585ms\n",
            "\u001b[32m2021-04-22T02:08:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7900/10000, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.1382, train/total_loss: 0.0006, train/total_loss/avg: 0.1382, max mem: 11667.0, experiment: run, epoch: 30, num_updates: 7900, iterations: 7900, max_updates: 10000, lr: 0., ups: 1.14, time: 44s 607ms, time_since_start: 02h 12m 13s 499ms, eta: 32m 17s 219ms\n",
            "\u001b[32m2021-04-22T02:08:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7950/10000, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.1374, train/total_loss: 0.0006, train/total_loss/avg: 0.1374, max mem: 11667.0, experiment: run, epoch: 30, num_updates: 7950, iterations: 7950, max_updates: 10000, lr: 0., ups: 1.14, time: 44s 652ms, time_since_start: 02h 12m 58s 152ms, eta: 31m 33s 015ms\n",
            "\u001b[32m2021-04-22T02:09:35 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[32m2021-04-22T02:09:35 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-04-22T02:09:44 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-04-22T02:10:11 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-04-22T02:10:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8000/10000, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.1367, train/total_loss: 0.0006, train/total_loss/avg: 0.1367, max mem: 11667.0, experiment: run, epoch: 31, num_updates: 8000, iterations: 8000, max_updates: 10000, lr: 0., ups: 0.60, time: 01m 23s 477ms, time_since_start: 02h 14m 21s 630ms, eta: 57m 32s 637ms\n",
            "\u001b[32m2021-04-22T02:10:12 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2021-04-22T02:10:12 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-22T02:10:12 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-22T02:10:12 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2021-04-22T02:10:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8000/10000, val/hateful_memes/cross_entropy: 2.9589, val/total_loss: 2.9589, val/hateful_memes/accuracy: 0.6352, val/hateful_memes/binary_f1: 0.3541, val/hateful_memes/roc_auc: 0.6074, num_updates: 8000, epoch: 31, iterations: 8000, max_updates: 10000, val_time: 08s 135ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.619485\n",
            "\u001b[32m2021-04-22T02:11:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8050/10000, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.1359, train/total_loss: 0.0006, train/total_loss/avg: 0.1359, max mem: 11667.0, experiment: run, epoch: 31, num_updates: 8050, iterations: 8050, max_updates: 10000, lr: 0., ups: 1.11, time: 45s 195ms, time_since_start: 02h 15m 14s 963ms, eta: 30m 22s 559ms\n",
            "\u001b[32m2021-04-22T02:11:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8100/10000, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.1350, train/total_loss: 0.0006, train/total_loss/avg: 0.1350, max mem: 11667.0, experiment: run, epoch: 31, num_updates: 8100, iterations: 8100, max_updates: 10000, lr: 0., ups: 1.14, time: 44s 641ms, time_since_start: 02h 15m 59s 604ms, eta: 29m 14s 054ms\n",
            "\u001b[32m2021-04-22T02:12:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8150/10000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.1342, train/total_loss: 0.0004, train/total_loss/avg: 0.1342, max mem: 11667.0, experiment: run, epoch: 31, num_updates: 8150, iterations: 8150, max_updates: 10000, lr: 0., ups: 1.14, time: 44s 603ms, time_since_start: 02h 16m 44s 207ms, eta: 28m 26s 428ms\n",
            "\u001b[32m2021-04-22T02:13:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8200/10000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.1334, train/total_loss: 0.0004, train/total_loss/avg: 0.1334, max mem: 11667.0, experiment: run, epoch: 31, num_updates: 8200, iterations: 8200, max_updates: 10000, lr: 0., ups: 1.14, time: 44s 612ms, time_since_start: 02h 17m 28s 820ms, eta: 27m 40s 674ms\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-22T02:14:00 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-22T02:14:00 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2021-04-22T02:14:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8250/10000, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.1326, train/total_loss: 0.0006, train/total_loss/avg: 0.1326, max mem: 11667.0, experiment: run, epoch: 32, num_updates: 8250, iterations: 8250, max_updates: 10000, lr: 0., ups: 1.09, time: 46s 575ms, time_since_start: 02h 18m 15s 396ms, eta: 28m 05s 583ms\n",
            "\u001b[32m2021-04-22T02:14:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8300/10000, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.1318, train/total_loss: 0.0006, train/total_loss/avg: 0.1318, max mem: 11667.0, experiment: run, epoch: 32, num_updates: 8300, iterations: 8300, max_updates: 10000, lr: 0., ups: 1.14, time: 44s 682ms, time_since_start: 02h 19m 079ms, eta: 26m 10s 854ms\n",
            "\u001b[32m2021-04-22T02:15:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8350/10000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.1310, train/total_loss: 0.0004, train/total_loss/avg: 0.1310, max mem: 11667.0, experiment: run, epoch: 32, num_updates: 8350, iterations: 8350, max_updates: 10000, lr: 0., ups: 1.14, time: 44s 601ms, time_since_start: 02h 19m 44s 680ms, eta: 25m 21s 888ms\n",
            "\u001b[32m2021-04-22T02:16:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8400/10000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.1302, train/total_loss: 0.0004, train/total_loss/avg: 0.1302, max mem: 11667.0, experiment: run, epoch: 32, num_updates: 8400, iterations: 8400, max_updates: 10000, lr: 0., ups: 1.14, time: 44s 599ms, time_since_start: 02h 20m 29s 280ms, eta: 24m 35s 710ms\n",
            "\u001b[32m2021-04-22T02:17:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8450/10000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.1295, train/total_loss: 0.0004, train/total_loss/avg: 0.1295, max mem: 11667.0, experiment: run, epoch: 32, num_updates: 8450, iterations: 8450, max_updates: 10000, lr: 0., ups: 1.14, time: 44s 592ms, time_since_start: 02h 21m 13s 872ms, eta: 23m 49s 378ms\n",
            "\u001b[32m2021-04-22T02:17:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8500/10000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.1287, train/total_loss: 0.0004, train/total_loss/avg: 0.1287, max mem: 11667.0, experiment: run, epoch: 32, num_updates: 8500, iterations: 8500, max_updates: 10000, lr: 0., ups: 1.14, time: 44s 611ms, time_since_start: 02h 21m 58s 484ms, eta: 23m 03s 850ms\n",
            "\u001b[32m2021-04-22T02:17:49 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2021-04-22T02:17:49 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[32m2021-04-22T02:17:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8500/10000, val/hateful_memes/cross_entropy: 3.1748, val/total_loss: 3.1748, val/hateful_memes/accuracy: 0.6296, val/hateful_memes/binary_f1: 0.3506, val/hateful_memes/roc_auc: 0.6044, num_updates: 8500, epoch: 32, iterations: 8500, max_updates: 10000, val_time: 08s 010ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.619485\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-22T02:18:08 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-22T02:18:08 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2021-04-22T02:18:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8550/10000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.1280, train/total_loss: 0.0004, train/total_loss/avg: 0.1280, max mem: 11667.0, experiment: run, epoch: 33, num_updates: 8550, iterations: 8550, max_updates: 10000, lr: 0., ups: 1.06, time: 47s 165ms, time_since_start: 02h 22m 53s 662ms, eta: 23m 34s 310ms\n",
            "\u001b[32m2021-04-22T02:19:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8600/10000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.1272, train/total_loss: 0.0004, train/total_loss/avg: 0.1272, max mem: 11667.0, experiment: run, epoch: 33, num_updates: 8600, iterations: 8600, max_updates: 10000, lr: 0., ups: 1.14, time: 44s 619ms, time_since_start: 02h 23m 38s 282ms, eta: 21m 31s 835ms\n",
            "\u001b[32m2021-04-22T02:20:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8650/10000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.1265, train/total_loss: 0.0004, train/total_loss/avg: 0.1265, max mem: 11667.0, experiment: run, epoch: 33, num_updates: 8650, iterations: 8650, max_updates: 10000, lr: 0., ups: 1.14, time: 44s 593ms, time_since_start: 02h 24m 22s 875ms, eta: 20m 44s 954ms\n",
            "\u001b[32m2021-04-22T02:20:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8700/10000, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.1258, train/total_loss: 0.0006, train/total_loss/avg: 0.1258, max mem: 11667.0, experiment: run, epoch: 33, num_updates: 8700, iterations: 8700, max_updates: 10000, lr: 0., ups: 1.14, time: 44s 620ms, time_since_start: 02h 25m 07s 496ms, eta: 19m 59s 582ms\n",
            "\u001b[32m2021-04-22T02:21:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8750/10000, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.1251, train/total_loss: 0.0006, train/total_loss/avg: 0.1251, max mem: 11667.0, experiment: run, epoch: 33, num_updates: 8750, iterations: 8750, max_updates: 10000, lr: 0., ups: 1.14, time: 44s 606ms, time_since_start: 02h 25m 52s 102ms, eta: 19m 13s 075ms\n",
            "\u001b[32m2021-04-22T02:23:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8850/10000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.1237, train/total_loss: 0.0004, train/total_loss/avg: 0.1237, max mem: 11667.0, experiment: run, epoch: 34, num_updates: 8850, iterations: 8850, max_updates: 10000, lr: 0., ups: 1.14, time: 44s 625ms, time_since_start: 02h 27m 23s 276ms, eta: 17m 41s 287ms\n",
            "\u001b[32m2021-04-22T02:23:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8900/10000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.1230, train/total_loss: 0.0004, train/total_loss/avg: 0.1230, max mem: 11667.0, experiment: run, epoch: 34, num_updates: 8900, iterations: 8900, max_updates: 10000, lr: 0., ups: 1.14, time: 44s 607ms, time_since_start: 02h 28m 07s 884ms, eta: 16m 54s 726ms\n",
            "\u001b[32m2021-04-22T02:24:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8950/10000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.1223, train/total_loss: 0.0003, train/total_loss/avg: 0.1223, max mem: 11667.0, experiment: run, epoch: 34, num_updates: 8950, iterations: 8950, max_updates: 10000, lr: 0., ups: 1.14, time: 44s 647ms, time_since_start: 02h 28m 52s 532ms, eta: 16m 09s 485ms\n",
            "\u001b[32m2021-04-22T02:25:27 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[32m2021-04-22T02:25:27 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-04-22T02:25:37 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-04-22T02:26:02 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-04-22T02:26:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9000/10000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.1216, train/total_loss: 0.0003, train/total_loss/avg: 0.1216, max mem: 11667.0, experiment: run, epoch: 34, num_updates: 9000, iterations: 9000, max_updates: 10000, lr: 0., ups: 0.63, time: 01m 19s 760ms, time_since_start: 02h 30m 12s 292ms, eta: 27m 29s 440ms\n",
            "\u001b[32m2021-04-22T02:26:02 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2021-04-22T02:26:02 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-22T02:26:02 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-22T02:26:02 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2021-04-22T02:26:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9000/10000, val/hateful_memes/cross_entropy: 3.2362, val/total_loss: 3.2362, val/hateful_memes/accuracy: 0.6296, val/hateful_memes/binary_f1: 0.3548, val/hateful_memes/roc_auc: 0.6040, num_updates: 9000, epoch: 34, iterations: 9000, max_updates: 10000, val_time: 08s 233ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.619485\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-22T02:26:50 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-22T02:26:50 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2021-04-22T02:26:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9050/10000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.1210, train/total_loss: 0.0002, train/total_loss/avg: 0.1210, max mem: 11667.0, experiment: run, epoch: 35, num_updates: 9050, iterations: 9050, max_updates: 10000, lr: 0., ups: 1.06, time: 47s 064ms, time_since_start: 02h 31m 07s 593ms, eta: 15m 24s 638ms\n",
            "\u001b[32m2021-04-22T02:27:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9100/10000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.1203, train/total_loss: 0.0002, train/total_loss/avg: 0.1203, max mem: 11667.0, experiment: run, epoch: 35, num_updates: 9100, iterations: 9100, max_updates: 10000, lr: 0., ups: 1.14, time: 44s 605ms, time_since_start: 02h 31m 52s 198ms, eta: 13m 50s 199ms\n",
            "\u001b[32m2021-04-22T02:28:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9150/10000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.1196, train/total_loss: 0.0002, train/total_loss/avg: 0.1196, max mem: 11667.0, experiment: run, epoch: 35, num_updates: 9150, iterations: 9150, max_updates: 10000, lr: 0., ups: 1.14, time: 44s 609ms, time_since_start: 02h 32m 36s 808ms, eta: 13m 04s 145ms\n",
            "\u001b[32m2021-04-22T02:29:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9200/10000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.1190, train/total_loss: 0.0002, train/total_loss/avg: 0.1190, max mem: 11667.0, experiment: run, epoch: 35, num_updates: 9200, iterations: 9200, max_updates: 10000, lr: 0., ups: 1.14, time: 44s 592ms, time_since_start: 02h 33m 21s 400ms, eta: 12m 17s 734ms\n",
            "\u001b[32m2021-04-22T02:29:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9250/10000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.1183, train/total_loss: 0.0002, train/total_loss/avg: 0.1183, max mem: 11667.0, experiment: run, epoch: 35, num_updates: 9250, iterations: 9250, max_updates: 10000, lr: 0., ups: 1.14, time: 44s 605ms, time_since_start: 02h 34m 06s 006ms, eta: 11m 31s 834ms\n",
            "\u001b[32m2021-04-22T02:30:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9300/10000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.1177, train/total_loss: 0.0002, train/total_loss/avg: 0.1177, max mem: 11667.0, experiment: run, epoch: 35, num_updates: 9300, iterations: 9300, max_updates: 10000, lr: 0., ups: 1.14, time: 44s 623ms, time_since_start: 02h 34m 50s 629ms, eta: 10m 45s 970ms\n",
            "\u001b[32m2021-04-22T02:31:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9350/10000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.1171, train/total_loss: 0.0002, train/total_loss/avg: 0.1171, max mem: 11667.0, experiment: run, epoch: 36, num_updates: 9350, iterations: 9350, max_updates: 10000, lr: 0., ups: 1.09, time: 46s 672ms, time_since_start: 02h 35m 37s 302ms, eta: 10m 27s 373ms\n",
            "\u001b[32m2021-04-22T02:32:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9400/10000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.1165, train/total_loss: 0.0002, train/total_loss/avg: 0.1165, max mem: 11667.0, experiment: run, epoch: 36, num_updates: 9400, iterations: 9400, max_updates: 10000, lr: 0., ups: 1.14, time: 44s 607ms, time_since_start: 02h 36m 21s 910ms, eta: 09m 13s 492ms\n",
            "\u001b[32m2021-04-22T02:32:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9450/10000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.1158, train/total_loss: 0.0001, train/total_loss/avg: 0.1158, max mem: 11667.0, experiment: run, epoch: 36, num_updates: 9450, iterations: 9450, max_updates: 10000, lr: 0., ups: 1.14, time: 44s 614ms, time_since_start: 02h 37m 06s 524ms, eta: 08m 27s 445ms\n",
            "\u001b[32m2021-04-22T02:33:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9500/10000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.1152, train/total_loss: 0.0001, train/total_loss/avg: 0.1152, max mem: 11667.0, experiment: run, epoch: 36, num_updates: 9500, iterations: 9500, max_updates: 10000, lr: 0., ups: 1.14, time: 44s 611ms, time_since_start: 02h 37m 51s 135ms, eta: 07m 41s 278ms\n",
            "\u001b[32m2021-04-22T02:33:41 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2021-04-22T02:33:41 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[32m2021-04-22T02:33:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9500/10000, val/hateful_memes/cross_entropy: 3.2575, val/total_loss: 3.2575, val/hateful_memes/accuracy: 0.6259, val/hateful_memes/binary_f1: 0.3648, val/hateful_memes/roc_auc: 0.6011, num_updates: 9500, epoch: 36, iterations: 9500, max_updates: 10000, val_time: 08s 263ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.619485\n",
            "\u001b[32m2021-04-22T02:34:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9550/10000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.1146, train/total_loss: 0.0001, train/total_loss/avg: 0.1146, max mem: 11667.0, experiment: run, epoch: 36, num_updates: 9550, iterations: 9550, max_updates: 10000, lr: 0., ups: 1.11, time: 45s 193ms, time_since_start: 02h 38m 44s 594ms, eta: 07m 575ms\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-22T02:34:58 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-22T02:34:58 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2021-04-22T02:35:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9600/10000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.1140, train/total_loss: 0.0001, train/total_loss/avg: 0.1140, max mem: 11667.0, experiment: run, epoch: 37, num_updates: 9600, iterations: 9600, max_updates: 10000, lr: 0., ups: 1.09, time: 46s 643ms, time_since_start: 02h 39m 31s 238ms, eta: 06m 25s 837ms\n",
            "\u001b[32m2021-04-22T02:36:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9650/10000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.1134, train/total_loss: 0.0001, train/total_loss/avg: 0.1134, max mem: 11667.0, experiment: run, epoch: 37, num_updates: 9650, iterations: 9650, max_updates: 10000, lr: 0., ups: 1.14, time: 44s 616ms, time_since_start: 02h 40m 15s 854ms, eta: 05m 22s 931ms\n",
            "\u001b[32m2021-04-22T02:36:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9700/10000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.1129, train/total_loss: 0.0001, train/total_loss/avg: 0.1129, max mem: 11667.0, experiment: run, epoch: 37, num_updates: 9700, iterations: 9700, max_updates: 10000, lr: 0., ups: 1.14, time: 44s 607ms, time_since_start: 02h 41m 462ms, eta: 04m 36s 743ms\n",
            "\u001b[32m2021-04-22T02:37:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9750/10000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.1123, train/total_loss: 0.0001, train/total_loss/avg: 0.1123, max mem: 11667.0, experiment: run, epoch: 37, num_updates: 9750, iterations: 9750, max_updates: 10000, lr: 0., ups: 1.14, time: 44s 622ms, time_since_start: 02h 41m 45s 084ms, eta: 03m 50s 698ms\n",
            "\u001b[32m2021-04-22T02:38:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9800/10000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.1117, train/total_loss: 0.0001, train/total_loss/avg: 0.1117, max mem: 11667.0, experiment: run, epoch: 37, num_updates: 9800, iterations: 9800, max_updates: 10000, lr: 0., ups: 1.14, time: 44s 631ms, time_since_start: 02h 42m 29s 716ms, eta: 03m 04s 596ms\n",
            "\u001b[32m2021-04-22T02:39:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9850/10000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.1111, train/total_loss: 0.0001, train/total_loss/avg: 0.1111, max mem: 11667.0, experiment: run, epoch: 38, num_updates: 9850, iterations: 9850, max_updates: 10000, lr: 0., ups: 1.09, time: 46s 593ms, time_since_start: 02h 43m 16s 309ms, eta: 02m 24s 531ms\n",
            "\u001b[32m2021-04-22T02:39:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9900/10000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.1106, train/total_loss: 0.0001, train/total_loss/avg: 0.1106, max mem: 11667.0, experiment: run, epoch: 38, num_updates: 9900, iterations: 9900, max_updates: 10000, lr: 0., ups: 1.14, time: 44s 636ms, time_since_start: 02h 44m 945ms, eta: 01m 32s 307ms\n",
            "\u001b[32m2021-04-22T02:40:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9950/10000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.1100, train/total_loss: 0.0001, train/total_loss/avg: 0.1100, max mem: 11667.0, experiment: run, epoch: 38, num_updates: 9950, iterations: 9950, max_updates: 10000, lr: 0., ups: 1.14, time: 44s 640ms, time_since_start: 02h 44m 45s 585ms, eta: 46s 157ms\n",
            "\u001b[32m2021-04-22T02:41:20 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[32m2021-04-22T02:41:20 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-04-22T02:41:30 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-04-22T02:41:55 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-04-22T02:41:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10000/10000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.1095, train/total_loss: 0.0001, train/total_loss/avg: 0.1095, max mem: 11667.0, experiment: run, epoch: 38, num_updates: 10000, iterations: 10000, max_updates: 10000, lr: 0., ups: 0.63, time: 01m 19s 716ms, time_since_start: 02h 46m 05s 302ms, eta: 0ms\n",
            "\u001b[32m2021-04-22T02:41:55 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2021-04-22T02:41:55 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-22T02:41:55 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-22T02:41:55 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[32m2021-04-22T02:42:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10000/10000, val/hateful_memes/cross_entropy: 3.3053, val/total_loss: 3.3053, val/hateful_memes/accuracy: 0.6333, val/hateful_memes/binary_f1: 0.3444, val/hateful_memes/roc_auc: 0.6029, num_updates: 10000, epoch: 38, iterations: 10000, max_updates: 10000, val_time: 08s 253ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.619485\n",
            "\u001b[32m2021-04-22T02:42:04 | mmf.trainers.core.training_loop: \u001b[0mStepping into final validation check\n",
            "\u001b[32m2021-04-22T02:42:04 | mmf.utils.checkpoint: \u001b[0mRestoring checkpoint\n",
            "\u001b[32m2021-04-22T02:42:04 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
            "\u001b[32m2021-04-22T02:42:33 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
            "\u001b[32m2021-04-22T02:42:33 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 1000\n",
            "\u001b[32m2021-04-22T02:42:33 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 1000\n",
            "\u001b[32m2021-04-22T02:42:33 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 4\n",
            "\u001b[32m2021-04-22T02:42:35 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on test set\n",
            "\u001b[32m2021-04-22T02:42:35 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-22T02:42:35 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-22T02:42:35 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "  0% 0/63 [00:00<?, ?it/s]\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-22T02:42:37 | py.warnings: \u001b[0m/content/mmf/mmf/modules/losses.py:109: UserWarning: Sample list has not field 'targets', are you sure that your ImDB has labels? you may have wanted to run with evaluation.predict=true\n",
            "  \"Sample list has not field 'targets', are you \"\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-22T02:42:37 | py.warnings: \u001b[0m/content/mmf/mmf/modules/losses.py:109: UserWarning: Sample list has not field 'targets', are you sure that your ImDB has labels? you may have wanted to run with evaluation.predict=true\n",
            "  \"Sample list has not field 'targets', are you \"\n",
            "\n",
            "  2% 1/63 [00:02<02:08,  2.07s/it]\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-22T02:42:37 | py.warnings: \u001b[0m/content/mmf/mmf/common/report.py:164: UserWarning: targets not found in report. Metrics calculation might not work as expected.\n",
            "  + \"might not work as expected.\"\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-22T02:42:37 | py.warnings: \u001b[0m/content/mmf/mmf/common/report.py:164: UserWarning: targets not found in report. Metrics calculation might not work as expected.\n",
            "  + \"might not work as expected.\"\n",
            "\n",
            "100% 63/63 [00:25<00:00,  2.51it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/mmf_run\", line 33, in <module>\n",
            "    sys.exit(load_entry_point('mmf', 'console_scripts', 'mmf_run')())\n",
            "  File \"/content/mmf/mmf_cli/run.py\", line 133, in run\n",
            "    main(configuration, predict=predict)\n",
            "  File \"/content/mmf/mmf_cli/run.py\", line 56, in main\n",
            "    trainer.train()\n",
            "  File \"/content/mmf/mmf/trainers/mmf_trainer.py\", line 144, in train\n",
            "    self.inference()\n",
            "  File \"/content/mmf/mmf/trainers/mmf_trainer.py\", line 162, in inference\n",
            "    report, meter = self.evaluation_loop(dataset, use_tqdm=True)\n",
            "  File \"/content/mmf/mmf/trainers/core/evaluation_loop.py\", line 86, in evaluation_loop\n",
            "    combined_report.metrics = self.metrics(combined_report, combined_report)\n",
            "  File \"/content/mmf/mmf/modules/metrics.py\", line 156, in __call__\n",
            "    sample_list, model_output, *args, **kwargs\n",
            "  File \"/content/mmf/mmf/modules/metrics.py\", line 221, in _calculate_with_checks\n",
            "    value = self.calculate(*args, **kwargs)\n",
            "  File \"/content/mmf/mmf/modules/metrics.py\", line 254, in calculate\n",
            "    expected = sample_list[\"targets\"]\n",
            "KeyError: 'targets'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mB-z-6XWdBd"
      },
      "source": [
        "## Build your own model\n",
        "\n",
        "Using MMF's encoders, modules and utilities, we can easily build a custom model. In this example, we are building a fusion model which fuses ResNet pooled grid features with fasttext embedding vectors to classify a meme as hateful or not hateful. \n",
        "\n",
        "Steps involved in building the model are:\n",
        "\n",
        "1. Create a new processor to get fasttext sentence embeddings. (Read more on processors [here]())\n",
        "2. Create new model using encoders from MMF.\n",
        "3. Move hardcoded stuff from model to configuration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2yjX5JxIu2C"
      },
      "source": [
        "import torch \n",
        "\n",
        "# We will inherit the FastText Processor already present in MMF\n",
        "from mmf.datasets.processors import FastTextProcessor\n",
        "# registry is needed to register processor and model to be MMF discoverable\n",
        "from mmf.common.registry import registry\n",
        "\n",
        "# Register the processor so that MMF can discover it\n",
        "@registry.register_processor(\"fasttext_sentence_vector\")\n",
        "class FastTextSentenceVectorProcessor(FastTextProcessor):\n",
        "    # Override the call method\n",
        "    def __call__(self, item):\n",
        "        # This function is present in FastTextProcessor class and loads\n",
        "        # fasttext bin\n",
        "        self._load_fasttext_model(self.model_file)\n",
        "        if \"text\" in item:\n",
        "            text = item[\"text\"]\n",
        "        elif \"tokens\" in item:\n",
        "            text = \" \".join(item[\"tokens\"])\n",
        "\n",
        "        # Get a sentence vector for sentence and convert it to torch tensor\n",
        "        sentence_vector = torch.tensor(\n",
        "            self.model.get_sentence_vector(text),\n",
        "            dtype=torch.float\n",
        "        )\n",
        "\n",
        "        # Return back a dict\n",
        "        return {\n",
        "            \"text\": sentence_vector\n",
        "        }\n",
        "    \n",
        "    # Make dataset builder happy, return a random number\n",
        "    def get_vocab_size(self):\n",
        "        return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlB4n0nwKWZn"
      },
      "source": [
        "import torch\n",
        "\n",
        "# registry is need to register our new model so as to be MMF discoverable\n",
        "from mmf.common.registry import registry\n",
        "# All model using MMF need to inherit BaseModel\n",
        "from mmf.models.base_model import BaseModel\n",
        "# ProjectionEmbedding will act as proxy encoder for FastText Sentence Vector\n",
        "from mmf.modules.embeddings import ProjectionEmbedding\n",
        "# Builder methods for image encoder and classifier\n",
        "from mmf.utils.build import build_classifier_layer, build_image_encoder\n",
        "\n",
        "# Register the model for MMF, \"concat_vl\" key would be used to find the model\n",
        "@registry.register_model(\"concat_vl\")\n",
        "class LanguageAndVisionConcat(BaseModel):\n",
        "    # All models in MMF get first argument as config which contains all\n",
        "    # of the information you stored in this model's config (hyperparameters)\n",
        "    def __init__(self, config, *args, **kwargs):\n",
        "        # This is not needed in most cases as it just calling parent's init\n",
        "        # with same parameters. But to explain how config is initialized we \n",
        "        # have kept this\n",
        "        super().__init__(config, *args, **kwargs)\n",
        "    \n",
        "    # This classmethod tells MMF where to look for default config of this model\n",
        "    @classmethod\n",
        "    def config_path(cls):\n",
        "        # Relative to user dir root\n",
        "        return \"/content/hm_example_mmf/configs/models/concat_vl.yaml\"\n",
        "    \n",
        "    # Each method need to define a build method where the model's modules\n",
        "    # are actually build and assigned to the model\n",
        "    def build(self):\n",
        "        \"\"\"\n",
        "        Config's image_encoder attribute will used to build an MMF image\n",
        "        encoder. This config in yaml will look like:\n",
        "\n",
        "        # \"type\" parameter specifies the type of encoder we are using here. \n",
        "        # In this particular case, we are using resnet152\n",
        "        type: resnet152\n",
        "      \n",
        "        # Parameters are passed to underlying encoder class by \n",
        "        # build_image_encoder\n",
        "        params:\n",
        "          # Specifies whether to use a pretrained version\n",
        "          pretrained: true \n",
        "          # Pooling type, use max to use AdaptiveMaxPool2D\n",
        "          pool_type: avg \n",
        "      \n",
        "          # Number of output features from the encoder, -1 for original\n",
        "          # otherwise, supports between 1 to 9\n",
        "          num_output_features: 1 \n",
        "        \"\"\"\n",
        "        self.vision_module = build_image_encoder(self.config.image_encoder)\n",
        "\n",
        "        \"\"\"\n",
        "        For classifer, configuration would look like:\n",
        "        # Specifies the type of the classifier, in this case mlp\n",
        "        type: mlp\n",
        "        # Parameter to the classifier passed through build_classifier_layer\n",
        "        params:\n",
        "          # Dimension of the tensor coming into the classifier\n",
        "          in_dim: 512\n",
        "          # Dimension of the tensor going out of the classifier\n",
        "          out_dim: 2\n",
        "          # Number of MLP layers in the classifier\n",
        "          num_layers: 0\n",
        "        \"\"\"\n",
        "        self.classifier = build_classifier_layer(self.config.classifier)\n",
        "        \n",
        "        # ProjectionEmbeddings takes in params directly as it is module\n",
        "        # So, pass in kwargs, which are in_dim, out_dim and module\n",
        "        # whose value would be \"linear\" as we want linear layer\n",
        "        self.language_module = ProjectionEmbedding(\n",
        "            **self.config.text_encoder.params\n",
        "        )\n",
        "        # Dropout value will come from config now\n",
        "        self.dropout = torch.nn.Dropout(self.config.dropout)\n",
        "        # Same as Projection Embedding, fusion's layer params (which are param \n",
        "        # for linear layer) will come from config now\n",
        "        self.fusion = torch.nn.Linear(**self.config.fusion.params)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "\n",
        "    # Each model in MMF gets a dict called sample_list which contains\n",
        "    # all of the necessary information returned from the image\n",
        "    def forward(self, sample_list):\n",
        "        # Text input features will be in \"text\" key\n",
        "        text = sample_list[\"text\"]\n",
        "        # Similarly, image input will be in \"image\" key\n",
        "        image = sample_list[\"image\"]\n",
        "\n",
        "        text_features = self.relu(self.language_module(text))\n",
        "        image_features = self.relu(self.vision_module(image))\n",
        "\n",
        "        # Concatenate the features returned from two modality encoders\n",
        "        combined = torch.cat([text_features, image_features.squeeze()], dim=1)\n",
        "\n",
        "        # Pass through the fusion layer, relu and dropout\n",
        "        fused = self.dropout(self.relu(self.fusion(combined)))\n",
        "\n",
        "        # Pass final tensor from classifier to get scores\n",
        "        logits = self.classifier(fused)\n",
        "\n",
        "        # For loss calculations (automatically done by MMF based on loss defined\n",
        "        # in the config), we need to return a dict with \"scores\" key as logits\n",
        "        output = {\"scores\": logits}\n",
        "\n",
        "        # MMF will automatically calculate loss\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8VCzWStDwkJ"
      },
      "source": [
        "Now, we will install the example repo that we have already created on top of MMF and contains code in this colab. We do this so that we don't have to build configs again from scratch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjvxZYBXTrRG",
        "outputId": "704a5f56-5828-4bce-dcf1-c691368b21cf"
      },
      "source": [
        "!git clone https://github.com/apsdehal/hm_example_mmf /content/hm_example_mmf"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into '/content/hm_example_mmf'...\n",
            "remote: Enumerating objects: 25, done.\u001b[K\n",
            "remote: Counting objects: 100% (25/25), done.\u001b[K\n",
            "remote: Compressing objects: 100% (16/16), done.\u001b[K\n",
            "remote: Total 25 (delta 5), reused 22 (delta 3), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (25/25), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taXGqCxQXJbo"
      },
      "source": [
        "## Train your model\n",
        "\n",
        "In this step, we will train the model we just built. A dot list can be passed as either a dict or a list to the run to override the configuration parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SfkYilLQRMrD",
        "outputId": "5dcafa30-62d6-4aa6-ae4b-655a9ef2a26d"
      },
      "source": [
        "!ls /content/hm_example_mmf/configs/experiments/defaults.yaml"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ls: cannot access '/content/hm_example_mmf/configs/experiments/defaults.yaml': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "f8f354d41290471bb16e594edb96ef4f",
            "f9f889609c9146ed8ccdfcfedb8e75eb",
            "5ebb862f16e642fa8f24ffdc8b3d14cb",
            "d444436836164d2cb470521725b0b169",
            "4876536620aa44a4a0b448f554d3b1f5",
            "45f46f25466840c5b6653d05e395abfb",
            "ff03394f803d4033978dac3cc947195d",
            "15bfadcb8dba41879e4b84a7f0a08ea3"
          ]
        },
        "id": "9Aci1mtsURL9",
        "outputId": "3bede906-b2c8-4c35-ac52-35d05db23fc5"
      },
      "source": [
        "import sys\n",
        "from mmf_cli.run import run\n",
        "opts = opts=[\n",
        "    \"config='/content/hm_example_mmf/configs/experiments/defaults.yaml'\", \n",
        "    \"model=concat_vl\", \n",
        "    \"dataset=hateful_memes\", \n",
        "    \"training.num_workers=0\"\n",
        "]\n",
        "run(opts=opts)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m2021-04-17T23:20:14 | mmf.utils.configuration: \u001b[0mOverriding option config to '/content/hm_example_mmf/configs/experiments/defaults.yaml'\n",
            "\u001b[32m2021-04-17T23:20:14 | mmf.utils.configuration: \u001b[0mOverriding option model to concat_vl\n",
            "\u001b[32m2021-04-17T23:20:14 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n",
            "\u001b[32m2021-04-17T23:20:14 | mmf.utils.configuration: \u001b[0mOverriding option training.num_workers to 0\n",
            "\u001b[32m2021-04-17T23:20:14 | mmf: \u001b[0mLogging to: ./save/train.log\n",
            "\u001b[32m2021-04-17T23:20:14 | mmf_cli.run: \u001b[0mNamespace(config_override=None, opts=[\"config='/content/hm_example_mmf/configs/experiments/defaults.yaml'\", 'model=concat_vl', 'dataset=hateful_memes', 'training.num_workers=0'])\n",
            "\u001b[32m2021-04-17T23:20:14 | mmf_cli.run: \u001b[0mTorch version: 1.8.1+cu102\n",
            "\u001b[32m2021-04-17T23:20:14 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla P100-PCIE-16GB\n",
            "\u001b[32m2021-04-17T23:20:14 | mmf_cli.run: \u001b[0mUsing seed 14307354\n",
            "\u001b[32m2021-04-17T23:20:14 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-17T23:20:15 | py.warnings: \u001b[0m/content/mmf/mmf/datasets/processors/processors.py:442: UserWarning: No model file present at /root/.cache/torch/mmf/wiki.en.bin.\n",
            "  warnings.warn(f\"No model file present at {model_file}.\")\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-17T23:20:15 | py.warnings: \u001b[0m/content/mmf/mmf/datasets/processors/processors.py:442: UserWarning: No model file present at /root/.cache/torch/mmf/wiki.en.bin.\n",
            "  warnings.warn(f\"No model file present at {model_file}.\")\n",
            "\n",
            "\u001b[32m2021-04-17T23:20:15 | mmf.datasets.processors.processors: \u001b[0mDownloading FastText bin\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "169876453it [03:16, 866696.08it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m2021-04-17T23:23:32 | mmf.datasets.processors.processors: \u001b[0mfastText bin downloaded at /root/.cache/torch/mmf/wiki.en.bin.\n",
            "\u001b[32m2021-04-17T23:23:32 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
            "\u001b[32m2021-04-17T23:23:32 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
            "\u001b[32m2021-04-17T23:23:32 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
            "\u001b[32m2021-04-17T23:23:32 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Downloading: \"https://download.pytorch.org/models/resnet152-b121ed2d.pth\" to /root/.cache/torch/hub/checkpoints/resnet152-b121ed2d.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f8f354d41290471bb16e594edb96ef4f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=241530880.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[32m2021-04-17T23:23:45 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
            "\u001b[32m2021-04-17T23:23:45 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-17T23:23:45 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/basecontainer.py:232: UserWarning: cfg.pretty() is deprecated and will be removed in a future version.\n",
            "Use OmegaConf.to_yaml(cfg)\n",
            "\n",
            "  category=UserWarning,\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-17T23:23:45 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/basecontainer.py:232: UserWarning: cfg.pretty() is deprecated and will be removed in a future version.\n",
            "Use OmegaConf.to_yaml(cfg)\n",
            "\n",
            "  category=UserWarning,\n",
            "\n",
            "\u001b[32m2021-04-17T23:23:45 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n",
            "\u001b[32m2021-04-17T23:23:45 | mmf.trainers.mmf_trainer: \u001b[0mLanguageAndVisionConcat(\n",
            "  (vision_module): ResNet152ImageEncoder(\n",
            "    (model): Sequential(\n",
            "      (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "      (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "      (4): Sequential(\n",
            "        (0): Bottleneck(\n",
            "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (downsample): Sequential(\n",
            "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (1): Bottleneck(\n",
            "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (2): Bottleneck(\n",
            "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "      (5): Sequential(\n",
            "        (0): Bottleneck(\n",
            "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (downsample): Sequential(\n",
            "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (1): Bottleneck(\n",
            "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (2): Bottleneck(\n",
            "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (3): Bottleneck(\n",
            "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (4): Bottleneck(\n",
            "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (5): Bottleneck(\n",
            "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (6): Bottleneck(\n",
            "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (7): Bottleneck(\n",
            "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "      (6): Sequential(\n",
            "        (0): Bottleneck(\n",
            "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (downsample): Sequential(\n",
            "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (1): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (2): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (3): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (4): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (5): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (6): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (7): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (8): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (9): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (10): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (11): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (12): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (13): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (14): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (15): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (16): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (17): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (18): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (19): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (20): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (21): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (22): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (23): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (24): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (25): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (26): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (27): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (28): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (29): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (30): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (31): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (32): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (33): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (34): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (35): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "      (7): Sequential(\n",
            "        (0): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (downsample): Sequential(\n",
            "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (1): Bottleneck(\n",
            "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (2): Bottleneck(\n",
            "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  )\n",
            "  (classifier): MLPClassifer(\n",
            "    (layers): ModuleList(\n",
            "      (0): Linear(in_features=512, out_features=2, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (language_module): ProjectionEmbedding(\n",
            "    (layers): Linear(in_features=300, out_features=300, bias=True)\n",
            "  )\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (fusion): Linear(in_features=2348, out_features=512, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (losses): Losses(\n",
            "    (losses): ModuleList(\n",
            "      (0): MMFLoss(\n",
            "        (loss_criterion): CrossEntropyLoss(\n",
            "          (loss_fn): CrossEntropyLoss()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "\u001b[32m2021-04-17T23:23:45 | mmf.utils.general: \u001b[0mTotal Parameters: 59437822. Trained Parameters: 59437822\n",
            "\u001b[32m2021-04-17T23:23:45 | mmf.trainers.core.training_loop: \u001b[0mStarting training...\n",
            "\u001b[32m2021-04-17T23:23:45 | mmf.datasets.processors.processors: \u001b[0mLoading fasttext model now from /root/.cache/torch/mmf/wiki.en.bin\n",
            "\u001b[32m2021-04-17T23:25:49 | mmf.datasets.processors.processors: \u001b[0mFinished loading fasttext model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m2021-04-17T23:29:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/22000, train/hateful_memes/cross_entropy: 0.6420, train/hateful_memes/cross_entropy/avg: 0.6420, train/total_loss: 0.6420, train/total_loss/avg: 0.6420, max mem: 11928.0, experiment: run, epoch: 1, num_updates: 100, iterations: 100, max_updates: 22000, lr: 0., ups: 0.31, time: 05m 19s 047ms, time_since_start: 05m 19s 067ms, eta: 19h 35m 299ms\n",
            "\u001b[32m2021-04-17T23:32:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/22000, train/hateful_memes/cross_entropy: 0.6322, train/hateful_memes/cross_entropy/avg: 0.6371, train/total_loss: 0.6322, train/total_loss/avg: 0.6371, max mem: 11928.0, experiment: run, epoch: 2, num_updates: 200, iterations: 200, max_updates: 22000, lr: 0.00001, ups: 0.54, time: 03m 06s 487ms, time_since_start: 08m 25s 555ms, eta: 11h 23m 40s 267ms\n",
            "\u001b[32m2021-04-17T23:35:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/22000, train/hateful_memes/cross_entropy: 0.6322, train/hateful_memes/cross_entropy/avg: 0.6148, train/total_loss: 0.6322, train/total_loss/avg: 0.6148, max mem: 11928.0, experiment: run, epoch: 3, num_updates: 300, iterations: 300, max_updates: 22000, lr: 0.00001, ups: 0.54, time: 03m 04s 962ms, time_since_start: 11m 30s 518ms, eta: 11h 14m 58s 176ms\n",
            "\u001b[32m2021-04-17T23:38:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/22000, train/hateful_memes/cross_entropy: 0.5702, train/hateful_memes/cross_entropy/avg: 0.5956, train/total_loss: 0.5702, train/total_loss/avg: 0.5956, max mem: 11928.0, experiment: run, epoch: 4, num_updates: 400, iterations: 400, max_updates: 22000, lr: 0.00001, ups: 0.54, time: 03m 04s 978ms, time_since_start: 14m 35s 497ms, eta: 11h 11m 55s 057ms\n",
            "\u001b[32m2021-04-17T23:41:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/22000, train/hateful_memes/cross_entropy: 0.5702, train/hateful_memes/cross_entropy/avg: 0.5769, train/total_loss: 0.5702, train/total_loss/avg: 0.5769, max mem: 11928.0, experiment: run, epoch: 4, num_updates: 500, iterations: 500, max_updates: 22000, lr: 0.00001, ups: 0.54, time: 03m 04s 602ms, time_since_start: 17m 40s 100ms, eta: 11h 07m 26s 835ms\n",
            "\u001b[32m2021-04-17T23:44:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/22000, train/hateful_memes/cross_entropy: 0.5381, train/hateful_memes/cross_entropy/avg: 0.4978, train/total_loss: 0.5381, train/total_loss/avg: 0.4978, max mem: 11928.0, experiment: run, epoch: 5, num_updates: 600, iterations: 600, max_updates: 22000, lr: 0.00002, ups: 0.54, time: 03m 05s 417ms, time_since_start: 20m 45s 517ms, eta: 11h 07m 16s 467ms\n",
            "\u001b[32m2021-04-17T23:47:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/22000, train/hateful_memes/cross_entropy: 0.5381, train/hateful_memes/cross_entropy/avg: 0.4387, train/total_loss: 0.5381, train/total_loss/avg: 0.4387, max mem: 11928.0, experiment: run, epoch: 6, num_updates: 700, iterations: 700, max_updates: 22000, lr: 0.00002, ups: 0.54, time: 03m 04s 421ms, time_since_start: 23m 49s 938ms, eta: 11h 35s 228ms\n",
            "\u001b[32m2021-04-17T23:50:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/22000, train/hateful_memes/cross_entropy: 0.5020, train/hateful_memes/cross_entropy/avg: 0.3888, train/total_loss: 0.5020, train/total_loss/avg: 0.3888, max mem: 11928.0, experiment: run, epoch: 7, num_updates: 800, iterations: 800, max_updates: 22000, lr: 0.00002, ups: 0.54, time: 03m 04s 535ms, time_since_start: 26m 54s 474ms, eta: 10h 57m 53s 662ms\n",
            "\u001b[32m2021-04-17T23:53:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/22000, train/hateful_memes/cross_entropy: 0.5020, train/hateful_memes/cross_entropy/avg: 0.3699, train/total_loss: 0.5020, train/total_loss/avg: 0.3699, max mem: 11928.0, experiment: run, epoch: 7, num_updates: 900, iterations: 900, max_updates: 22000, lr: 0.00002, ups: 0.54, time: 03m 04s 898ms, time_since_start: 29m 59s 372ms, eta: 10h 56m 04s 697ms\n",
            "\u001b[32m2021-04-17T23:56:49 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[32m2021-04-17T23:56:49 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-04-17T23:56:54 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-04-17T23:56:56 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-04-17T23:56:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/22000, train/hateful_memes/cross_entropy: 0.2188, train/hateful_memes/cross_entropy/avg: 0.3426, train/total_loss: 0.2188, train/total_loss/avg: 0.3426, max mem: 11928.0, experiment: run, epoch: 8, num_updates: 1000, iterations: 1000, max_updates: 22000, lr: 0.00003, ups: 0.52, time: 03m 11s 509ms, time_since_start: 33m 10s 882ms, eta: 11h 16m 18s 995ms\n",
            "\u001b[32m2021-04-17T23:56:56 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2021-04-17T23:56:56 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[32m2021-04-17T23:57:05 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-04-17T23:57:08 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
            "\u001b[32m2021-04-17T23:57:11 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-04-17T23:57:20 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-04-17T23:57:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/22000, val/hateful_memes/cross_entropy: 1.8143, val/total_loss: 1.8143, val/hateful_memes/accuracy: 0.5796, val/hateful_memes/binary_f1: 0.3058, val/hateful_memes/roc_auc: 0.4885, num_updates: 1000, epoch: 8, iterations: 1000, max_updates: 22000, val_time: 24s 080ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.488515\n",
            "\u001b[32m2021-04-18T00:00:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1100/22000, train/hateful_memes/cross_entropy: 0.2188, train/hateful_memes/cross_entropy/avg: 0.3173, train/total_loss: 0.2188, train/total_loss/avg: 0.3173, max mem: 11929.0, experiment: run, epoch: 9, num_updates: 1100, iterations: 1100, max_updates: 22000, lr: 0.00003, ups: 0.53, time: 03m 07s 618ms, time_since_start: 36m 42s 583ms, eta: 10h 59m 25s 149ms\n",
            "\u001b[32m2021-04-18T00:03:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/22000, train/hateful_memes/cross_entropy: 0.2044, train/hateful_memes/cross_entropy/avg: 0.3079, train/total_loss: 0.2044, train/total_loss/avg: 0.3079, max mem: 11929.0, experiment: run, epoch: 10, num_updates: 1200, iterations: 1200, max_updates: 22000, lr: 0.00003, ups: 0.54, time: 03m 04s 828ms, time_since_start: 39m 47s 412ms, eta: 10h 46m 30s 305ms\n",
            "\u001b[32m2021-04-18T00:06:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1300/22000, train/hateful_memes/cross_entropy: 0.2044, train/hateful_memes/cross_entropy/avg: 0.2921, train/total_loss: 0.2044, train/total_loss/avg: 0.2921, max mem: 11929.0, experiment: run, epoch: 10, num_updates: 1300, iterations: 1300, max_updates: 22000, lr: 0.00003, ups: 0.54, time: 03m 04s 442ms, time_since_start: 42m 51s 854ms, eta: 10h 42m 03s 169ms\n",
            "\u001b[32m2021-04-18T00:09:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/22000, train/hateful_memes/cross_entropy: 0.1024, train/hateful_memes/cross_entropy/avg: 0.2748, train/total_loss: 0.1024, train/total_loss/avg: 0.2748, max mem: 11929.0, experiment: run, epoch: 11, num_updates: 1400, iterations: 1400, max_updates: 22000, lr: 0.00003, ups: 0.54, time: 03m 04s 548ms, time_since_start: 45m 56s 402ms, eta: 10h 39m 19s 116ms\n",
            "\u001b[32m2021-04-18T00:12:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1500/22000, train/hateful_memes/cross_entropy: 0.1024, train/hateful_memes/cross_entropy/avg: 0.2581, train/total_loss: 0.1024, train/total_loss/avg: 0.2581, max mem: 11929.0, experiment: run, epoch: 12, num_updates: 1500, iterations: 1500, max_updates: 22000, lr: 0.00004, ups: 0.54, time: 03m 04s 712ms, time_since_start: 49m 01s 115ms, eta: 10h 36m 46s 883ms\n",
            "\u001b[32m2021-04-18T00:15:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1600/22000, train/hateful_memes/cross_entropy: 0.1020, train/hateful_memes/cross_entropy/avg: 0.2463, train/total_loss: 0.1020, train/total_loss/avg: 0.2463, max mem: 11929.0, experiment: run, epoch: 13, num_updates: 1600, iterations: 1600, max_updates: 22000, lr: 0.00004, ups: 0.54, time: 03m 04s 253ms, time_since_start: 52m 05s 368ms, eta: 10h 32m 05s 958ms\n",
            "\u001b[32m2021-04-18T00:18:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1700/22000, train/hateful_memes/cross_entropy: 0.1020, train/hateful_memes/cross_entropy/avg: 0.2338, train/total_loss: 0.1020, train/total_loss/avg: 0.2338, max mem: 11929.0, experiment: run, epoch: 13, num_updates: 1700, iterations: 1700, max_updates: 22000, lr: 0.00004, ups: 0.54, time: 03m 04s 948ms, time_since_start: 55m 10s 317ms, eta: 10h 31m 22s 422ms\n",
            "\u001b[32m2021-04-18T00:22:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1800/22000, train/hateful_memes/cross_entropy: 0.0966, train/hateful_memes/cross_entropy/avg: 0.2224, train/total_loss: 0.0966, train/total_loss/avg: 0.2224, max mem: 11929.0, experiment: run, epoch: 14, num_updates: 1800, iterations: 1800, max_updates: 22000, lr: 0.00005, ups: 0.54, time: 03m 04s 806ms, time_since_start: 58m 15s 124ms, eta: 10h 27m 46s 992ms\n",
            "\u001b[32m2021-04-18T00:25:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1900/22000, train/hateful_memes/cross_entropy: 0.0966, train/hateful_memes/cross_entropy/avg: 0.2133, train/total_loss: 0.0966, train/total_loss/avg: 0.2133, max mem: 11929.0, experiment: run, epoch: 15, num_updates: 1900, iterations: 1900, max_updates: 22000, lr: 0.00005, ups: 0.55, time: 03m 03s 748ms, time_since_start: 01h 01m 18s 872ms, eta: 10h 21m 05s 797ms\n",
            "\u001b[32m2021-04-18T00:28:08 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[32m2021-04-18T00:28:08 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-04-18T00:28:11 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-04-18T00:28:15 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-04-18T00:28:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/22000, train/hateful_memes/cross_entropy: 0.0836, train/hateful_memes/cross_entropy/avg: 0.2046, train/total_loss: 0.0836, train/total_loss/avg: 0.2046, max mem: 11929.0, experiment: run, epoch: 16, num_updates: 2000, iterations: 2000, max_updates: 22000, lr: 0.00005, ups: 0.53, time: 03m 10s 726ms, time_since_start: 01h 04m 29s 598ms, eta: 10h 41m 28s 560ms\n",
            "\u001b[32m2021-04-18T00:28:15 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2021-04-18T00:28:15 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[32m2021-04-18T00:28:24 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-04-18T00:28:27 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-04-18T00:28:31 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-04-18T00:28:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/22000, val/hateful_memes/cross_entropy: 1.6998, val/total_loss: 1.6998, val/hateful_memes/accuracy: 0.5444, val/hateful_memes/binary_f1: 0.3167, val/hateful_memes/roc_auc: 0.4830, num_updates: 2000, epoch: 16, iterations: 2000, max_updates: 22000, val_time: 16s 110ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.488515\n",
            "\u001b[32m2021-04-18T00:31:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2100/22000, train/hateful_memes/cross_entropy: 0.0836, train/hateful_memes/cross_entropy/avg: 0.2044, train/total_loss: 0.0836, train/total_loss/avg: 0.2044, max mem: 11929.0, experiment: run, epoch: 16, num_updates: 2100, iterations: 2100, max_updates: 22000, lr: 0.00005, ups: 0.54, time: 03m 05s 103ms, time_since_start: 01h 07m 50s 817ms, eta: 10h 19m 27s 193ms\n",
            "\u001b[32m2021-04-18T00:34:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2200/22000, train/hateful_memes/cross_entropy: 0.0688, train/hateful_memes/cross_entropy/avg: 0.1960, train/total_loss: 0.0688, train/total_loss/avg: 0.1960, max mem: 11929.0, experiment: run, epoch: 17, num_updates: 2200, iterations: 2200, max_updates: 22000, lr: 0.00005, ups: 0.54, time: 03m 04s 008ms, time_since_start: 01h 10m 54s 826ms, eta: 10h 12m 41s 658ms\n",
            "\u001b[32m2021-04-18T00:37:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2300/22000, train/hateful_memes/cross_entropy: 0.0650, train/hateful_memes/cross_entropy/avg: 0.1877, train/total_loss: 0.0650, train/total_loss/avg: 0.1877, max mem: 11929.0, experiment: run, epoch: 18, num_updates: 2300, iterations: 2300, max_updates: 22000, lr: 0.00005, ups: 0.54, time: 03m 04s 824ms, time_since_start: 01h 13m 59s 650ms, eta: 10h 12m 18s 078ms\n",
            "\u001b[32m2021-04-18T00:40:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2400/22000, train/hateful_memes/cross_entropy: 0.0495, train/hateful_memes/cross_entropy/avg: 0.1811, train/total_loss: 0.0495, train/total_loss/avg: 0.1811, max mem: 11929.0, experiment: run, epoch: 19, num_updates: 2400, iterations: 2400, max_updates: 22000, lr: 0.00005, ups: 0.54, time: 03m 04s 811ms, time_since_start: 01h 17m 04s 462ms, eta: 10h 09m 08s 990ms\n",
            "\u001b[32m2021-04-18T00:43:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2500/22000, train/hateful_memes/cross_entropy: 0.0492, train/hateful_memes/cross_entropy/avg: 0.1742, train/total_loss: 0.0492, train/total_loss/avg: 0.1742, max mem: 11929.0, experiment: run, epoch: 19, num_updates: 2500, iterations: 2500, max_updates: 22000, lr: 0.00005, ups: 0.54, time: 03m 04s 410ms, time_since_start: 01h 20m 08s 872ms, eta: 10h 04m 43s 627ms\n",
            "\u001b[32m2021-04-18T00:46:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2600/22000, train/hateful_memes/cross_entropy: 0.0492, train/hateful_memes/cross_entropy/avg: 0.1719, train/total_loss: 0.0492, train/total_loss/avg: 0.1719, max mem: 11929.0, experiment: run, epoch: 20, num_updates: 2600, iterations: 2600, max_updates: 22000, lr: 0.00005, ups: 0.54, time: 03m 04s 271ms, time_since_start: 01h 23m 13s 143ms, eta: 10h 01m 10s 325ms\n",
            "\u001b[32m2021-04-18T00:50:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2700/22000, train/hateful_memes/cross_entropy: 0.0403, train/hateful_memes/cross_entropy/avg: 0.1661, train/total_loss: 0.0403, train/total_loss/avg: 0.1661, max mem: 11929.0, experiment: run, epoch: 21, num_updates: 2700, iterations: 2700, max_updates: 22000, lr: 0.00005, ups: 0.54, time: 03m 04s 625ms, time_since_start: 01h 26m 17s 768ms, eta: 09h 59m 13s 393ms\n",
            "\u001b[32m2021-04-18T00:53:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2800/22000, train/hateful_memes/cross_entropy: 0.0492, train/hateful_memes/cross_entropy/avg: 0.1620, train/total_loss: 0.0492, train/total_loss/avg: 0.1620, max mem: 11929.0, experiment: run, epoch: 22, num_updates: 2800, iterations: 2800, max_updates: 22000, lr: 0.00005, ups: 0.54, time: 03m 04s 363ms, time_since_start: 01h 29m 22s 132ms, eta: 09h 55m 16s 382ms\n",
            "\u001b[32m2021-04-18T00:56:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2900/22000, train/hateful_memes/cross_entropy: 0.0492, train/hateful_memes/cross_entropy/avg: 0.1607, train/total_loss: 0.0492, train/total_loss/avg: 0.1607, max mem: 11929.0, experiment: run, epoch: 22, num_updates: 2900, iterations: 2900, max_updates: 22000, lr: 0.00005, ups: 0.54, time: 03m 04s 717ms, time_since_start: 01h 32m 26s 850ms, eta: 09h 53m 18s 671ms\n",
            "\u001b[32m2021-04-18T00:59:17 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[32m2021-04-18T00:59:17 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-04-18T00:59:19 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-04-18T00:59:23 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-04-18T00:59:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/22000, train/hateful_memes/cross_entropy: 0.0403, train/hateful_memes/cross_entropy/avg: 0.1554, train/total_loss: 0.0403, train/total_loss/avg: 0.1554, max mem: 11929.0, experiment: run, epoch: 23, num_updates: 3000, iterations: 3000, max_updates: 22000, lr: 0.00005, ups: 0.53, time: 03m 10s 851ms, time_since_start: 01h 35m 37s 701ms, eta: 10h 09m 48s 151ms\n",
            "\u001b[32m2021-04-18T00:59:23 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2021-04-18T00:59:23 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[32m2021-04-18T00:59:31 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-04-18T00:59:34 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
            "\u001b[32m2021-04-18T00:59:38 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-04-18T00:59:47 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-04-18T00:59:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/22000, val/hateful_memes/cross_entropy: 1.8268, val/total_loss: 1.8268, val/hateful_memes/accuracy: 0.5889, val/hateful_memes/binary_f1: 0.3273, val/hateful_memes/roc_auc: 0.5029, num_updates: 3000, epoch: 23, iterations: 3000, max_updates: 22000, val_time: 23s 913ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.502937\n",
            "\u001b[32m2021-04-18T01:02:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3100/22000, train/hateful_memes/cross_entropy: 0.0403, train/hateful_memes/cross_entropy/avg: 0.1532, train/total_loss: 0.0403, train/total_loss/avg: 0.1532, max mem: 11929.0, experiment: run, epoch: 24, num_updates: 3100, iterations: 3100, max_updates: 22000, lr: 0.00005, ups: 0.54, time: 03m 05s 090ms, time_since_start: 01h 39m 06s 708ms, eta: 09h 48m 16s 855ms\n",
            "\u001b[32m2021-04-18T01:05:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3200/22000, train/hateful_memes/cross_entropy: 0.0341, train/hateful_memes/cross_entropy/avg: 0.1493, train/total_loss: 0.0341, train/total_loss/avg: 0.1493, max mem: 11929.0, experiment: run, epoch: 25, num_updates: 3200, iterations: 3200, max_updates: 22000, lr: 0.00005, ups: 0.54, time: 03m 04s 326ms, time_since_start: 01h 42m 11s 035ms, eta: 09h 42m 45s 265ms\n",
            "\u001b[32m2021-04-18T01:09:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3300/22000, train/hateful_memes/cross_entropy: 0.0341, train/hateful_memes/cross_entropy/avg: 0.1463, train/total_loss: 0.0341, train/total_loss/avg: 0.1463, max mem: 11929.0, experiment: run, epoch: 25, num_updates: 3300, iterations: 3300, max_updates: 22000, lr: 0.00005, ups: 0.54, time: 03m 04s 616ms, time_since_start: 01h 45m 15s 651ms, eta: 09h 40m 33s 976ms\n",
            "\u001b[32m2021-04-18T01:12:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3400/22000, train/hateful_memes/cross_entropy: 0.0293, train/hateful_memes/cross_entropy/avg: 0.1422, train/total_loss: 0.0293, train/total_loss/avg: 0.1422, max mem: 11929.0, experiment: run, epoch: 26, num_updates: 3400, iterations: 3400, max_updates: 22000, lr: 0.00005, ups: 0.55, time: 03m 03s 366ms, time_since_start: 01h 48m 19s 018ms, eta: 09h 33m 33s 131ms\n",
            "\u001b[32m2021-04-18T01:15:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3500/22000, train/hateful_memes/cross_entropy: 0.0341, train/hateful_memes/cross_entropy/avg: 0.1415, train/total_loss: 0.0341, train/total_loss/avg: 0.1415, max mem: 11929.0, experiment: run, epoch: 27, num_updates: 3500, iterations: 3500, max_updates: 22000, lr: 0.00005, ups: 0.54, time: 03m 04s 223ms, time_since_start: 01h 51m 23s 241ms, eta: 09h 33m 08s 047ms\n",
            "\u001b[32m2021-04-18T01:18:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3600/22000, train/hateful_memes/cross_entropy: 0.0293, train/hateful_memes/cross_entropy/avg: 0.1379, train/total_loss: 0.0293, train/total_loss/avg: 0.1379, max mem: 11929.0, experiment: run, epoch: 28, num_updates: 3600, iterations: 3600, max_updates: 22000, lr: 0.00005, ups: 0.54, time: 03m 04s 322ms, time_since_start: 01h 54m 27s 563ms, eta: 09h 30m 20s 524ms\n",
            "\u001b[32m2021-04-18T01:21:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3700/22000, train/hateful_memes/cross_entropy: 0.0290, train/hateful_memes/cross_entropy/avg: 0.1347, train/total_loss: 0.0290, train/total_loss/avg: 0.1347, max mem: 11929.0, experiment: run, epoch: 28, num_updates: 3700, iterations: 3700, max_updates: 22000, lr: 0.00005, ups: 0.54, time: 03m 04s 698ms, time_since_start: 01h 57m 32s 262ms, eta: 09h 28m 24s 004ms\n",
            "\u001b[32m2021-04-18T01:24:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3800/22000, train/hateful_memes/cross_entropy: 0.0286, train/hateful_memes/cross_entropy/avg: 0.1317, train/total_loss: 0.0286, train/total_loss/avg: 0.1317, max mem: 11929.0, experiment: run, epoch: 29, num_updates: 3800, iterations: 3800, max_updates: 22000, lr: 0.00005, ups: 0.54, time: 03m 04s 326ms, time_since_start: 02h 36s 588ms, eta: 09h 24m 09s 337ms\n",
            "\u001b[32m2021-04-18T01:27:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3900/22000, train/hateful_memes/cross_entropy: 0.0286, train/hateful_memes/cross_entropy/avg: 0.1296, train/total_loss: 0.0286, train/total_loss/avg: 0.1296, max mem: 11929.0, experiment: run, epoch: 30, num_updates: 3900, iterations: 3900, max_updates: 22000, lr: 0.00005, ups: 0.54, time: 03m 04s 407ms, time_since_start: 02h 03m 40s 995ms, eta: 09h 21m 18s 094ms\n",
            "\u001b[32m2021-04-18T01:30:30 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[32m2021-04-18T01:30:30 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-04-18T01:30:32 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-04-18T01:30:36 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-04-18T01:30:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4000/22000, train/hateful_memes/cross_entropy: 0.0210, train/hateful_memes/cross_entropy/avg: 0.1266, train/total_loss: 0.0210, train/total_loss/avg: 0.1266, max mem: 11929.0, experiment: run, epoch: 31, num_updates: 4000, iterations: 4000, max_updates: 22000, lr: 0.00005, ups: 0.53, time: 03m 10s 036ms, time_since_start: 02h 06m 51s 032ms, eta: 09h 35m 14s 409ms\n",
            "\u001b[32m2021-04-18T01:30:36 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2021-04-18T01:30:36 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[32m2021-04-18T01:30:45 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-04-18T01:30:47 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
            "\u001b[32m2021-04-18T01:30:51 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-04-18T01:30:59 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-04-18T01:30:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4000/22000, val/hateful_memes/cross_entropy: 2.0949, val/total_loss: 2.0949, val/hateful_memes/accuracy: 0.5537, val/hateful_memes/binary_f1: 0.3469, val/hateful_memes/roc_auc: 0.5189, num_updates: 4000, epoch: 31, iterations: 4000, max_updates: 22000, val_time: 23s 061ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.518926\n",
            "\u001b[32m2021-04-18T01:34:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4100/22000, train/hateful_memes/cross_entropy: 0.0210, train/hateful_memes/cross_entropy/avg: 0.1247, train/total_loss: 0.0210, train/total_loss/avg: 0.1247, max mem: 11929.0, experiment: run, epoch: 31, num_updates: 4100, iterations: 4100, max_updates: 22000, lr: 0.00004, ups: 0.54, time: 03m 04s 786ms, time_since_start: 02h 10m 18s 883ms, eta: 09h 16m 14s 476ms\n",
            "\u001b[32m2021-04-18T01:37:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4200/22000, train/hateful_memes/cross_entropy: 0.0286, train/hateful_memes/cross_entropy/avg: 0.1250, train/total_loss: 0.0286, train/total_loss/avg: 0.1250, max mem: 11929.0, experiment: run, epoch: 32, num_updates: 4200, iterations: 4200, max_updates: 22000, lr: 0.00004, ups: 0.54, time: 03m 04s 179ms, time_since_start: 02h 13m 23s 063ms, eta: 09h 11m 19s 085ms\n",
            "\u001b[32m2021-04-18T01:40:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4300/22000, train/hateful_memes/cross_entropy: 0.0286, train/hateful_memes/cross_entropy/avg: 0.1226, train/total_loss: 0.0286, train/total_loss/avg: 0.1226, max mem: 11929.0, experiment: run, epoch: 33, num_updates: 4300, iterations: 4300, max_updates: 22000, lr: 0.00004, ups: 0.55, time: 03m 03s 695ms, time_since_start: 02h 16m 26s 758ms, eta: 09h 06m 46s 656ms\n",
            "\u001b[32m2021-04-18T01:43:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4400/22000, train/hateful_memes/cross_entropy: 0.0219, train/hateful_memes/cross_entropy/avg: 0.1201, train/total_loss: 0.0219, train/total_loss/avg: 0.1201, max mem: 11929.0, experiment: run, epoch: 34, num_updates: 4400, iterations: 4400, max_updates: 22000, lr: 0.00004, ups: 0.54, time: 03m 05s 386ms, time_since_start: 02h 19m 32s 144ms, eta: 09h 08m 41s 592ms\n",
            "\u001b[32m2021-04-18T01:46:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4500/22000, train/hateful_memes/cross_entropy: 0.0286, train/hateful_memes/cross_entropy/avg: 0.1204, train/total_loss: 0.0286, train/total_loss/avg: 0.1204, max mem: 11929.0, experiment: run, epoch: 34, num_updates: 4500, iterations: 4500, max_updates: 22000, lr: 0.00004, ups: 0.54, time: 03m 04s 097ms, time_since_start: 02h 22m 36s 241ms, eta: 09h 01m 46s 945ms\n",
            "\u001b[32m2021-04-18T01:49:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4600/22000, train/hateful_memes/cross_entropy: 0.0272, train/hateful_memes/cross_entropy/avg: 0.1184, train/total_loss: 0.0272, train/total_loss/avg: 0.1184, max mem: 11929.0, experiment: run, epoch: 35, num_updates: 4600, iterations: 4600, max_updates: 22000, lr: 0.00004, ups: 0.54, time: 03m 04s 438ms, time_since_start: 02h 25m 40s 679ms, eta: 08h 59m 41s 121ms\n",
            "\u001b[32m2021-04-18T01:52:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4700/22000, train/hateful_memes/cross_entropy: 0.0272, train/hateful_memes/cross_entropy/avg: 0.1161, train/total_loss: 0.0272, train/total_loss/avg: 0.1161, max mem: 11929.0, experiment: run, epoch: 36, num_updates: 4700, iterations: 4700, max_updates: 22000, lr: 0.00004, ups: 0.54, time: 03m 04s 128ms, time_since_start: 02h 28m 44s 808ms, eta: 08h 55m 40s 849ms\n",
            "\u001b[32m2021-04-18T01:55:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4800/22000, train/hateful_memes/cross_entropy: 0.0219, train/hateful_memes/cross_entropy/avg: 0.1137, train/total_loss: 0.0219, train/total_loss/avg: 0.1137, max mem: 11929.0, experiment: run, epoch: 37, num_updates: 4800, iterations: 4800, max_updates: 22000, lr: 0.00004, ups: 0.55, time: 03m 03s 465ms, time_since_start: 02h 31m 48s 273ms, eta: 08h 50m 40s 117ms\n",
            "\u001b[32m2021-04-18T01:58:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4900/22000, train/hateful_memes/cross_entropy: 0.0210, train/hateful_memes/cross_entropy/avg: 0.1115, train/total_loss: 0.0210, train/total_loss/avg: 0.1115, max mem: 11929.0, experiment: run, epoch: 37, num_updates: 4900, iterations: 4900, max_updates: 22000, lr: 0.00004, ups: 0.54, time: 03m 04s 309ms, time_since_start: 02h 34m 52s 583ms, eta: 08h 50m 600ms\n",
            "\u001b[32m2021-04-18T02:01:42 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[32m2021-04-18T02:01:42 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-04-18T02:01:45 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-04-18T02:01:48 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-04-18T02:01:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5000/22000, train/hateful_memes/cross_entropy: 0.0210, train/hateful_memes/cross_entropy/avg: 0.1093, train/total_loss: 0.0210, train/total_loss/avg: 0.1093, max mem: 11929.0, experiment: run, epoch: 38, num_updates: 5000, iterations: 5000, max_updates: 22000, lr: 0.00004, ups: 0.53, time: 03m 10s 627ms, time_since_start: 02h 38m 03s 211ms, eta: 09h 04m 58s 384ms\n",
            "\u001b[32m2021-04-18T02:01:48 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2021-04-18T02:01:48 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[32m2021-04-18T02:01:57 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-04-18T02:02:00 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-04-18T02:02:03 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-04-18T02:02:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5000/22000, val/hateful_memes/cross_entropy: 2.0478, val/total_loss: 2.0478, val/hateful_memes/accuracy: 0.5259, val/hateful_memes/binary_f1: 0.3663, val/hateful_memes/roc_auc: 0.5009, num_updates: 5000, epoch: 38, iterations: 5000, max_updates: 22000, val_time: 14s 232ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.518926\n",
            "\u001b[32m2021-04-18T02:05:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5100/22000, train/hateful_memes/cross_entropy: 0.0197, train/hateful_memes/cross_entropy/avg: 0.1072, train/total_loss: 0.0197, train/total_loss/avg: 0.1072, max mem: 11929.0, experiment: run, epoch: 39, num_updates: 5100, iterations: 5100, max_updates: 22000, lr: 0.00004, ups: 0.54, time: 03m 04s 661ms, time_since_start: 02h 41m 22s 112ms, eta: 08h 44m 48s 707ms\n",
            "\u001b[32m2021-04-18T02:08:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5200/22000, train/hateful_memes/cross_entropy: 0.0197, train/hateful_memes/cross_entropy/avg: 0.1077, train/total_loss: 0.0197, train/total_loss/avg: 0.1077, max mem: 11929.0, experiment: run, epoch: 40, num_updates: 5200, iterations: 5200, max_updates: 22000, lr: 0.00004, ups: 0.54, time: 03m 04s 892ms, time_since_start: 02h 44m 27s 005ms, eta: 08h 42m 21s 570ms\n",
            "\u001b[32m2021-04-18T02:11:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5300/22000, train/hateful_memes/cross_entropy: 0.0146, train/hateful_memes/cross_entropy/avg: 0.1058, train/total_loss: 0.0146, train/total_loss/avg: 0.1058, max mem: 11929.0, experiment: run, epoch: 40, num_updates: 5300, iterations: 5300, max_updates: 22000, lr: 0.00004, ups: 0.54, time: 03m 05s 286ms, time_since_start: 02h 47m 32s 292ms, eta: 08h 40m 21s 392ms\n",
            "\u001b[32m2021-04-18T02:14:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5400/22000, train/hateful_memes/cross_entropy: 0.0146, train/hateful_memes/cross_entropy/avg: 0.1039, train/total_loss: 0.0146, train/total_loss/avg: 0.1039, max mem: 11929.0, experiment: run, epoch: 41, num_updates: 5400, iterations: 5400, max_updates: 22000, lr: 0.00004, ups: 0.54, time: 03m 04s 333ms, time_since_start: 02h 50m 36s 625ms, eta: 08h 34m 34s 776ms\n",
            "\u001b[32m2021-04-18T02:17:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5500/22000, train/hateful_memes/cross_entropy: 0.0146, train/hateful_memes/cross_entropy/avg: 0.1023, train/total_loss: 0.0146, train/total_loss/avg: 0.1023, max mem: 11929.0, experiment: run, epoch: 42, num_updates: 5500, iterations: 5500, max_updates: 22000, lr: 0.00004, ups: 0.54, time: 03m 04s 710ms, time_since_start: 02h 53m 41s 336ms, eta: 08h 32m 31s 544ms\n",
            "\u001b[32m2021-04-18T02:20:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5600/22000, train/hateful_memes/cross_entropy: 0.0146, train/hateful_memes/cross_entropy/avg: 0.1006, train/total_loss: 0.0146, train/total_loss/avg: 0.1006, max mem: 11929.0, experiment: run, epoch: 43, num_updates: 5600, iterations: 5600, max_updates: 22000, lr: 0.00004, ups: 0.54, time: 03m 04s 657ms, time_since_start: 02h 56m 45s 994ms, eta: 08h 29m 16s 433ms\n",
            "\u001b[32m2021-04-18T02:23:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5700/22000, train/hateful_memes/cross_entropy: 0.0129, train/hateful_memes/cross_entropy/avg: 0.0989, train/total_loss: 0.0129, train/total_loss/avg: 0.0989, max mem: 11929.0, experiment: run, epoch: 43, num_updates: 5700, iterations: 5700, max_updates: 22000, lr: 0.00004, ups: 0.54, time: 03m 05s 122ms, time_since_start: 02h 59m 51s 116ms, eta: 08h 27m 26s 568ms\n",
            "\u001b[32m2021-04-18T02:26:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5800/22000, train/hateful_memes/cross_entropy: 0.0129, train/hateful_memes/cross_entropy/avg: 0.0975, train/total_loss: 0.0129, train/total_loss/avg: 0.0975, max mem: 11929.0, experiment: run, epoch: 44, num_updates: 5800, iterations: 5800, max_updates: 22000, lr: 0.00004, ups: 0.55, time: 03m 03s 987ms, time_since_start: 03h 02m 55s 104ms, eta: 08h 21m 14s 186ms\n",
            "\u001b[32m2021-04-18T02:29:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5900/22000, train/hateful_memes/cross_entropy: 0.0129, train/hateful_memes/cross_entropy/avg: 0.0970, train/total_loss: 0.0129, train/total_loss/avg: 0.0970, max mem: 11929.0, experiment: run, epoch: 45, num_updates: 5900, iterations: 5900, max_updates: 22000, lr: 0.00004, ups: 0.55, time: 03m 03s 583ms, time_since_start: 03h 05m 58s 687ms, eta: 08h 17m 03s 015ms\n",
            "\u001b[32m2021-04-18T02:32:49 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[32m2021-04-18T02:32:49 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-04-18T02:32:52 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-04-18T02:32:55 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-04-18T02:32:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6000/22000, train/hateful_memes/cross_entropy: 0.0129, train/hateful_memes/cross_entropy/avg: 0.0954, train/total_loss: 0.0129, train/total_loss/avg: 0.0954, max mem: 11929.0, experiment: run, epoch: 46, num_updates: 6000, iterations: 6000, max_updates: 22000, lr: 0.00004, ups: 0.52, time: 03m 11s 551ms, time_since_start: 03h 09m 10s 239ms, eta: 08h 35m 24s 028ms\n",
            "\u001b[32m2021-04-18T02:32:55 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2021-04-18T02:32:55 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[32m2021-04-18T02:33:04 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-04-18T02:33:07 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-04-18T02:33:11 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-04-18T02:33:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6000/22000, val/hateful_memes/cross_entropy: 2.3243, val/total_loss: 2.3243, val/hateful_memes/accuracy: 0.5852, val/hateful_memes/binary_f1: 0.2222, val/hateful_memes/roc_auc: 0.4828, num_updates: 6000, epoch: 46, iterations: 6000, max_updates: 22000, val_time: 15s 570ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.518926\n",
            "\u001b[32m2021-04-18T02:36:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6100/22000, train/hateful_memes/cross_entropy: 0.0086, train/hateful_memes/cross_entropy/avg: 0.0939, train/total_loss: 0.0086, train/total_loss/avg: 0.0939, max mem: 11929.0, experiment: run, epoch: 46, num_updates: 6100, iterations: 6100, max_updates: 22000, lr: 0.00004, ups: 0.54, time: 03m 04s 852ms, time_since_start: 03h 12m 30s 666ms, eta: 08h 14m 16s 062ms\n",
            "\u001b[32m2021-04-18T02:39:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6200/22000, train/hateful_memes/cross_entropy: 0.0056, train/hateful_memes/cross_entropy/avg: 0.0924, train/total_loss: 0.0056, train/total_loss/avg: 0.0924, max mem: 11929.0, experiment: run, epoch: 47, num_updates: 6200, iterations: 6200, max_updates: 22000, lr: 0.00004, ups: 0.54, time: 03m 04s 349ms, time_since_start: 03h 15m 35s 016ms, eta: 08h 09m 49s 326ms\n",
            "\u001b[32m2021-04-18T02:42:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6300/22000, train/hateful_memes/cross_entropy: 0.0056, train/hateful_memes/cross_entropy/avg: 0.0911, train/total_loss: 0.0056, train/total_loss/avg: 0.0911, max mem: 11929.0, experiment: run, epoch: 48, num_updates: 6300, iterations: 6300, max_updates: 22000, lr: 0.00004, ups: 0.54, time: 03m 04s 995ms, time_since_start: 03h 18m 40s 011ms, eta: 08h 08m 25s 636ms\n",
            "\u001b[32m2021-04-18T02:45:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6400/22000, train/hateful_memes/cross_entropy: 0.0056, train/hateful_memes/cross_entropy/avg: 0.0899, train/total_loss: 0.0056, train/total_loss/avg: 0.0899, max mem: 11929.0, experiment: run, epoch: 49, num_updates: 6400, iterations: 6400, max_updates: 22000, lr: 0.00004, ups: 0.55, time: 03m 03s 266ms, time_since_start: 03h 21m 43s 277ms, eta: 08h 46s 886ms\n",
            "\u001b[32m2021-04-18T02:48:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6500/22000, train/hateful_memes/cross_entropy: 0.0053, train/hateful_memes/cross_entropy/avg: 0.0885, train/total_loss: 0.0053, train/total_loss/avg: 0.0885, max mem: 11929.0, experiment: run, epoch: 49, num_updates: 6500, iterations: 6500, max_updates: 22000, lr: 0.00004, ups: 0.54, time: 03m 04s 781ms, time_since_start: 03h 24m 48s 059ms, eta: 08h 01m 38s 849ms\n",
            "\u001b[32m2021-04-18T02:51:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6600/22000, train/hateful_memes/cross_entropy: 0.0037, train/hateful_memes/cross_entropy/avg: 0.0872, train/total_loss: 0.0037, train/total_loss/avg: 0.0872, max mem: 11929.0, experiment: run, epoch: 50, num_updates: 6600, iterations: 6600, max_updates: 22000, lr: 0.00004, ups: 0.55, time: 03m 03s 874ms, time_since_start: 03h 27m 51s 933ms, eta: 07h 56m 11s 459ms\n",
            "\u001b[32m2021-04-18T02:54:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6700/22000, train/hateful_memes/cross_entropy: 0.0037, train/hateful_memes/cross_entropy/avg: 0.0872, train/total_loss: 0.0037, train/total_loss/avg: 0.0872, max mem: 11929.0, experiment: run, epoch: 51, num_updates: 6700, iterations: 6700, max_updates: 22000, lr: 0.00004, ups: 0.55, time: 03m 03s 692ms, time_since_start: 03h 30m 55s 625ms, eta: 07h 52m 37s 925ms\n",
            "\u001b[32m2021-04-18T02:57:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6800/22000, train/hateful_memes/cross_entropy: 0.0037, train/hateful_memes/cross_entropy/avg: 0.0860, train/total_loss: 0.0037, train/total_loss/avg: 0.0860, max mem: 11929.0, experiment: run, epoch: 52, num_updates: 6800, iterations: 6800, max_updates: 22000, lr: 0.00004, ups: 0.54, time: 03m 04s 003ms, time_since_start: 03h 33m 59s 629ms, eta: 07h 50m 20s 222ms\n",
            "\u001b[32m2021-04-18T03:00:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6900/22000, train/hateful_memes/cross_entropy: 0.0033, train/hateful_memes/cross_entropy/avg: 0.0847, train/total_loss: 0.0033, train/total_loss/avg: 0.0847, max mem: 11929.0, experiment: run, epoch: 52, num_updates: 6900, iterations: 6900, max_updates: 22000, lr: 0.00004, ups: 0.55, time: 03m 03s 351ms, time_since_start: 03h 37m 02s 980ms, eta: 07h 45m 35s 261ms\n",
            "\u001b[32m2021-04-18T03:03:52 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[32m2021-04-18T03:03:52 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-04-18T03:03:54 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-04-18T03:03:58 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-04-18T03:03:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7000/22000, train/hateful_memes/cross_entropy: 0.0033, train/hateful_memes/cross_entropy/avg: 0.0835, train/total_loss: 0.0033, train/total_loss/avg: 0.0835, max mem: 11929.0, experiment: run, epoch: 53, num_updates: 7000, iterations: 7000, max_updates: 22000, lr: 0.00004, ups: 0.53, time: 03m 09s 732ms, time_since_start: 03h 40m 12s 713ms, eta: 07h 58m 35s 989ms\n",
            "\u001b[32m2021-04-18T03:03:58 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2021-04-18T03:03:58 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[32m2021-04-18T03:04:06 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-04-18T03:04:09 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-04-18T03:04:13 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-04-18T03:04:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7000/22000, val/hateful_memes/cross_entropy: 2.7276, val/total_loss: 2.7276, val/hateful_memes/accuracy: 0.6093, val/hateful_memes/binary_f1: 0.2648, val/hateful_memes/roc_auc: 0.4963, num_updates: 7000, epoch: 53, iterations: 7000, max_updates: 22000, val_time: 15s 228ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.518926\n",
            "\u001b[32m2021-04-18T03:07:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7100/22000, train/hateful_memes/cross_entropy: 0.0033, train/hateful_memes/cross_entropy/avg: 0.0824, train/total_loss: 0.0033, train/total_loss/avg: 0.0824, max mem: 11929.0, experiment: run, epoch: 54, num_updates: 7100, iterations: 7100, max_updates: 22000, lr: 0.00004, ups: 0.54, time: 03m 04s 852ms, time_since_start: 03h 43m 32s 800ms, eta: 07h 43m 10s 963ms\n",
            "\u001b[32m2021-04-18T03:10:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7200/22000, train/hateful_memes/cross_entropy: 0.0033, train/hateful_memes/cross_entropy/avg: 0.0834, train/total_loss: 0.0033, train/total_loss/avg: 0.0834, max mem: 11929.0, experiment: run, epoch: 55, num_updates: 7200, iterations: 7200, max_updates: 22000, lr: 0.00004, ups: 0.55, time: 03m 03s 679ms, time_since_start: 03h 46m 36s 479ms, eta: 07h 37m 09s 173ms\n",
            "\u001b[32m2021-04-18T03:13:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7300/22000, train/hateful_memes/cross_entropy: 0.0029, train/hateful_memes/cross_entropy/avg: 0.0823, train/total_loss: 0.0029, train/total_loss/avg: 0.0823, max mem: 11929.0, experiment: run, epoch: 55, num_updates: 7300, iterations: 7300, max_updates: 22000, lr: 0.00004, ups: 0.54, time: 03m 04s 420ms, time_since_start: 03h 49m 40s 900ms, eta: 07h 35m 53s 853ms\n",
            "\u001b[32m2021-04-18T03:16:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7400/22000, train/hateful_memes/cross_entropy: 0.0033, train/hateful_memes/cross_entropy/avg: 0.0819, train/total_loss: 0.0033, train/total_loss/avg: 0.0819, max mem: 11929.0, experiment: run, epoch: 56, num_updates: 7400, iterations: 7400, max_updates: 22000, lr: 0.00004, ups: 0.54, time: 03m 04s 238ms, time_since_start: 03h 52m 45s 138ms, eta: 07h 32m 20s 935ms\n",
            "\u001b[32m2021-04-18T03:19:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7500/22000, train/hateful_memes/cross_entropy: 0.0033, train/hateful_memes/cross_entropy/avg: 0.0809, train/total_loss: 0.0033, train/total_loss/avg: 0.0809, max mem: 11929.0, experiment: run, epoch: 57, num_updates: 7500, iterations: 7500, max_updates: 22000, lr: 0.00004, ups: 0.54, time: 03m 04s 274ms, time_since_start: 03h 55m 49s 413ms, eta: 07h 29m 20s 356ms\n",
            "\u001b[32m2021-04-18T03:22:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7600/22000, train/hateful_memes/cross_entropy: 0.0033, train/hateful_memes/cross_entropy/avg: 0.0799, train/total_loss: 0.0033, train/total_loss/avg: 0.0799, max mem: 11929.0, experiment: run, epoch: 58, num_updates: 7600, iterations: 7600, max_updates: 22000, lr: 0.00004, ups: 0.54, time: 03m 04s 007ms, time_since_start: 03h 58m 53s 421ms, eta: 07h 25m 35s 518ms\n",
            "\u001b[32m2021-04-18T03:25:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7700/22000, train/hateful_memes/cross_entropy: 0.0037, train/hateful_memes/cross_entropy/avg: 0.0790, train/total_loss: 0.0037, train/total_loss/avg: 0.0790, max mem: 11929.0, experiment: run, epoch: 58, num_updates: 7700, iterations: 7700, max_updates: 22000, lr: 0.00004, ups: 0.55, time: 03m 03s 625ms, time_since_start: 04h 01m 57s 046ms, eta: 07h 21m 34s 783ms\n",
            "\u001b[32m2021-04-18T03:28:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7800/22000, train/hateful_memes/cross_entropy: 0.0033, train/hateful_memes/cross_entropy/avg: 0.0780, train/total_loss: 0.0033, train/total_loss/avg: 0.0780, max mem: 11929.0, experiment: run, epoch: 59, num_updates: 7800, iterations: 7800, max_updates: 22000, lr: 0.00004, ups: 0.55, time: 03m 03s 339ms, time_since_start: 04h 05m 386ms, eta: 07h 17m 48s 552ms\n",
            "\u001b[32m2021-04-18T03:31:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7900/22000, train/hateful_memes/cross_entropy: 0.0033, train/hateful_memes/cross_entropy/avg: 0.0771, train/total_loss: 0.0033, train/total_loss/avg: 0.0771, max mem: 11929.0, experiment: run, epoch: 60, num_updates: 7900, iterations: 7900, max_updates: 22000, lr: 0.00004, ups: 0.54, time: 03m 04s 547ms, time_since_start: 04h 08m 04s 933ms, eta: 07h 17m 35s 326ms\n",
            "\u001b[32m2021-04-18T03:34:54 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[32m2021-04-18T03:34:54 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-04-18T03:34:57 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-04-18T03:35:00 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-04-18T03:35:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8000/22000, train/hateful_memes/cross_entropy: 0.0033, train/hateful_memes/cross_entropy/avg: 0.0761, train/total_loss: 0.0033, train/total_loss/avg: 0.0761, max mem: 11929.0, experiment: run, epoch: 61, num_updates: 8000, iterations: 8000, max_updates: 22000, lr: 0.00003, ups: 0.53, time: 03m 10s 375ms, time_since_start: 04h 11m 15s 309ms, eta: 07h 28m 12s 481ms\n",
            "\u001b[32m2021-04-18T03:35:00 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2021-04-18T03:35:00 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[32m2021-04-18T03:35:09 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-04-18T03:35:12 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-04-18T03:35:15 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-04-18T03:35:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8000/22000, val/hateful_memes/cross_entropy: 2.7457, val/total_loss: 2.7457, val/hateful_memes/accuracy: 0.5870, val/hateful_memes/binary_f1: 0.2591, val/hateful_memes/roc_auc: 0.4606, num_updates: 8000, epoch: 61, iterations: 8000, max_updates: 22000, val_time: 14s 737ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.518926\n",
            "\u001b[32m2021-04-18T03:38:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8100/22000, train/hateful_memes/cross_entropy: 0.0037, train/hateful_memes/cross_entropy/avg: 0.0753, train/total_loss: 0.0037, train/total_loss/avg: 0.0753, max mem: 11929.0, experiment: run, epoch: 61, num_updates: 8100, iterations: 8100, max_updates: 22000, lr: 0.00003, ups: 0.54, time: 03m 04s 509ms, time_since_start: 04h 14m 34s 561ms, eta: 07h 11m 17s 638ms\n",
            "\u001b[32m2021-04-18T03:41:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8200/22000, train/hateful_memes/cross_entropy: 0.0033, train/hateful_memes/cross_entropy/avg: 0.0744, train/total_loss: 0.0033, train/total_loss/avg: 0.0744, max mem: 11929.0, experiment: run, epoch: 62, num_updates: 8200, iterations: 8200, max_updates: 22000, lr: 0.00003, ups: 0.55, time: 03m 03s 192ms, time_since_start: 04h 17m 37s 754ms, eta: 07h 05m 08s 144ms\n",
            "\u001b[32m2021-04-18T03:44:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8300/22000, train/hateful_memes/cross_entropy: 0.0033, train/hateful_memes/cross_entropy/avg: 0.0736, train/total_loss: 0.0033, train/total_loss/avg: 0.0736, max mem: 11929.0, experiment: run, epoch: 63, num_updates: 8300, iterations: 8300, max_updates: 22000, lr: 0.00003, ups: 0.54, time: 03m 04s 484ms, time_since_start: 04h 20m 42s 239ms, eta: 07h 05m 01s 864ms\n",
            "\u001b[32m2021-04-18T03:47:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8400/22000, train/hateful_memes/cross_entropy: 0.0023, train/hateful_memes/cross_entropy/avg: 0.0727, train/total_loss: 0.0023, train/total_loss/avg: 0.0727, max mem: 11929.0, experiment: run, epoch: 64, num_updates: 8400, iterations: 8400, max_updates: 22000, lr: 0.00003, ups: 0.54, time: 03m 04s 069ms, time_since_start: 04h 23m 46s 308ms, eta: 07h 58s 715ms\n",
            "\u001b[32m2021-04-18T03:50:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8500/22000, train/hateful_memes/cross_entropy: 0.0023, train/hateful_memes/cross_entropy/avg: 0.0719, train/total_loss: 0.0023, train/total_loss/avg: 0.0719, max mem: 11929.0, experiment: run, epoch: 64, num_updates: 8500, iterations: 8500, max_updates: 22000, lr: 0.00003, ups: 0.54, time: 03m 04s 293ms, time_since_start: 04h 26m 50s 602ms, eta: 06h 58m 23s 578ms\n",
            "\u001b[32m2021-04-18T03:53:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8600/22000, train/hateful_memes/cross_entropy: 0.0023, train/hateful_memes/cross_entropy/avg: 0.0721, train/total_loss: 0.0023, train/total_loss/avg: 0.0721, max mem: 11929.0, experiment: run, epoch: 65, num_updates: 8600, iterations: 8600, max_updates: 22000, lr: 0.00003, ups: 0.55, time: 03m 03s 642ms, time_since_start: 04h 29m 54s 244ms, eta: 06h 53m 49s 527ms\n",
            "\u001b[32m2021-04-18T03:56:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8700/22000, train/hateful_memes/cross_entropy: 0.0023, train/hateful_memes/cross_entropy/avg: 0.0713, train/total_loss: 0.0023, train/total_loss/avg: 0.0713, max mem: 11929.0, experiment: run, epoch: 66, num_updates: 8700, iterations: 8700, max_updates: 22000, lr: 0.00003, ups: 0.54, time: 03m 04s 279ms, time_since_start: 04h 32m 58s 523ms, eta: 06h 52m 09s 734ms\n",
            "\u001b[32m2021-04-18T03:59:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8800/22000, train/hateful_memes/cross_entropy: 0.0023, train/hateful_memes/cross_entropy/avg: 0.0705, train/total_loss: 0.0023, train/total_loss/avg: 0.0705, max mem: 11929.0, experiment: run, epoch: 67, num_updates: 8800, iterations: 8800, max_updates: 22000, lr: 0.00003, ups: 0.55, time: 03m 03s 404ms, time_since_start: 04h 36m 01s 928ms, eta: 06h 47m 07s 269ms\n",
            "\u001b[32m2021-04-18T04:02:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8900/22000, train/hateful_memes/cross_entropy: 0.0023, train/hateful_memes/cross_entropy/avg: 0.0698, train/total_loss: 0.0023, train/total_loss/avg: 0.0698, max mem: 11929.0, experiment: run, epoch: 67, num_updates: 8900, iterations: 8900, max_updates: 22000, lr: 0.00003, ups: 0.54, time: 03m 04s 137ms, time_since_start: 04h 39m 06s 065ms, eta: 06h 45m 39s 097ms\n",
            "\u001b[32m2021-04-18T04:05:55 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[32m2021-04-18T04:05:55 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-04-18T04:05:58 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-04-18T04:06:01 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-04-18T04:06:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9000/22000, train/hateful_memes/cross_entropy: 0.0039, train/hateful_memes/cross_entropy/avg: 0.0692, train/total_loss: 0.0039, train/total_loss/avg: 0.0692, max mem: 11929.0, experiment: run, epoch: 68, num_updates: 9000, iterations: 9000, max_updates: 22000, lr: 0.00003, ups: 0.53, time: 03m 10s 148ms, time_since_start: 04h 42m 16s 214ms, eta: 06h 55m 41s 821ms\n",
            "\u001b[32m2021-04-18T04:06:01 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2021-04-18T04:06:01 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[32m2021-04-18T04:06:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9000/22000, val/hateful_memes/cross_entropy: 2.4911, val/total_loss: 2.4911, val/hateful_memes/accuracy: 0.5648, val/hateful_memes/binary_f1: 0.2633, val/hateful_memes/roc_auc: 0.4957, num_updates: 9000, epoch: 68, iterations: 9000, max_updates: 22000, val_time: 08s 267ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.518926\n",
            "\u001b[32m2021-04-18T04:09:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9100/22000, train/hateful_memes/cross_entropy: 0.0053, train/hateful_memes/cross_entropy/avg: 0.0688, train/total_loss: 0.0053, train/total_loss/avg: 0.0688, max mem: 11929.0, experiment: run, epoch: 69, num_updates: 9100, iterations: 9100, max_updates: 22000, lr: 0.00003, ups: 0.55, time: 03m 03s 560ms, time_since_start: 04h 45m 28s 046ms, eta: 06h 38m 12s 428ms\n",
            "\u001b[32m2021-04-18T04:12:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9200/22000, train/hateful_memes/cross_entropy: 0.0045, train/hateful_memes/cross_entropy/avg: 0.0681, train/total_loss: 0.0045, train/total_loss/avg: 0.0681, max mem: 11929.0, experiment: run, epoch: 70, num_updates: 9200, iterations: 9200, max_updates: 22000, lr: 0.00003, ups: 0.54, time: 03m 04s 345ms, time_since_start: 04h 48m 32s 392ms, eta: 06h 36m 48s 645ms\n",
            "\u001b[32m2021-04-18T04:15:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9300/22000, train/hateful_memes/cross_entropy: 0.0045, train/hateful_memes/cross_entropy/avg: 0.0674, train/total_loss: 0.0045, train/total_loss/avg: 0.0674, max mem: 11929.0, experiment: run, epoch: 70, num_updates: 9300, iterations: 9300, max_updates: 22000, lr: 0.00003, ups: 0.54, time: 03m 04s 540ms, time_since_start: 04h 51m 36s 932ms, eta: 06h 34m 07s 610ms\n",
            "\u001b[32m2021-04-18T04:18:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9400/22000, train/hateful_memes/cross_entropy: 0.0039, train/hateful_memes/cross_entropy/avg: 0.0667, train/total_loss: 0.0039, train/total_loss/avg: 0.0667, max mem: 11929.0, experiment: run, epoch: 71, num_updates: 9400, iterations: 9400, max_updates: 22000, lr: 0.00003, ups: 0.55, time: 03m 03s 103ms, time_since_start: 04h 54m 40s 035ms, eta: 06h 27m 58s 631ms\n",
            "\u001b[32m2021-04-18T04:21:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9500/22000, train/hateful_memes/cross_entropy: 0.0024, train/hateful_memes/cross_entropy/avg: 0.0660, train/total_loss: 0.0024, train/total_loss/avg: 0.0660, max mem: 11929.0, experiment: run, epoch: 72, num_updates: 9500, iterations: 9500, max_updates: 22000, lr: 0.00003, ups: 0.54, time: 03m 04s 380ms, time_since_start: 04h 57m 44s 416ms, eta: 06h 27m 34s 949ms\n",
            "\u001b[32m2021-04-18T04:24:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9600/22000, train/hateful_memes/cross_entropy: 0.0024, train/hateful_memes/cross_entropy/avg: 0.0653, train/total_loss: 0.0024, train/total_loss/avg: 0.0653, max mem: 11929.0, experiment: run, epoch: 73, num_updates: 9600, iterations: 9600, max_updates: 22000, lr: 0.00003, ups: 0.55, time: 03m 03s 719ms, time_since_start: 05h 48s 135ms, eta: 06h 23m 06s 216ms\n",
            "\u001b[32m2021-04-18T04:27:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9700/22000, train/hateful_memes/cross_entropy: 0.0024, train/hateful_memes/cross_entropy/avg: 0.0651, train/total_loss: 0.0024, train/total_loss/avg: 0.0651, max mem: 11929.0, experiment: run, epoch: 73, num_updates: 9700, iterations: 9700, max_updates: 22000, lr: 0.00003, ups: 0.54, time: 03m 04s 015ms, time_since_start: 05h 03m 52s 150ms, eta: 06h 20m 37s 614ms\n",
            "\u001b[32m2021-04-18T04:30:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9800/22000, train/hateful_memes/cross_entropy: 0.0024, train/hateful_memes/cross_entropy/avg: 0.0645, train/total_loss: 0.0024, train/total_loss/avg: 0.0645, max mem: 11929.0, experiment: run, epoch: 74, num_updates: 9800, iterations: 9800, max_updates: 22000, lr: 0.00003, ups: 0.55, time: 03m 03s 541ms, time_since_start: 05h 06m 55s 692ms, eta: 06h 16m 33s 551ms\n",
            "\u001b[32m2021-04-18T04:33:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9900/22000, train/hateful_memes/cross_entropy: 0.0024, train/hateful_memes/cross_entropy/avg: 0.0638, train/total_loss: 0.0024, train/total_loss/avg: 0.0638, max mem: 11929.0, experiment: run, epoch: 75, num_updates: 9900, iterations: 9900, max_updates: 22000, lr: 0.00003, ups: 0.55, time: 03m 03s 956ms, time_since_start: 05h 09m 59s 648ms, eta: 06h 14m 19s 015ms\n",
            "\u001b[32m2021-04-18T04:36:49 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[32m2021-04-18T04:36:49 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-04-18T04:36:52 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-04-18T04:36:56 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-04-18T04:36:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10000/22000, train/hateful_memes/cross_entropy: 0.0024, train/hateful_memes/cross_entropy/avg: 0.0635, train/total_loss: 0.0024, train/total_loss/avg: 0.0635, max mem: 11929.0, experiment: run, epoch: 76, num_updates: 10000, iterations: 10000, max_updates: 22000, lr: 0.00003, ups: 0.53, time: 03m 10s 879ms, time_since_start: 05h 13m 10s 527ms, eta: 06h 25m 11s 690ms\n",
            "\u001b[32m2021-04-18T04:36:56 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2021-04-18T04:36:56 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[32m2021-04-18T04:37:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10000/22000, val/hateful_memes/cross_entropy: 2.8633, val/total_loss: 2.8633, val/hateful_memes/accuracy: 0.5722, val/hateful_memes/binary_f1: 0.2848, val/hateful_memes/roc_auc: 0.4817, num_updates: 10000, epoch: 76, iterations: 10000, max_updates: 22000, val_time: 08s 297ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.518926\n",
            "\u001b[32m2021-04-18T04:40:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10100/22000, train/hateful_memes/cross_entropy: 0.0024, train/hateful_memes/cross_entropy/avg: 0.0629, train/total_loss: 0.0024, train/total_loss/avg: 0.0629, max mem: 11929.0, experiment: run, epoch: 76, num_updates: 10100, iterations: 10100, max_updates: 22000, lr: 0.00003, ups: 0.54, time: 03m 04s 893ms, time_since_start: 05h 16m 23s 725ms, eta: 06h 10m 394ms\n",
            "\u001b[32m2021-04-18T04:43:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10200/22000, train/hateful_memes/cross_entropy: 0.0024, train/hateful_memes/cross_entropy/avg: 0.0625, train/total_loss: 0.0024, train/total_loss/avg: 0.0625, max mem: 11929.0, experiment: run, epoch: 77, num_updates: 10200, iterations: 10200, max_updates: 22000, lr: 0.00003, ups: 0.55, time: 03m 03s 572ms, time_since_start: 05h 19m 27s 297ms, eta: 06h 04m 16s 503ms\n",
            "\u001b[32m2021-04-18T04:46:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10300/22000, train/hateful_memes/cross_entropy: 0.0024, train/hateful_memes/cross_entropy/avg: 0.0619, train/total_loss: 0.0024, train/total_loss/avg: 0.0619, max mem: 11929.0, experiment: run, epoch: 78, num_updates: 10300, iterations: 10300, max_updates: 22000, lr: 0.00003, ups: 0.55, time: 03m 03s 753ms, time_since_start: 05h 22m 31s 050ms, eta: 06h 01m 32s 605ms\n",
            "\u001b[32m2021-04-18T04:49:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10400/22000, train/hateful_memes/cross_entropy: 0.0024, train/hateful_memes/cross_entropy/avg: 0.0614, train/total_loss: 0.0024, train/total_loss/avg: 0.0614, max mem: 11929.0, experiment: run, epoch: 79, num_updates: 10400, iterations: 10400, max_updates: 22000, lr: 0.00003, ups: 0.54, time: 03m 04s 100ms, time_since_start: 05h 25m 35s 151ms, eta: 05h 59m 07s 911ms\n",
            "\u001b[32m2021-04-18T04:52:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10500/22000, train/hateful_memes/cross_entropy: 0.0024, train/hateful_memes/cross_entropy/avg: 0.0608, train/total_loss: 0.0024, train/total_loss/avg: 0.0608, max mem: 11929.0, experiment: run, epoch: 79, num_updates: 10500, iterations: 10500, max_updates: 22000, lr: 0.00003, ups: 0.54, time: 03m 04s 019ms, time_since_start: 05h 28m 39s 171ms, eta: 05h 55m 52s 720ms\n",
            "\u001b[32m2021-04-18T04:55:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10600/22000, train/hateful_memes/cross_entropy: 0.0024, train/hateful_memes/cross_entropy/avg: 0.0602, train/total_loss: 0.0024, train/total_loss/avg: 0.0602, max mem: 11929.0, experiment: run, epoch: 80, num_updates: 10600, iterations: 10600, max_updates: 22000, lr: 0.00003, ups: 0.55, time: 03m 03s 147ms, time_since_start: 05h 31m 42s 319ms, eta: 05h 51m 06s 781ms\n",
            "\u001b[32m2021-04-18T04:58:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10700/22000, train/hateful_memes/cross_entropy: 0.0024, train/hateful_memes/cross_entropy/avg: 0.0597, train/total_loss: 0.0024, train/total_loss/avg: 0.0597, max mem: 11929.0, experiment: run, epoch: 81, num_updates: 10700, iterations: 10700, max_updates: 22000, lr: 0.00003, ups: 0.54, time: 03m 04s 537ms, time_since_start: 05h 34m 46s 857ms, eta: 05h 50m 40s 461ms\n",
            "\u001b[32m2021-04-18T05:01:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10800/22000, train/hateful_memes/cross_entropy: 0.0024, train/hateful_memes/cross_entropy/avg: 0.0591, train/total_loss: 0.0024, train/total_loss/avg: 0.0591, max mem: 11929.0, experiment: run, epoch: 82, num_updates: 10800, iterations: 10800, max_updates: 22000, lr: 0.00003, ups: 0.54, time: 03m 04s 891ms, time_since_start: 05h 37m 51s 748ms, eta: 05h 48m 14s 164ms\n",
            "\u001b[32m2021-04-18T05:04:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10900/22000, train/hateful_memes/cross_entropy: 0.0024, train/hateful_memes/cross_entropy/avg: 0.0587, train/total_loss: 0.0024, train/total_loss/avg: 0.0587, max mem: 11929.0, experiment: run, epoch: 82, num_updates: 10900, iterations: 10900, max_updates: 22000, lr: 0.00003, ups: 0.54, time: 03m 04s 294ms, time_since_start: 05h 40m 56s 043ms, eta: 05h 44m 851ms\n",
            "\u001b[32m2021-04-18T05:07:45 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[32m2021-04-18T05:07:45 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-04-18T05:07:47 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-04-18T05:07:51 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-04-18T05:07:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11000/22000, train/hateful_memes/cross_entropy: 0.0024, train/hateful_memes/cross_entropy/avg: 0.0582, train/total_loss: 0.0024, train/total_loss/avg: 0.0582, max mem: 11929.0, experiment: run, epoch: 83, num_updates: 11000, iterations: 11000, max_updates: 22000, lr: 0.00003, ups: 0.53, time: 03m 09s 634ms, time_since_start: 05h 44m 05s 677ms, eta: 05h 50m 47s 491ms\n",
            "\u001b[32m2021-04-18T05:07:51 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2021-04-18T05:07:51 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[32m2021-04-18T05:07:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11000/22000, val/hateful_memes/cross_entropy: 2.7871, val/total_loss: 2.7871, val/hateful_memes/accuracy: 0.5815, val/hateful_memes/binary_f1: 0.2803, val/hateful_memes/roc_auc: 0.4825, num_updates: 11000, epoch: 83, iterations: 11000, max_updates: 22000, val_time: 08s 331ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.518926\n",
            "\u001b[32m2021-04-18T05:11:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11100/22000, train/hateful_memes/cross_entropy: 0.0022, train/hateful_memes/cross_entropy/avg: 0.0577, train/total_loss: 0.0022, train/total_loss/avg: 0.0577, max mem: 11929.0, experiment: run, epoch: 84, num_updates: 11100, iterations: 11100, max_updates: 22000, lr: 0.00003, ups: 0.54, time: 03m 05s 275ms, time_since_start: 05h 47m 19s 289ms, eta: 05h 39m 36s 804ms\n",
            "\u001b[32m2021-04-18T05:14:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11200/22000, train/hateful_memes/cross_entropy: 0.0018, train/hateful_memes/cross_entropy/avg: 0.0571, train/total_loss: 0.0018, train/total_loss/avg: 0.0571, max mem: 11929.0, experiment: run, epoch: 85, num_updates: 11200, iterations: 11200, max_updates: 22000, lr: 0.00003, ups: 0.54, time: 03m 04s 173ms, time_since_start: 05h 50m 23s 463ms, eta: 05h 34m 29s 787ms\n",
            "\u001b[32m2021-04-18T05:17:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11300/22000, train/hateful_memes/cross_entropy: 0.0017, train/hateful_memes/cross_entropy/avg: 0.0566, train/total_loss: 0.0017, train/total_loss/avg: 0.0566, max mem: 11929.0, experiment: run, epoch: 85, num_updates: 11300, iterations: 11300, max_updates: 22000, lr: 0.00003, ups: 0.54, time: 03m 04s 822ms, time_since_start: 05h 53m 28s 285ms, eta: 05h 32m 34s 006ms\n",
            "\u001b[32m2021-04-18T05:20:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11400/22000, train/hateful_memes/cross_entropy: 0.0017, train/hateful_memes/cross_entropy/avg: 0.0561, train/total_loss: 0.0017, train/total_loss/avg: 0.0561, max mem: 11929.0, experiment: run, epoch: 86, num_updates: 11400, iterations: 11400, max_updates: 22000, lr: 0.00003, ups: 0.54, time: 03m 05s 078ms, time_since_start: 05h 56m 33s 364ms, eta: 05h 29m 54s 865ms\n",
            "\u001b[32m2021-04-18T05:23:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11500/22000, train/hateful_memes/cross_entropy: 0.0012, train/hateful_memes/cross_entropy/avg: 0.0557, train/total_loss: 0.0012, train/total_loss/avg: 0.0557, max mem: 11929.0, experiment: run, epoch: 87, num_updates: 11500, iterations: 11500, max_updates: 22000, lr: 0.00003, ups: 0.54, time: 03m 04s 329ms, time_since_start: 05h 59m 37s 693ms, eta: 05h 25m 28s 767ms\n",
            "\u001b[32m2021-04-18T05:26:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11600/22000, train/hateful_memes/cross_entropy: 0.0011, train/hateful_memes/cross_entropy/avg: 0.0552, train/total_loss: 0.0011, train/total_loss/avg: 0.0552, max mem: 11929.0, experiment: run, epoch: 88, num_updates: 11600, iterations: 11600, max_updates: 22000, lr: 0.00003, ups: 0.54, time: 03m 04s 797ms, time_since_start: 06h 02m 42s 491ms, eta: 05h 23m 11s 913ms\n",
            "\u001b[32m2021-04-18T05:29:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11700/22000, train/hateful_memes/cross_entropy: 0.0011, train/hateful_memes/cross_entropy/avg: 0.0553, train/total_loss: 0.0011, train/total_loss/avg: 0.0553, max mem: 11929.0, experiment: run, epoch: 88, num_updates: 11700, iterations: 11700, max_updates: 22000, lr: 0.00003, ups: 0.54, time: 03m 05s 817ms, time_since_start: 06h 05m 48s 308ms, eta: 05h 21m 51s 488ms\n",
            "\u001b[32m2021-04-18T05:32:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11800/22000, train/hateful_memes/cross_entropy: 0.0012, train/hateful_memes/cross_entropy/avg: 0.0559, train/total_loss: 0.0012, train/total_loss/avg: 0.0559, max mem: 11929.0, experiment: run, epoch: 89, num_updates: 11800, iterations: 11800, max_updates: 22000, lr: 0.00003, ups: 0.54, time: 03m 05s 395ms, time_since_start: 06h 08m 53s 704ms, eta: 05h 18m 560ms\n",
            "\u001b[32m2021-04-18T05:35:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11900/22000, train/hateful_memes/cross_entropy: 0.0011, train/hateful_memes/cross_entropy/avg: 0.0554, train/total_loss: 0.0011, train/total_loss/avg: 0.0554, max mem: 11929.0, experiment: run, epoch: 90, num_updates: 11900, iterations: 11900, max_updates: 22000, lr: 0.00003, ups: 0.54, time: 03m 04s 126ms, time_since_start: 06h 11m 57s 831ms, eta: 05h 12m 44s 168ms\n",
            "\u001b[32m2021-04-18T05:38:48 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[32m2021-04-18T05:38:48 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-04-18T05:38:51 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-04-18T05:38:54 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-04-18T05:38:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12000/22000, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.0550, train/total_loss: 0.0007, train/total_loss/avg: 0.0550, max mem: 11929.0, experiment: run, epoch: 91, num_updates: 12000, iterations: 12000, max_updates: 22000, lr: 0.00003, ups: 0.52, time: 03m 11s 062ms, time_since_start: 06h 15m 08s 893ms, eta: 05h 21m 18s 185ms\n",
            "\u001b[32m2021-04-18T05:38:54 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2021-04-18T05:38:54 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[32m2021-04-18T05:39:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12000/22000, val/hateful_memes/cross_entropy: 2.4587, val/total_loss: 2.4587, val/hateful_memes/accuracy: 0.5963, val/hateful_memes/binary_f1: 0.2585, val/hateful_memes/roc_auc: 0.4709, num_updates: 12000, epoch: 91, iterations: 12000, max_updates: 22000, val_time: 08s 521ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.518926\n",
            "\u001b[32m2021-04-18T05:42:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12100/22000, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.0545, train/total_loss: 0.0007, train/total_loss/avg: 0.0545, max mem: 11929.0, experiment: run, epoch: 91, num_updates: 12100, iterations: 12100, max_updates: 22000, lr: 0.00002, ups: 0.54, time: 03m 05s 848ms, time_since_start: 06h 18m 23s 268ms, eta: 05h 09m 24s 568ms\n",
            "\u001b[32m2021-04-18T05:45:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12200/22000, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.0541, train/total_loss: 0.0007, train/total_loss/avg: 0.0541, max mem: 11929.0, experiment: run, epoch: 92, num_updates: 12200, iterations: 12200, max_updates: 22000, lr: 0.00002, ups: 0.54, time: 03m 04s 528ms, time_since_start: 06h 21m 27s 796ms, eta: 05h 04m 06s 541ms\n",
            "\u001b[32m2021-04-18T05:48:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12300/22000, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.0537, train/total_loss: 0.0006, train/total_loss/avg: 0.0537, max mem: 11929.0, experiment: run, epoch: 93, num_updates: 12300, iterations: 12300, max_updates: 22000, lr: 0.00002, ups: 0.54, time: 03m 05s 819ms, time_since_start: 06h 24m 33s 615ms, eta: 05h 03m 06s 700ms\n",
            "\u001b[32m2021-04-18T05:51:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12400/22000, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.0533, train/total_loss: 0.0006, train/total_loss/avg: 0.0533, max mem: 11929.0, experiment: run, epoch: 94, num_updates: 12400, iterations: 12400, max_updates: 22000, lr: 0.00002, ups: 0.54, time: 03m 05s 643ms, time_since_start: 06h 27m 39s 259ms, eta: 04h 59m 42s 162ms\n",
            "\u001b[32m2021-04-18T05:54:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12500/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0528, train/total_loss: 0.0003, train/total_loss/avg: 0.0528, max mem: 11929.0, experiment: run, epoch: 94, num_updates: 12500, iterations: 12500, max_updates: 22000, lr: 0.00002, ups: 0.54, time: 03m 04s 567ms, time_since_start: 06h 30m 43s 827ms, eta: 04h 54m 51s 734ms\n",
            "\u001b[32m2021-04-18T05:57:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12600/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0524, train/total_loss: 0.0003, train/total_loss/avg: 0.0524, max mem: 11929.0, experiment: run, epoch: 95, num_updates: 12600, iterations: 12600, max_updates: 22000, lr: 0.00002, ups: 0.54, time: 03m 05s 069ms, time_since_start: 06h 33m 48s 896ms, eta: 04h 52m 33s 080ms\n",
            "\u001b[32m2021-04-18T06:00:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12700/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0520, train/total_loss: 0.0003, train/total_loss/avg: 0.0520, max mem: 11929.0, experiment: run, epoch: 96, num_updates: 12700, iterations: 12700, max_updates: 22000, lr: 0.00002, ups: 0.54, time: 03m 04s 339ms, time_since_start: 06h 36m 53s 236ms, eta: 04h 48m 17s 900ms\n",
            "\u001b[32m2021-04-18T06:03:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12800/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0516, train/total_loss: 0.0003, train/total_loss/avg: 0.0516, max mem: 11929.0, experiment: run, epoch: 97, num_updates: 12800, iterations: 12800, max_updates: 22000, lr: 0.00002, ups: 0.54, time: 03m 04s 496ms, time_since_start: 06h 39m 57s 732ms, eta: 04h 45m 26s 457ms\n",
            "\u001b[32m2021-04-18T06:06:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12900/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0513, train/total_loss: 0.0003, train/total_loss/avg: 0.0513, max mem: 11929.0, experiment: run, epoch: 97, num_updates: 12900, iterations: 12900, max_updates: 22000, lr: 0.00002, ups: 0.54, time: 03m 04s 957ms, time_since_start: 06h 43m 02s 689ms, eta: 04h 43m 02s 569ms\n",
            "\u001b[32m2021-04-18T06:09:53 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[32m2021-04-18T06:09:53 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-04-18T06:09:56 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-04-18T06:09:59 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-04-18T06:09:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13000/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0511, train/total_loss: 0.0003, train/total_loss/avg: 0.0511, max mem: 11929.0, experiment: run, epoch: 98, num_updates: 13000, iterations: 13000, max_updates: 22000, lr: 0.00002, ups: 0.52, time: 03m 11s 449ms, time_since_start: 06h 46m 14s 139ms, eta: 04h 49m 45s 519ms\n",
            "\u001b[32m2021-04-18T06:09:59 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2021-04-18T06:09:59 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[32m2021-04-18T06:10:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13000/22000, val/hateful_memes/cross_entropy: 3.1204, val/total_loss: 3.1204, val/hateful_memes/accuracy: 0.5963, val/hateful_memes/binary_f1: 0.2585, val/hateful_memes/roc_auc: 0.4623, num_updates: 13000, epoch: 98, iterations: 13000, max_updates: 22000, val_time: 08s 283ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.518926\n",
            "\u001b[32m2021-04-18T06:13:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13100/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0507, train/total_loss: 0.0003, train/total_loss/avg: 0.0507, max mem: 11929.0, experiment: run, epoch: 99, num_updates: 13100, iterations: 13100, max_updates: 22000, lr: 0.00002, ups: 0.54, time: 03m 04s 160ms, time_since_start: 06h 49m 26s 588ms, eta: 04h 35m 37s 790ms\n",
            "\u001b[32m2021-04-18T06:16:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13200/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0504, train/total_loss: 0.0003, train/total_loss/avg: 0.0504, max mem: 11929.0, experiment: run, epoch: 100, num_updates: 13200, iterations: 13200, max_updates: 22000, lr: 0.00002, ups: 0.54, time: 03m 04s 642ms, time_since_start: 06h 52m 31s 230ms, eta: 04h 33m 14s 805ms\n",
            "\u001b[32m2021-04-18T06:19:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13300/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0501, train/total_loss: 0.0003, train/total_loss/avg: 0.0501, max mem: 11929.0, experiment: run, epoch: 100, num_updates: 13300, iterations: 13300, max_updates: 22000, lr: 0.00002, ups: 0.54, time: 03m 04s 083ms, time_since_start: 06h 55m 35s 314ms, eta: 04h 29m 19s 372ms\n",
            "\u001b[32m2021-04-18T06:22:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13400/22000, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.0498, train/total_loss: 0.0006, train/total_loss/avg: 0.0498, max mem: 11929.0, experiment: run, epoch: 101, num_updates: 13400, iterations: 13400, max_updates: 22000, lr: 0.00002, ups: 0.54, time: 03m 04s 575ms, time_since_start: 06h 58m 39s 889ms, eta: 04h 26m 56s 338ms\n",
            "\u001b[32m2021-04-18T06:25:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13500/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0494, train/total_loss: 0.0003, train/total_loss/avg: 0.0494, max mem: 11929.0, experiment: run, epoch: 102, num_updates: 13500, iterations: 13500, max_updates: 22000, lr: 0.00002, ups: 0.54, time: 03m 04s 544ms, time_since_start: 07h 01m 44s 433ms, eta: 04h 23m 47s 418ms\n",
            "\u001b[32m2021-04-18T06:28:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13600/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0490, train/total_loss: 0.0003, train/total_loss/avg: 0.0490, max mem: 11929.0, experiment: run, epoch: 103, num_updates: 13600, iterations: 13600, max_updates: 22000, lr: 0.00002, ups: 0.54, time: 03m 04s 691ms, time_since_start: 07h 04m 49s 125ms, eta: 04h 20m 53s 728ms\n",
            "\u001b[32m2021-04-18T06:31:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13700/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0487, train/total_loss: 0.0002, train/total_loss/avg: 0.0487, max mem: 11929.0, experiment: run, epoch: 104, num_updates: 13700, iterations: 13700, max_updates: 22000, lr: 0.00002, ups: 0.54, time: 03m 04s 523ms, time_since_start: 07h 07m 53s 648ms, eta: 04h 17m 33s 310ms\n",
            "\u001b[32m2021-04-18T06:34:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13800/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0483, train/total_loss: 0.0002, train/total_loss/avg: 0.0483, max mem: 11929.0, experiment: run, epoch: 104, num_updates: 13800, iterations: 13800, max_updates: 22000, lr: 0.00002, ups: 0.54, time: 03m 05s 057ms, time_since_start: 07h 10m 58s 706ms, eta: 04h 15m 11s 271ms\n",
            "\u001b[32m2021-04-18T06:37:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13900/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0480, train/total_loss: 0.0002, train/total_loss/avg: 0.0480, max mem: 11929.0, experiment: run, epoch: 105, num_updates: 13900, iterations: 13900, max_updates: 22000, lr: 0.00002, ups: 0.54, time: 03m 04s 405ms, time_since_start: 07h 14m 03s 112ms, eta: 04h 11m 11s 314ms\n",
            "\u001b[32m2021-04-18T06:40:52 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[32m2021-04-18T06:40:52 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-04-18T06:40:55 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-04-18T06:40:58 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-04-18T06:40:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14000/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0476, train/total_loss: 0.0002, train/total_loss/avg: 0.0476, max mem: 11929.0, experiment: run, epoch: 106, num_updates: 14000, iterations: 14000, max_updates: 22000, lr: 0.00002, ups: 0.53, time: 03m 10s 187ms, time_since_start: 07h 17m 13s 299ms, eta: 04h 15m 51s 956ms\n",
            "\u001b[32m2021-04-18T06:40:58 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2021-04-18T06:40:58 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[32m2021-04-18T06:41:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14000/22000, val/hateful_memes/cross_entropy: 3.4088, val/total_loss: 3.4088, val/hateful_memes/accuracy: 0.5889, val/hateful_memes/binary_f1: 0.2745, val/hateful_memes/roc_auc: 0.4705, num_updates: 14000, epoch: 106, iterations: 14000, max_updates: 22000, val_time: 08s 372ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.518926\n",
            "\u001b[32m2021-04-18T06:44:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14100/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0473, train/total_loss: 0.0002, train/total_loss/avg: 0.0473, max mem: 11929.0, experiment: run, epoch: 107, num_updates: 14100, iterations: 14100, max_updates: 22000, lr: 0.00002, ups: 0.54, time: 03m 05s 203ms, time_since_start: 07h 20m 26s 881ms, eta: 04h 06m 02s 739ms\n",
            "\u001b[32m2021-04-18T06:47:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14200/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0470, train/total_loss: 0.0002, train/total_loss/avg: 0.0470, max mem: 11929.0, experiment: run, epoch: 107, num_updates: 14200, iterations: 14200, max_updates: 22000, lr: 0.00002, ups: 0.54, time: 03m 04s 814ms, time_since_start: 07h 23m 31s 696ms, eta: 04h 02m 25s 279ms\n",
            "\u001b[32m2021-04-18T06:50:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14300/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0469, train/total_loss: 0.0002, train/total_loss/avg: 0.0469, max mem: 11929.0, experiment: run, epoch: 108, num_updates: 14300, iterations: 14300, max_updates: 22000, lr: 0.00002, ups: 0.54, time: 03m 04s 305ms, time_since_start: 07h 26m 36s 001ms, eta: 03h 58m 39s 239ms\n",
            "\u001b[32m2021-04-18T06:53:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14400/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0466, train/total_loss: 0.0002, train/total_loss/avg: 0.0466, max mem: 11929.0, experiment: run, epoch: 109, num_updates: 14400, iterations: 14400, max_updates: 22000, lr: 0.00002, ups: 0.54, time: 03m 04s 012ms, time_since_start: 07h 29m 40s 013ms, eta: 03h 55m 10s 779ms\n",
            "\u001b[32m2021-04-18T06:56:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14500/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0462, train/total_loss: 0.0002, train/total_loss/avg: 0.0462, max mem: 11929.0, experiment: run, epoch: 110, num_updates: 14500, iterations: 14500, max_updates: 22000, lr: 0.00002, ups: 0.54, time: 03m 04s 939ms, time_since_start: 07h 32m 44s 953ms, eta: 03h 53m 15s 307ms\n",
            "\u001b[32m2021-04-18T06:59:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14600/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0459, train/total_loss: 0.0003, train/total_loss/avg: 0.0459, max mem: 11929.0, experiment: run, epoch: 110, num_updates: 14600, iterations: 14600, max_updates: 22000, lr: 0.00002, ups: 0.54, time: 03m 04s 699ms, time_since_start: 07h 35m 49s 653ms, eta: 03h 49m 50s 800ms\n",
            "\u001b[32m2021-04-18T07:02:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14700/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0456, train/total_loss: 0.0003, train/total_loss/avg: 0.0456, max mem: 11929.0, experiment: run, epoch: 111, num_updates: 14700, iterations: 14700, max_updates: 22000, lr: 0.00002, ups: 0.54, time: 03m 05s 728ms, time_since_start: 07h 38m 55s 381ms, eta: 03h 48m 215ms\n",
            "\u001b[32m2021-04-18T07:05:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14800/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0453, train/total_loss: 0.0003, train/total_loss/avg: 0.0453, max mem: 11929.0, experiment: run, epoch: 112, num_updates: 14800, iterations: 14800, max_updates: 22000, lr: 0.00002, ups: 0.55, time: 03m 03s 988ms, time_since_start: 07h 41m 59s 370ms, eta: 03h 42m 46s 415ms\n",
            "\u001b[32m2021-04-18T07:08:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14900/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0450, train/total_loss: 0.0003, train/total_loss/avg: 0.0450, max mem: 11929.0, experiment: run, epoch: 113, num_updates: 14900, iterations: 14900, max_updates: 22000, lr: 0.00002, ups: 0.54, time: 03m 04s 754ms, time_since_start: 07h 45m 04s 125ms, eta: 03h 40m 35s 628ms\n",
            "\u001b[32m2021-04-18T07:11:54 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[32m2021-04-18T07:11:54 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-04-18T07:11:57 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-04-18T07:12:00 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-04-18T07:12:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15000/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0447, train/total_loss: 0.0003, train/total_loss/avg: 0.0447, max mem: 11929.0, experiment: run, epoch: 113, num_updates: 15000, iterations: 15000, max_updates: 22000, lr: 0.00002, ups: 0.52, time: 03m 11s 160ms, time_since_start: 07h 48m 15s 285ms, eta: 03h 45m 01s 654ms\n",
            "\u001b[32m2021-04-18T07:12:00 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2021-04-18T07:12:00 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[32m2021-04-18T07:12:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15000/22000, val/hateful_memes/cross_entropy: 3.3600, val/total_loss: 3.3600, val/hateful_memes/accuracy: 0.5519, val/hateful_memes/binary_f1: 0.3006, val/hateful_memes/roc_auc: 0.4641, num_updates: 15000, epoch: 113, iterations: 15000, max_updates: 22000, val_time: 08s 432ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.518926\n",
            "\u001b[32m2021-04-18T07:15:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15100/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0444, train/total_loss: 0.0002, train/total_loss/avg: 0.0444, max mem: 11929.0, experiment: run, epoch: 114, num_updates: 15100, iterations: 15100, max_updates: 22000, lr: 0.00002, ups: 0.54, time: 03m 05s 011ms, time_since_start: 07h 51m 28s 733ms, eta: 03h 34m 40s 708ms\n",
            "\u001b[32m2021-04-18T07:18:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15200/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0442, train/total_loss: 0.0002, train/total_loss/avg: 0.0442, max mem: 11929.0, experiment: run, epoch: 115, num_updates: 15200, iterations: 15200, max_updates: 22000, lr: 0.00002, ups: 0.54, time: 03m 05s 011ms, time_since_start: 07h 54m 33s 745ms, eta: 03h 31m 34s 017ms\n",
            "\u001b[32m2021-04-18T07:21:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15300/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0439, train/total_loss: 0.0003, train/total_loss/avg: 0.0439, max mem: 11929.0, experiment: run, epoch: 116, num_updates: 15300, iterations: 15300, max_updates: 22000, lr: 0.00002, ups: 0.54, time: 03m 04s 830ms, time_since_start: 07h 57m 38s 576ms, eta: 03h 28m 15s 126ms\n",
            "\u001b[32m2021-04-18T07:24:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15400/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0437, train/total_loss: 0.0003, train/total_loss/avg: 0.0437, max mem: 11929.0, experiment: run, epoch: 116, num_updates: 15400, iterations: 15400, max_updates: 22000, lr: 0.00002, ups: 0.54, time: 03m 05s 173ms, time_since_start: 08h 43s 749ms, eta: 03h 25m 31s 433ms\n",
            "\u001b[32m2021-04-18T07:27:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15500/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0434, train/total_loss: 0.0003, train/total_loss/avg: 0.0434, max mem: 11929.0, experiment: run, epoch: 117, num_updates: 15500, iterations: 15500, max_updates: 22000, lr: 0.00002, ups: 0.54, time: 03m 05s 705ms, time_since_start: 08h 03m 49s 455ms, eta: 03h 22m 59s 512ms\n",
            "\u001b[32m2021-04-18T07:30:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15600/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0431, train/total_loss: 0.0003, train/total_loss/avg: 0.0431, max mem: 11929.0, experiment: run, epoch: 118, num_updates: 15600, iterations: 15600, max_updates: 22000, lr: 0.00002, ups: 0.55, time: 03m 03s 832ms, time_since_start: 08h 06m 53s 288ms, eta: 03h 17m 51s 186ms\n",
            "\u001b[32m2021-04-18T07:33:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15700/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0428, train/total_loss: 0.0003, train/total_loss/avg: 0.0428, max mem: 11929.0, experiment: run, epoch: 119, num_updates: 15700, iterations: 15700, max_updates: 22000, lr: 0.00002, ups: 0.54, time: 03m 05s 203ms, time_since_start: 08h 09m 58s 491ms, eta: 03h 16m 12s 812ms\n",
            "\u001b[32m2021-04-18T07:36:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15800/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0426, train/total_loss: 0.0003, train/total_loss/avg: 0.0426, max mem: 11929.0, experiment: run, epoch: 119, num_updates: 15800, iterations: 15800, max_updates: 22000, lr: 0.00002, ups: 0.54, time: 03m 04s 622ms, time_since_start: 08h 13m 03s 113ms, eta: 03h 12m 29s 611ms\n",
            "\u001b[32m2021-04-18T07:39:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15900/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0423, train/total_loss: 0.0003, train/total_loss/avg: 0.0423, max mem: 11929.0, experiment: run, epoch: 120, num_updates: 15900, iterations: 15900, max_updates: 22000, lr: 0.00002, ups: 0.54, time: 03m 05s 667ms, time_since_start: 08h 16m 08s 781ms, eta: 03h 10m 27s 665ms\n",
            "\u001b[32m2021-04-18T07:42:58 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[32m2021-04-18T07:42:58 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-04-18T07:43:01 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-04-18T07:43:05 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-04-18T07:43:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16000/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0420, train/total_loss: 0.0003, train/total_loss/avg: 0.0420, max mem: 11929.0, experiment: run, epoch: 121, num_updates: 16000, iterations: 16000, max_updates: 22000, lr: 0.00002, ups: 0.53, time: 03m 10s 637ms, time_since_start: 08h 19m 19s 418ms, eta: 03h 12m 21s 177ms\n",
            "\u001b[32m2021-04-18T07:43:05 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2021-04-18T07:43:05 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[32m2021-04-18T07:43:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16000/22000, val/hateful_memes/cross_entropy: 3.6401, val/total_loss: 3.6401, val/hateful_memes/accuracy: 0.5759, val/hateful_memes/binary_f1: 0.2866, val/hateful_memes/roc_auc: 0.4718, num_updates: 16000, epoch: 121, iterations: 16000, max_updates: 22000, val_time: 08s 427ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.518926\n",
            "\u001b[32m2021-04-18T07:46:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16100/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0418, train/total_loss: 0.0001, train/total_loss/avg: 0.0418, max mem: 11929.0, experiment: run, epoch: 122, num_updates: 16100, iterations: 16100, max_updates: 22000, lr: 0.00001, ups: 0.54, time: 03m 05s 666ms, time_since_start: 08h 22m 33s 516ms, eta: 03h 04m 12s 938ms\n",
            "\u001b[32m2021-04-18T07:49:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16200/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0415, train/total_loss: 0.0001, train/total_loss/avg: 0.0415, max mem: 11929.0, experiment: run, epoch: 122, num_updates: 16200, iterations: 16200, max_updates: 22000, lr: 0.00001, ups: 0.54, time: 03m 05s 291ms, time_since_start: 08h 25m 38s 807ms, eta: 03h 43s 632ms\n",
            "\u001b[32m2021-04-18T07:52:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16300/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0412, train/total_loss: 0.0001, train/total_loss/avg: 0.0412, max mem: 11929.0, experiment: run, epoch: 123, num_updates: 16300, iterations: 16300, max_updates: 22000, lr: 0.00001, ups: 0.54, time: 03m 05s 608ms, time_since_start: 08h 28m 44s 416ms, eta: 02h 57m 54s 918ms\n",
            "\u001b[32m2021-04-18T07:55:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16400/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0410, train/total_loss: 0.0001, train/total_loss/avg: 0.0410, max mem: 11929.0, experiment: run, epoch: 124, num_updates: 16400, iterations: 16400, max_updates: 22000, lr: 0.00001, ups: 0.54, time: 03m 04s 815ms, time_since_start: 08h 31m 49s 232ms, eta: 02h 54m 02s 832ms\n",
            "\u001b[32m2021-04-18T07:58:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16500/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0407, train/total_loss: 0.0001, train/total_loss/avg: 0.0407, max mem: 11929.0, experiment: run, epoch: 125, num_updates: 16500, iterations: 16500, max_updates: 22000, lr: 0.00001, ups: 0.54, time: 03m 05s 147ms, time_since_start: 08h 34m 54s 379ms, eta: 02h 51m 14s 759ms\n",
            "\u001b[32m2021-04-18T08:01:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16600/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0405, train/total_loss: 0.0001, train/total_loss/avg: 0.0405, max mem: 11929.0, experiment: run, epoch: 125, num_updates: 16600, iterations: 16600, max_updates: 22000, lr: 0.00001, ups: 0.54, time: 03m 05s 231ms, time_since_start: 08h 37m 59s 611ms, eta: 02h 48m 12s 521ms\n",
            "\u001b[32m2021-04-18T08:04:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16700/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0403, train/total_loss: 0.0001, train/total_loss/avg: 0.0403, max mem: 11929.0, experiment: run, epoch: 126, num_updates: 16700, iterations: 16700, max_updates: 22000, lr: 0.00001, ups: 0.54, time: 03m 04s 267ms, time_since_start: 08h 41m 03s 878ms, eta: 02h 44m 14s 075ms\n",
            "\u001b[32m2021-04-18T08:07:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16800/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0400, train/total_loss: 0.0001, train/total_loss/avg: 0.0400, max mem: 11929.0, experiment: run, epoch: 127, num_updates: 16800, iterations: 16800, max_updates: 22000, lr: 0.00001, ups: 0.54, time: 03m 04s 754ms, time_since_start: 08h 44m 08s 633ms, eta: 02h 41m 33s 721ms\n",
            "\u001b[32m2021-04-18T08:10:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16900/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0398, train/total_loss: 0.0001, train/total_loss/avg: 0.0398, max mem: 11929.0, experiment: run, epoch: 128, num_updates: 16900, iterations: 16900, max_updates: 22000, lr: 0.00001, ups: 0.54, time: 03m 04s 643ms, time_since_start: 08h 47m 13s 277ms, eta: 02h 38m 21s 557ms\n",
            "\u001b[32m2021-04-18T08:14:04 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[32m2021-04-18T08:14:04 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-04-18T08:14:07 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-04-18T08:14:10 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-04-18T08:14:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17000/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0395, train/total_loss: 0.0001, train/total_loss/avg: 0.0395, max mem: 11929.0, experiment: run, epoch: 128, num_updates: 17000, iterations: 17000, max_updates: 22000, lr: 0.00001, ups: 0.52, time: 03m 11s 078ms, time_since_start: 08h 50m 24s 355ms, eta: 02h 40m 39s 896ms\n",
            "\u001b[32m2021-04-18T08:14:10 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2021-04-18T08:14:10 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[32m2021-04-18T08:14:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17000/22000, val/hateful_memes/cross_entropy: 3.6816, val/total_loss: 3.6816, val/hateful_memes/accuracy: 0.5889, val/hateful_memes/binary_f1: 0.2930, val/hateful_memes/roc_auc: 0.4653, num_updates: 17000, epoch: 128, iterations: 17000, max_updates: 22000, val_time: 08s 459ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.518926\n",
            "\u001b[32m2021-04-18T08:17:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17100/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0393, train/total_loss: 0.0001, train/total_loss/avg: 0.0393, max mem: 11929.0, experiment: run, epoch: 129, num_updates: 17100, iterations: 17100, max_updates: 22000, lr: 0.00001, ups: 0.54, time: 03m 04s 631ms, time_since_start: 08h 53m 37s 449ms, eta: 02h 32m 08s 347ms\n",
            "\u001b[32m2021-04-18T08:20:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17200/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0391, train/total_loss: 0.0001, train/total_loss/avg: 0.0391, max mem: 11929.0, experiment: run, epoch: 130, num_updates: 17200, iterations: 17200, max_updates: 22000, lr: 0.00001, ups: 0.54, time: 03m 04s 485ms, time_since_start: 08h 56m 41s 935ms, eta: 02h 28m 55s 013ms\n",
            "\u001b[32m2021-04-18T08:23:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17300/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0389, train/total_loss: 0.0001, train/total_loss/avg: 0.0389, max mem: 11929.0, experiment: run, epoch: 131, num_updates: 17300, iterations: 17300, max_updates: 22000, lr: 0.00001, ups: 0.54, time: 03m 04s 556ms, time_since_start: 08h 59m 46s 492ms, eta: 02h 25m 52s 242ms\n",
            "\u001b[32m2021-04-18T08:26:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17400/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0387, train/total_loss: 0.0001, train/total_loss/avg: 0.0387, max mem: 11929.0, experiment: run, epoch: 131, num_updates: 17400, iterations: 17400, max_updates: 22000, lr: 0.00001, ups: 0.54, time: 03m 04s 810ms, time_since_start: 09h 02m 51s 302ms, eta: 02h 22m 57s 783ms\n",
            "\u001b[32m2021-04-18T08:29:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17500/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0384, train/total_loss: 0.0001, train/total_loss/avg: 0.0384, max mem: 11929.0, experiment: run, epoch: 132, num_updates: 17500, iterations: 17500, max_updates: 22000, lr: 0.00001, ups: 0.54, time: 03m 04s 293ms, time_since_start: 09h 05m 55s 596ms, eta: 02h 19m 27s 848ms\n",
            "\u001b[32m2021-04-18T08:32:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17600/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0382, train/total_loss: 0.0000, train/total_loss/avg: 0.0382, max mem: 11929.0, experiment: run, epoch: 133, num_updates: 17600, iterations: 17600, max_updates: 22000, lr: 0.00001, ups: 0.54, time: 03m 04s 794ms, time_since_start: 09h 09m 391ms, eta: 02h 16m 44s 153ms\n",
            "\u001b[32m2021-04-18T08:35:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17700/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0380, train/total_loss: 0.0000, train/total_loss/avg: 0.0380, max mem: 11929.0, experiment: run, epoch: 134, num_updates: 17700, iterations: 17700, max_updates: 22000, lr: 0.00001, ups: 0.54, time: 03m 05s 151ms, time_since_start: 09h 12m 05s 542ms, eta: 02h 13m 53s 176ms\n",
            "\u001b[32m2021-04-18T08:38:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17800/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0378, train/total_loss: 0.0000, train/total_loss/avg: 0.0378, max mem: 11929.0, experiment: run, epoch: 134, num_updates: 17800, iterations: 17800, max_updates: 22000, lr: 0.00001, ups: 0.54, time: 03m 04s 855ms, time_since_start: 09h 15m 10s 398ms, eta: 02h 10m 33s 809ms\n",
            "\u001b[32m2021-04-18T08:42:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17900/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0376, train/total_loss: 0.0000, train/total_loss/avg: 0.0376, max mem: 11929.0, experiment: run, epoch: 135, num_updates: 17900, iterations: 17900, max_updates: 22000, lr: 0.00001, ups: 0.54, time: 03m 04s 850ms, time_since_start: 09h 18m 15s 249ms, eta: 02h 07m 27s 096ms\n",
            "\u001b[32m2021-04-18T08:45:05 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[32m2021-04-18T08:45:05 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-04-18T08:45:07 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-04-18T08:45:11 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-04-18T08:45:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18000/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0374, train/total_loss: 0.0000, train/total_loss/avg: 0.0374, max mem: 11929.0, experiment: run, epoch: 136, num_updates: 18000, iterations: 18000, max_updates: 22000, lr: 0.00001, ups: 0.53, time: 03m 10s 410ms, time_since_start: 09h 21m 25s 659ms, eta: 02h 08m 04s 972ms\n",
            "\u001b[32m2021-04-18T08:45:11 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2021-04-18T08:45:11 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[32m2021-04-18T08:45:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18000/22000, val/hateful_memes/cross_entropy: 3.6168, val/total_loss: 3.6168, val/hateful_memes/accuracy: 0.5685, val/hateful_memes/binary_f1: 0.2875, val/hateful_memes/roc_auc: 0.4692, num_updates: 18000, epoch: 136, iterations: 18000, max_updates: 22000, val_time: 08s 547ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.518926\n",
            "\u001b[32m2021-04-18T08:48:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18100/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0372, train/total_loss: 0.0000, train/total_loss/avg: 0.0372, max mem: 11929.0, experiment: run, epoch: 137, num_updates: 18100, iterations: 18100, max_updates: 22000, lr: 0.00001, ups: 0.54, time: 03m 05s 943ms, time_since_start: 09h 24m 40s 155ms, eta: 02h 01m 57s 051ms\n",
            "\u001b[32m2021-04-18T08:51:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18200/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0370, train/total_loss: 0.0000, train/total_loss/avg: 0.0370, max mem: 11929.0, experiment: run, epoch: 137, num_updates: 18200, iterations: 18200, max_updates: 22000, lr: 0.00001, ups: 0.54, time: 03m 05s 185ms, time_since_start: 09h 27m 45s 341ms, eta: 01h 58m 20s 390ms\n",
            "\u001b[32m2021-04-18T08:54:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18300/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0368, train/total_loss: 0.0000, train/total_loss/avg: 0.0368, max mem: 11929.0, experiment: run, epoch: 138, num_updates: 18300, iterations: 18300, max_updates: 22000, lr: 0.00001, ups: 0.54, time: 03m 05s 009ms, time_since_start: 09h 30m 50s 351ms, eta: 01h 55m 06s 974ms\n",
            "\u001b[32m2021-04-18T08:57:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18400/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0366, train/total_loss: 0.0000, train/total_loss/avg: 0.0366, max mem: 11929.0, experiment: run, epoch: 139, num_updates: 18400, iterations: 18400, max_updates: 22000, lr: 0.00001, ups: 0.54, time: 03m 04s 241ms, time_since_start: 09h 33m 54s 593ms, eta: 01h 51m 32s 393ms\n",
            "\u001b[32m2021-04-18T09:00:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18500/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0364, train/total_loss: 0.0000, train/total_loss/avg: 0.0364, max mem: 11929.0, experiment: run, epoch: 140, num_updates: 18500, iterations: 18500, max_updates: 22000, lr: 0.00001, ups: 0.54, time: 03m 04s 581ms, time_since_start: 09h 36m 59s 174ms, eta: 01h 48m 38s 483ms\n",
            "\u001b[32m2021-04-18T09:03:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18600/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0362, train/total_loss: 0.0001, train/total_loss/avg: 0.0362, max mem: 11929.0, experiment: run, epoch: 140, num_updates: 18600, iterations: 18600, max_updates: 22000, lr: 0.00001, ups: 0.54, time: 03m 04s 992ms, time_since_start: 09h 40m 04s 166ms, eta: 01h 45m 46s 344ms\n",
            "\u001b[32m2021-04-18T09:06:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18700/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0360, train/total_loss: 0.0001, train/total_loss/avg: 0.0360, max mem: 11929.0, experiment: run, epoch: 141, num_updates: 18700, iterations: 18700, max_updates: 22000, lr: 0.00001, ups: 0.54, time: 03m 04s 723ms, time_since_start: 09h 43m 08s 889ms, eta: 01h 42m 30s 730ms\n",
            "\u001b[32m2021-04-18T09:09:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18800/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0358, train/total_loss: 0.0001, train/total_loss/avg: 0.0358, max mem: 11929.0, experiment: run, epoch: 142, num_updates: 18800, iterations: 18800, max_updates: 22000, lr: 0.00001, ups: 0.54, time: 03m 04s 576ms, time_since_start: 09h 46m 13s 466ms, eta: 01h 39m 19s 615ms\n",
            "\u001b[32m2021-04-18T09:13:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18900/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0356, train/total_loss: 0.0001, train/total_loss/avg: 0.0356, max mem: 11929.0, experiment: run, epoch: 143, num_updates: 18900, iterations: 18900, max_updates: 22000, lr: 0.00001, ups: 0.54, time: 03m 04s 394ms, time_since_start: 09h 49m 17s 861ms, eta: 01h 36m 07s 675ms\n",
            "\u001b[32m2021-04-18T09:16:08 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[32m2021-04-18T09:16:08 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-04-18T09:16:10 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-04-18T09:16:14 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-04-18T09:16:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19000/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0354, train/total_loss: 0.0000, train/total_loss/avg: 0.0354, max mem: 11929.0, experiment: run, epoch: 143, num_updates: 19000, iterations: 19000, max_updates: 22000, lr: 0.00001, ups: 0.53, time: 03m 10s 574ms, time_since_start: 09h 52m 28s 435ms, eta: 01h 36m 08s 676ms\n",
            "\u001b[32m2021-04-18T09:16:14 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2021-04-18T09:16:14 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[32m2021-04-18T09:16:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19000/22000, val/hateful_memes/cross_entropy: 3.3587, val/total_loss: 3.3587, val/hateful_memes/accuracy: 0.5593, val/hateful_memes/binary_f1: 0.2654, val/hateful_memes/roc_auc: 0.4697, num_updates: 19000, epoch: 143, iterations: 19000, max_updates: 22000, val_time: 08s 407ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.518926\n",
            "\u001b[32m2021-04-18T09:19:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19100/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0352, train/total_loss: 0.0000, train/total_loss/avg: 0.0352, max mem: 11929.0, experiment: run, epoch: 144, num_updates: 19100, iterations: 19100, max_updates: 22000, lr: 0.00001, ups: 0.54, time: 03m 05s 867ms, time_since_start: 09h 55m 42s 714ms, eta: 01h 30m 38s 675ms\n",
            "\u001b[32m2021-04-18T09:22:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19200/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0350, train/total_loss: 0.0000, train/total_loss/avg: 0.0350, max mem: 11929.0, experiment: run, epoch: 145, num_updates: 19200, iterations: 19200, max_updates: 22000, lr: 0.00001, ups: 0.54, time: 03m 04s 230ms, time_since_start: 09h 58m 46s 945ms, eta: 01h 26m 44s 876ms\n",
            "\u001b[32m2021-04-18T09:25:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19300/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0349, train/total_loss: 0.0000, train/total_loss/avg: 0.0349, max mem: 11929.0, experiment: run, epoch: 146, num_updates: 19300, iterations: 19300, max_updates: 22000, lr: 0.00001, ups: 0.54, time: 03m 04s 374ms, time_since_start: 10h 01m 51s 319ms, eta: 01h 23m 42s 917ms\n",
            "\u001b[32m2021-04-18T09:28:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19400/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0347, train/total_loss: 0.0000, train/total_loss/avg: 0.0347, max mem: 11929.0, experiment: run, epoch: 146, num_updates: 19400, iterations: 19400, max_updates: 22000, lr: 0.00001, ups: 0.54, time: 03m 04s 583ms, time_since_start: 10h 04m 55s 903ms, eta: 01h 20m 42s 361ms\n",
            "\u001b[32m2021-04-18T09:31:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19500/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0345, train/total_loss: 0.0000, train/total_loss/avg: 0.0345, max mem: 11929.0, experiment: run, epoch: 147, num_updates: 19500, iterations: 19500, max_updates: 22000, lr: 0.00001, ups: 0.54, time: 03m 04s 334ms, time_since_start: 10h 08m 237ms, eta: 01h 17m 29s 839ms\n",
            "\u001b[32m2021-04-18T09:34:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19600/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0343, train/total_loss: 0.0000, train/total_loss/avg: 0.0343, max mem: 11929.0, experiment: run, epoch: 148, num_updates: 19600, iterations: 19600, max_updates: 22000, lr: 0.00001, ups: 0.54, time: 03m 04s 662ms, time_since_start: 10h 11m 04s 900ms, eta: 01h 14m 31s 791ms\n",
            "\u001b[32m2021-04-18T09:37:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19700/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0342, train/total_loss: 0.0000, train/total_loss/avg: 0.0342, max mem: 11929.0, experiment: run, epoch: 149, num_updates: 19700, iterations: 19700, max_updates: 22000, lr: 0.00001, ups: 0.55, time: 03m 03s 886ms, time_since_start: 10h 14m 08s 786ms, eta: 01h 11m 07s 454ms\n",
            "\u001b[32m2021-04-18T09:40:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19800/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0340, train/total_loss: 0.0000, train/total_loss/avg: 0.0340, max mem: 11929.0, experiment: run, epoch: 149, num_updates: 19800, iterations: 19800, max_updates: 22000, lr: 0.00001, ups: 0.54, time: 03m 05s 233ms, time_since_start: 10h 17m 14s 020ms, eta: 01h 08m 31s 810ms\n",
            "\u001b[32m2021-04-18T09:44:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19900/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0338, train/total_loss: 0.0000, train/total_loss/avg: 0.0338, max mem: 11929.0, experiment: run, epoch: 150, num_updates: 19900, iterations: 19900, max_updates: 22000, lr: 0.00001, ups: 0.54, time: 03m 04s 674ms, time_since_start: 10h 20m 18s 694ms, eta: 01h 05m 13s 067ms\n",
            "\u001b[32m2021-04-18T09:47:08 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[32m2021-04-18T09:47:08 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-04-18T09:47:11 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-04-18T09:47:14 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-04-18T09:47:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20000/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0336, train/total_loss: 0.0000, train/total_loss/avg: 0.0336, max mem: 11929.0, experiment: run, epoch: 151, num_updates: 20000, iterations: 20000, max_updates: 22000, lr: 0.00001, ups: 0.53, time: 03m 09s 739ms, time_since_start: 10h 23m 28s 433ms, eta: 01h 03m 48s 934ms\n",
            "\u001b[32m2021-04-18T09:47:14 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2021-04-18T09:47:14 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[32m2021-04-18T09:47:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20000/22000, val/hateful_memes/cross_entropy: 3.6774, val/total_loss: 3.6774, val/hateful_memes/accuracy: 0.5815, val/hateful_memes/binary_f1: 0.2517, val/hateful_memes/roc_auc: 0.4698, num_updates: 20000, epoch: 151, iterations: 20000, max_updates: 22000, val_time: 08s 371ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.518926\n",
            "\u001b[32m2021-04-18T09:50:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20100/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0335, train/total_loss: 0.0000, train/total_loss/avg: 0.0335, max mem: 11929.0, experiment: run, epoch: 152, num_updates: 20100, iterations: 20100, max_updates: 22000, lr: 0., ups: 0.54, time: 03m 04s 830ms, time_since_start: 10h 26m 41s 638ms, eta: 59m 03s 387ms\n",
            "\u001b[32m2021-04-18T09:53:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20200/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0333, train/total_loss: 0.0000, train/total_loss/avg: 0.0333, max mem: 11929.0, experiment: run, epoch: 152, num_updates: 20200, iterations: 20200, max_updates: 22000, lr: 0., ups: 0.54, time: 03m 05s 237ms, time_since_start: 10h 29m 46s 876ms, eta: 56m 04s 289ms\n",
            "\u001b[32m2021-04-18T09:56:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20300/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0331, train/total_loss: 0.0000, train/total_loss/avg: 0.0331, max mem: 11929.0, experiment: run, epoch: 153, num_updates: 20300, iterations: 20300, max_updates: 22000, lr: 0., ups: 0.55, time: 03m 03s 782ms, time_since_start: 10h 32m 50s 658ms, eta: 52m 32s 413ms\n",
            "\u001b[32m2021-04-18T09:59:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20400/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0330, train/total_loss: 0.0000, train/total_loss/avg: 0.0330, max mem: 11929.0, experiment: run, epoch: 154, num_updates: 20400, iterations: 20400, max_updates: 22000, lr: 0., ups: 0.54, time: 03m 04s 355ms, time_since_start: 10h 35m 55s 014ms, eta: 49m 36s 242ms\n",
            "\u001b[32m2021-04-18T10:02:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20500/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0328, train/total_loss: 0.0000, train/total_loss/avg: 0.0328, max mem: 11929.0, experiment: run, epoch: 155, num_updates: 20500, iterations: 20500, max_updates: 22000, lr: 0., ups: 0.54, time: 03m 04s 668ms, time_since_start: 10h 38m 59s 682ms, eta: 46m 34s 952ms\n",
            "\u001b[32m2021-04-18T10:05:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20600/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0327, train/total_loss: 0.0000, train/total_loss/avg: 0.0327, max mem: 11929.0, experiment: run, epoch: 155, num_updates: 20600, iterations: 20600, max_updates: 22000, lr: 0., ups: 0.54, time: 03m 04s 814ms, time_since_start: 10h 42m 04s 497ms, eta: 43m 30s 687ms\n",
            "\u001b[32m2021-04-18T10:08:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20700/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0325, train/total_loss: 0.0000, train/total_loss/avg: 0.0325, max mem: 11929.0, experiment: run, epoch: 156, num_updates: 20700, iterations: 20700, max_updates: 22000, lr: 0., ups: 0.54, time: 03m 04s 922ms, time_since_start: 10h 45m 09s 420ms, eta: 40m 25s 631ms\n",
            "\u001b[32m2021-04-18T10:12:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20800/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0324, train/total_loss: 0.0000, train/total_loss/avg: 0.0324, max mem: 11929.0, experiment: run, epoch: 157, num_updates: 20800, iterations: 20800, max_updates: 22000, lr: 0., ups: 0.54, time: 03m 05s 143ms, time_since_start: 10h 48m 14s 563ms, eta: 37m 21s 722ms\n",
            "\u001b[32m2021-04-18T10:15:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20900/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0322, train/total_loss: 0.0000, train/total_loss/avg: 0.0322, max mem: 11929.0, experiment: run, epoch: 158, num_updates: 20900, iterations: 20900, max_updates: 22000, lr: 0., ups: 0.54, time: 03m 04s 560ms, time_since_start: 10h 51m 19s 124ms, eta: 34m 08s 438ms\n",
            "\u001b[32m2021-04-18T10:18:09 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[32m2021-04-18T10:18:09 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-04-18T10:18:12 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-04-18T10:18:16 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-04-18T10:18:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21000/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0320, train/total_loss: 0.0000, train/total_loss/avg: 0.0320, max mem: 11929.0, experiment: run, epoch: 158, num_updates: 21000, iterations: 21000, max_updates: 22000, lr: 0., ups: 0.52, time: 03m 11s 266ms, time_since_start: 10h 54m 30s 390ms, eta: 32m 09s 875ms\n",
            "\u001b[32m2021-04-18T10:18:16 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2021-04-18T10:18:16 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[32m2021-04-18T10:18:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21000/22000, val/hateful_memes/cross_entropy: 3.7617, val/total_loss: 3.7617, val/hateful_memes/accuracy: 0.5741, val/hateful_memes/binary_f1: 0.2532, val/hateful_memes/roc_auc: 0.4673, num_updates: 21000, epoch: 158, iterations: 21000, max_updates: 22000, val_time: 08s 351ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.518926\n",
            "\u001b[32m2021-04-18T10:21:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21100/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0319, train/total_loss: 0.0000, train/total_loss/avg: 0.0319, max mem: 11929.0, experiment: run, epoch: 159, num_updates: 21100, iterations: 21100, max_updates: 22000, lr: 0., ups: 0.55, time: 03m 03s 953ms, time_since_start: 10h 57m 42s 702ms, eta: 27m 50s 477ms\n",
            "\u001b[32m2021-04-18T10:24:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21200/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0317, train/total_loss: 0.0000, train/total_loss/avg: 0.0317, max mem: 11929.0, experiment: run, epoch: 160, num_updates: 21200, iterations: 21200, max_updates: 22000, lr: 0., ups: 0.54, time: 03m 04s 505ms, time_since_start: 11h 47s 208ms, eta: 24m 49s 329ms\n",
            "\u001b[32m2021-04-18T10:27:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21300/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0316, train/total_loss: 0.0000, train/total_loss/avg: 0.0316, max mem: 11929.0, experiment: run, epoch: 161, num_updates: 21300, iterations: 21300, max_updates: 22000, lr: 0., ups: 0.54, time: 03m 04s 670ms, time_since_start: 11h 03m 51s 878ms, eta: 21m 44s 325ms\n",
            "\u001b[32m2021-04-18T10:30:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21400/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0314, train/total_loss: 0.0000, train/total_loss/avg: 0.0314, max mem: 11929.0, experiment: run, epoch: 161, num_updates: 21400, iterations: 21400, max_updates: 22000, lr: 0., ups: 0.54, time: 03m 04s 600ms, time_since_start: 11h 06m 56s 478ms, eta: 18m 37s 570ms\n",
            "\u001b[32m2021-04-18T10:33:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21500/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0313, train/total_loss: 0.0000, train/total_loss/avg: 0.0313, max mem: 11929.0, experiment: run, epoch: 162, num_updates: 21500, iterations: 21500, max_updates: 22000, lr: 0., ups: 0.54, time: 03m 04s 865ms, time_since_start: 11h 10m 01s 344ms, eta: 15m 32s 646ms\n",
            "\u001b[32m2021-04-18T10:36:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21600/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0312, train/total_loss: 0.0000, train/total_loss/avg: 0.0312, max mem: 11929.0, experiment: run, epoch: 163, num_updates: 21600, iterations: 21600, max_updates: 22000, lr: 0., ups: 0.54, time: 03m 04s 466ms, time_since_start: 11h 13m 05s 810ms, eta: 12m 24s 505ms\n",
            "\u001b[32m2021-04-18T10:39:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21700/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0310, train/total_loss: 0.0000, train/total_loss/avg: 0.0310, max mem: 11929.0, experiment: run, epoch: 164, num_updates: 21700, iterations: 21700, max_updates: 22000, lr: 0., ups: 0.54, time: 03m 05s 682ms, time_since_start: 11h 16m 11s 493ms, eta: 09m 22s 061ms\n",
            "\u001b[32m2021-04-18T10:43:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21800/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0309, train/total_loss: 0.0000, train/total_loss/avg: 0.0309, max mem: 11929.0, experiment: run, epoch: 164, num_updates: 21800, iterations: 21800, max_updates: 22000, lr: 0., ups: 0.54, time: 03m 04s 907ms, time_since_start: 11h 19m 16s 400ms, eta: 06m 13s 143ms\n",
            "\u001b[32m2021-04-18T10:46:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21900/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0307, train/total_loss: 0.0000, train/total_loss/avg: 0.0307, max mem: 11929.0, experiment: run, epoch: 165, num_updates: 21900, iterations: 21900, max_updates: 22000, lr: 0., ups: 0.54, time: 03m 04s 657ms, time_since_start: 11h 22m 21s 058ms, eta: 03m 06s 319ms\n",
            "\u001b[32m2021-04-18T10:49:10 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[32m2021-04-18T10:49:10 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
            "\u001b[32m2021-04-18T10:49:13 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
            "\u001b[32m2021-04-18T10:49:16 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
            "\u001b[32m2021-04-18T10:49:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 22000/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0306, train/total_loss: 0.0000, train/total_loss/avg: 0.0306, max mem: 11929.0, experiment: run, epoch: 166, num_updates: 22000, iterations: 22000, max_updates: 22000, lr: 0., ups: 0.53, time: 03m 09s 546ms, time_since_start: 11h 25m 30s 604ms, eta: 0ms\n",
            "\u001b[32m2021-04-18T10:49:16 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2021-04-18T10:49:16 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
            "\u001b[32m2021-04-18T10:49:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 22000/22000, val/hateful_memes/cross_entropy: 3.8290, val/total_loss: 3.8290, val/hateful_memes/accuracy: 0.5630, val/hateful_memes/binary_f1: 0.2625, val/hateful_memes/roc_auc: 0.4665, num_updates: 22000, epoch: 166, iterations: 22000, max_updates: 22000, val_time: 08s 727ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.518926\n",
            "\u001b[32m2021-04-18T10:49:25 | mmf.trainers.core.training_loop: \u001b[0mStepping into final validation check\n",
            "\u001b[32m2021-04-18T10:49:25 | mmf.utils.checkpoint: \u001b[0mRestoring checkpoint\n",
            "\u001b[32m2021-04-18T10:49:25 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
            "\u001b[32m2021-04-18T10:49:35 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
            "\u001b[32m2021-04-18T10:49:35 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 4000\n",
            "\u001b[32m2021-04-18T10:49:35 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 4000\n",
            "\u001b[32m2021-04-18T10:49:35 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 31\n",
            "\u001b[32m2021-04-18T10:49:36 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on test set\n",
            "\u001b[32m2021-04-18T10:49:36 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/32 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-18T10:49:37 | py.warnings: \u001b[0m/content/mmf/mmf/modules/losses.py:109: UserWarning: Sample list has not field 'targets', are you sure that your ImDB has labels? you may have wanted to run with evaluation.predict=true\n",
            "  \"Sample list has not field 'targets', are you \"\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-18T10:49:37 | py.warnings: \u001b[0m/content/mmf/mmf/modules/losses.py:109: UserWarning: Sample list has not field 'targets', are you sure that your ImDB has labels? you may have wanted to run with evaluation.predict=true\n",
            "  \"Sample list has not field 'targets', are you \"\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  3%|â–Ž         | 1/32 [00:01<00:34,  1.11s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-18T10:49:38 | py.warnings: \u001b[0m/content/mmf/mmf/common/report.py:164: UserWarning: targets not found in report. Metrics calculation might not work as expected.\n",
            "  + \"might not work as expected.\"\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-18T10:49:38 | py.warnings: \u001b[0m/content/mmf/mmf/common/report.py:164: UserWarning: targets not found in report. Metrics calculation might not work as expected.\n",
            "  + \"might not work as expected.\"\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:28<00:00,  1.11it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-19c9ca985f21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;34m\"training.num_workers=0\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m ]\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/mmf/mmf_cli/run.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(opts, predict)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfiguration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/mmf/mmf_cli/run.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(configuration, init_distributed, predict)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/mmf/mmf/trainers/mmf_trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mteardown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/mmf/mmf/trainers/mmf_trainer.py\u001b[0m in \u001b[0;36minference\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    160\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Starting inference on {dataset} set\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m                 \u001b[0mreport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_tqdm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreport\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmeter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/mmf/mmf/trainers/core/evaluation_loop.py\u001b[0m in \u001b[0;36mevaluation_loop\u001b[0;34m(self, dataset_type, use_tqdm, single_batch)\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0mcombined_report\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_report\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreporter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreport\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                 \u001b[0mcombined_report\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined_report\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcombined_report\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;31m# Since update_meter will reduce the metrics over GPUs, we need to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/mmf/mmf/modules/metrics.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, sample_list, model_output, *args, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{dataset_type}/{dataset_name}/{metric_name}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m                 values[key] = metric_object._calculate_with_checks(\n\u001b[0;32m--> 156\u001b[0;31m                     \u001b[0msample_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m                 )\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/mmf/mmf/modules/metrics.py\u001b[0m in \u001b[0;36m_calculate_with_checks\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_calculate_with_checks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/mmf/mmf/modules/metrics.py\u001b[0m in \u001b[0;36mcalculate\u001b[0;34m(self, sample_list, model_output, *args, **kwargs)\u001b[0m\n\u001b[1;32m    252\u001b[0m         \"\"\"\n\u001b[1;32m    253\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"scores\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m         \u001b[0mexpected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"targets\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         assert (\n",
            "\u001b[0;31mKeyError\u001b[0m: 'targets'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTAshObTm1ey"
      },
      "source": [
        "## Using your module\n",
        "\n",
        "Since, we have cloned the repo that contains the example we built in this colab notebook we can use it also to run the training from command line by using the `env.user_dir` option or by overriding the environment variable `MMF_USER_DIR`. Expand the cell below the next code cell to see how it can be done."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rwggw7XLUYuO"
      },
      "source": [
        "!MMF_USER_DIR=\"/content/hm_example_mmf\" mmf_run \\\n",
        "  config=\"configs/experiments/defaults.yaml\" \\\n",
        "  model=concat_vl \\\n",
        "  dataset=hateful_memes \\\n",
        "  training.num_workers=0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-Y2aJC_nRpy"
      },
      "source": [
        "## Conclusion and Further Steps\n",
        "\n",
        "In this colab notebook, we learned how we can use MMF to train and predict already existing models in MMF's zoo. We also learned how we can build custom models using various modules and goodies provided in MMF easily.\n",
        "\n",
        "If you have any issues, feedback or comments, please reach us out at mmf@fb.com or open up an issue at [GitHub](https://github.com/facebookresearch/mmf/issues/new/choose). We are also accepting PRs if you want to add your cool model to MMF and we are always open to community contributions.\n",
        "\n",
        "At Facebook AI, weâ€™ll continuously improve and expand on the multimodal capabilities available through MMF, and we welcome contributions from the community as well to build this resource. We hope MMF will be the framework of choice and be a catalyst for research in this area by providing a powerful, versatile platform for multimodal research. "
      ]
    }
  ]
}